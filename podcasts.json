[
  {
    "title": "2026 trends: Gas turbines, Texas\u2019 load queue and China electrifies",
    "description": "It\u2019s a new year, which means the veteran energy analyst Nat Bullard has dropped another annual, data-rich presentation on the state of energy and decarbonization.\n\nAnd per what has become tradition, Nat is back on Catalyst \u2013 for the fourth time \u2013 to discuss some of Shayle\u2019s favorite slides, cherry-picked from the 200-page deck.\u00a0\n\nIn part one of their two-part conversation, they cover topics like:\n\n\n  \nThe significance of China\u2019s rapid electrification\n\n\n\n  \nWhy the proportion of GDP spent on electricity has remained flat while oil has proven volatile\n\n\n\n  \nThe massive backlog and rising capital costs for gas turbines\n\n\n\n  \nHow current tech CapEx compares to past large-scale endeavors like the Manhattan Project and broadband build-out\n\n\n\n  \nThe extraordinary explosion of large load interconnection requests in Texas\n\n\n\n  \nThe divergence in load forecasting between grid operators and transmission providers\n\n\n\n  \nGlobal drivers of electricity demand growth beyond data centers\n\n\n\nResources\n\n\n\n\n\n  \nNat Bullard's 2026 presentation deck\n\n\n\n  \nCatalyst: 2025 trends: aerosols, oil demand, and carbon removal\n\n\n\n  \nCatalyst: 2024 trends: batteries, transferable tax credits, and the cost of capital\n\n\n\n\nCredits: Hosted by Shayle Kann. Produced and edited by Max Savage Levenson. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\n\nCatalyst is brought to you by Uplight. Uplight activates energy customers and their connected devices to generate, shift, and save energy\u2014improving grid resilience and energy affordability while accelerating decarbonization. Learn how Uplight is helping utilities unlock flexible load at scale at uplight.com.\u00a0Catalyst is brought to you by Antenna Group, the public relations and strategic marketing agency of choice for climate, energy, and infrastructure leaders. If you're a startup, investor, or global corporation that's looking to tell your climate story, demonstrate your impact, or accelerate your growth, Antenna Group's team of industry insiders is ready to help. Learn more at antennagroup.com.",
    "summary": "And then I think the second one was like 313, 131,313 dollars or something like that. So you'd have to have like $10 barrel and people using three times as much electricity or something roughly like that. Right. I really like this next one slide 32. Yeah, right. Right? Electrification of industry is going to be like 30% of the demand growth from electricity between 2024 and 2030.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Chio Kahn and this is Catalyst. So for all the talk that we have, at least in looking ahead the next couple of years at like spiking prices for electricity and things like that and the share of GDP that might come from electricity expenditures, it's really fascinating how range-bound it is. We basically spend between 3% and 4% of GDP on electricity and that is that essentially. Coming up, handcrafted artisanal slides on the state of decarbonization. What if utilities could meet surging electricity demand with energy assets already in homes and businesses? Uplight is making this possible by turning customers and their smart energy devices into predictable grid capacity through an integrated demand stack. Uplight's AI-driven platform activates smart thermostats, batteries, EVs and customers to generate, shift and save energy when the grid needs it most. Learn how Uplight is helping utilities unlock flexible load at scale, reduce costs and accelerate decarbonization at Uplight.com. Catalyst is brought to you by Antenna Group, the communications and marketing partner for mission-driven organizations developing and adopting climate, energy and infrastructure solutions. Their team of experts helps businesses like yours identify, refine and amplify your authentic climate story. With over 3 decades of experience as a growth partner to the most consequential brands in the industry, their team is ready to make an impact on day one. It started today at AntennaGroup.com. I'm Shale Khan. I lead the early stage venture strategy of energy impact partners. Welcome. All right, we're back. Long-time listeners will be familiar with my favorite time of the year, at the beginning of the year, when my friend, Nat Bullard, who is a longtime analyst and researcher in the energy and climate space, but also now the co-founder of Halcyon, puts together his annual opus of hundreds of slides on the state of energy and decarbonization. It's a chock full of fun data that you don't get to see elsewhere. As always, I picked my favorites and Nat and I talked through them. It's all sorts of interesting stuff. We talked data centers, obviously, but not just data centers, oil markets, solar batteries, all sorts of things. As usual, this was too long a conversation to fit into one podcast. So, this is part one. We're going to cover a whole bunch of interesting things now and then come back next week when we will cover part two. With no further ado, here's Nat. Nat, welcome back. Shale, great to be back as always. Happy 2026. This is year four that we've been doing this, something like that. It is indeed year four. We've been doing this recording as long as I've been doing a big presentation. So, yeah, fourth year it is. I do love a big deck. 200 slides this year. 200 slides, exactly. So, the way I have an important question for you, there's no way it just happened to be 200 slides. You made it an even number. What was the one that got cut out? Oh, the one that got cut out is nothing I can tell you. I had about 365 slides as of the start of November. So, the better way of thinking about it is that I essentially cut one slide for every slide that's in there. And in fact, I usually cut more than that. So, I start off with like 300 plus and cut it down to about 150, 160 and then build back up. So, it's first an exercise in addition, then it's an exercise in subtraction to get to the kind of the magic number. As you remember, way back when I had 141, which is like a sort of arbitrary prime number count or maybe it's not even a prime number of slides. And then I always felt like there was stuff that I had left. I made it bigger and then kept it there. I think it's quite possible that if you do more than that, you're going to lose the edit capability that gives it some of the strength over the course of 12 months. Yeah. I know. I think you need to start making the number like a meaningful number. Our mutual friend Andrew Bibi, I always appreciated at his for obvious ventures, their first couple of funds, the fund sizes were always very fun. The first one was 123,456,789. And then I think the second one was like 313, 131,313 dollars or something like that. It was palindromic. Anyway, you got to come up with something better than 200. I was thinking about that natural log pi. It's a rich tapestry of options of numbers, multiplied by something that I should be able to get in there. But I do like 200 for now. It's easier for people to wrap their heads around and to benchmark a little bit when they're paging through it too. All right. Enough navel gazing about numbers of slides. Let's get into some slides. Okay. As usual, I've picked a subset of my favorite slides that I found interesting and we're just going to run through them. So we're going to start on slide 15, which is something that given that this trend has been ongoing for quite some time and in fact the lines crossed like a decade ago, I'm surprised I didn't already know, which is that China is significantly more electrified, at least as measured by the share of final energy that comes from electricity than the United States, like substantially more so. And I had somehow missed that trend. Really, this is a great one. It's some work from Ember that it's been now doing for quite some time. What it tracks is, as you say, that the share of final energy that comes from electricity. But another way to think about it is how electrified is an economy. There are lots of different ways that you can get to a high number. One of them would be that you have very little primary industry. Another would be that you have plenty of plant primary industry, but you apply a great deal of electrification to processes that otherwise would be driven with some kind of thermal input. Or you have a tiny bit of primary industry and it's all luminous melting or something. Exactly. Like an example of a country like that, for instance, would be Norway, which is both an advanced economy, has some industry, is highly primary energy from or highly electricity rather than primary energy. But China is none of those things. It's a huge industrial economy. It's a huge user of primary energy. But it's also a consistent user of electricity for its final energy source. And it's also moving at a much more rapid pace than either North America or Europe, which is slowly ticked up over the course of five decades from 10% to a little bit north of 20%. China, meanwhile, has gone since 1970 from like 3% to 30%. Well, I want to benchmark to 1990, actually, because that's where I find the chart looks really interesting. In 1990, North America is already at roughly 20% electricity, which is, by the way, still the truth. It's true today, 22%, 23%. And that's the number that I always use. I knew that number. I always tell people, if they over index on electricity or over, they're like solar is going to be the whatever, it's worth remembering in the US that electricity is 20% of final energy consumption, or I guess it's actually 22%. But that's been true since 1990, whereas China in 1990 is down at what, 7% electricity and then jumped to 30% today. It's a very different trajectory. It's a totally different trajectory at a totally different scale, too. Everything in China is bigger when it comes to energy and in particular when it comes to the primary inputs. So I just think it's a really important measure as you talk about electrified future or the electric tech stack or whatever it might be, that China is sort of grasping this as opportunity that's also being done at scale. It's one thing to say Norway can do this. It's another thing to say that China is doing this. Not that you necessarily can speak for the Chinese central government, but you're certainly closer to it than I am. I've heard that one of the reasons, one of the rationales of China focusing so much on electrification is that they wish to control their own destiny. They don't have massive domestic reserves of hydrocarbons, but they can produce their own electricity. That is why they're investing in solar or the batteries to play chain and nuclear for that matter. So do you think that that explains this? China is just saying we can't rely on energy imports long term, so we're going to electrify. There's a bit of a nuance to that, which is that the primary imports that you'd have of primary energy in China are going to be oil, which is still a major importer, and natural gas for which it's still a major importer. It does import coal for energy balance reasons, but it has an absolutely enormous indigenous coal supply that will last for centuries. One thing to remember about this primary energy from electricity is that it doesn't mean that it's entirely coming from say, hydropower or solar wind. It can be coming from thermally generated, of the thoroughly generated sources, but it is definitely within the realm of one zone destiny. Where in the electricity is generated within boundaries, within a nation state, it is effectively sovereign. So in that sense, yes, it does provide a lot more control over destiny, less exposure to market forces to geopolitics to everything else if you're firmly in control of that element of energy. Where in electricity is almost entirely within the national purview, then you would want to spend more and more energy, so to speak, getting that electricity share of energy up as high as you can. Okay. So let's move on to slide 17, which I think is an interesting coda to slide 15. Five 15 is about how electrified an economy is. Slide 17 is super interesting, and I had never seen this data put together. It's about what share of GDP is spent on electricity versus spent on oil specifically. And the shapes of those two curves are very different from each other in a way that I guess if you had asked me, I might have predicted, but is stark when you look at it. So describe the difference between the two. Absolutely. So we're like five and a half decades into an era of thinking about energy shocks. And when we talk about those, we make it seem sort of system-wide, but it's really about a shock in liquid hydrocarbon prices and specifically oil. And if you look back at the data, you can see just how indexed the global economy was to oil in terms of how many units it took of oil input to get a unit of GDP and then the spend within different economies on not just energy, red and large, but oil specifically. In 1980, so year after the second oil shock coming from the Iranian revolution, just under 9% of per capita GDP expenditures globally were going to oil. That's pretty amazing. Imagine $1 out of every 11 being spent on oil of per capita GDP expenditure. That's pretty extraordinary. At the same time, the share for electricity was a little over 3%. If you carry this trend across the entirety of the last 45 years, what you see is that the oil share, A, goes down significantly. By the late 1990s, it's less than 5%. But it also bounces around quite a bit. So right now, the share is in the range of still about 5%, but it's been as high as 6.5 or 7. And in 2020, it was below 4%. Electricity, meanwhile, is essentially completely range-bound. The highest it's ever gotten is close to 4%, and the lowest it's ever gotten is 3%. So for all the talk that we have, at least in looking ahead the next couple of years at spiking prices for electricity and things like that, and the share of GDP that might come from electricity expenditures, it's really fascinating how range-bound it is. We basically spend between 3% and 4% of GDP on electricity, and that is that, essentially. Yeah, that's the question. The reason this is interesting is because of the future, not necessarily because of the past. So just to reiterate the past, electricity looks like a flat line for 50 years. Basically rising prices of electricity, or at least rising spend on electricity overall, let's say matches GDP growth, essentially. It has to, because it's a flat line. Whereas oil is super spiky. It's gone down since the 80s, sure, the early 80s, but it moves around a lot because oil prices move around a lot. Okay, so that's how it's gone historically. What happens now is a super interesting question, right? Because we're in this moment where oil prices are pretty low, President Trump is trying to do everything he can do to get oil prices even lower. He's got this stated goal of $50 a barrel if we start exporting a ton of Venezuelan oil, et cetera, et cetera. So he's trying to get oil prices low. Meanwhile, electricity prices are under upward pressure. I don't think anybody would debate that. And so do we see electricity escape its collar and spike? Could we see the lines cross, which, by the way, tier in this chart, they never have. We've never spent more of GDP on electricity than oil historically. It's just interesting to see whether this dynamic of one super volatile thing, which is oil and one super stable thing, which is electricity, whether that's going to hold. So let's do this as a thought experiment. What would it take to make those lines cross? First of all, it would take much lower costs for oil, much more oil price for one, to a much lower reliance upon oil as an input to GDP or it's an input to economic growth. We already get more units of economic activity out of a barrel of oil effectively every year, but you'd need to rapidly increase or enhance that. Secondly, you'd need to both spend a lot more on electricity and get less from it. You would need to have it be less of a contribution to GDP growth. So if you had both of those things happen, you'd be spending a lot more, but you'd be getting that GDP out of it. And therefore, GDP is not going up as much. The expenditure is going up. That's how you would do it. So you'd have to have like $10 barrel and people using three times as much electricity or something roughly like that. Well, this is what's going to be interesting, right? So let's just take electricity on its own, forget the comparison for a second. I think most people would bet that we're going to spend more on electricity overall over the next few years, five years, 10 years, whatever it is. The question is will GDP keep up and they're tied to each other because the primary reason we're going to spend so much more on electricity is AI and there's a bunch of people betting AI is going to help GDP go to the moon. Other people saying it's going to hurt GDP. That question sort of underlies whether we break this 50 year trend of basically spending the same portion of our GDP on electricity. Right. Remember that it is also global. So there are global, not just the US, not just Western European questions within there. What happens when places that have a limited but nonzero reliance on oil rapidly electrify and electricity becomes more of GDP, but you're in turn electrifying more of everything and more people have access to electricity. We've somewhat plateaued on global access to electricity. There's like a million different ways that we can think about cutting this up to make it look possible, but it's the right kind of question to ask without a clear answer. Let's put it down. My not very satisfying response. Okay, good. Not a clear answer. So let's move on. Slide 28. I want to talk about gas turbines. This one, we've talked about this a bunch of the spot and many of our listeners are going to be well familiar with this. I hadn't actually seen the data laid out though. I think it's interesting measuring the order book for gas turbines that we have already seen relative to current production capacity. So basically how under supplied are we? I'm gas turbines. So what do you see there? So I think it's actually important to start this at the front of the series, which is 2001. They were more than 80. In fact, closer to 90 gigawatts of gas plant orders in 2001, which is an awful lot if you think about it. We have to remember that the dash for gas that you and I started hearing about from industry veterans when we began is now quite some time ago, like two and a half decades ago. But there was a time when the world was ordering quite a lot of gas turbines and manufacturing obviously was of the mood to meet that demand with new supply only to find order books that collapsed from 80 something to well under 40 the next year. And then staying steadily below production capability for pretty much the entire time with the exception of a few years all the way up until right now. The current production limit and as you know, there's not that many companies that make gas turbines is somewhere in the range of about 60 gigawatts a year. And we're likely last year to be passed that by about 20 gigawatts and to be passed that by about 30 gigawatts this year. And who knows based on current orders, you know, 40 gigawatts above a 60 gigawatt production limit. And there's a lot of reasons for this. But the first and foremost is if you are in the process and the process and have the priority to manufacture gas turbines, what you really don't want to do is be over supplied. It's not really a great, a great tentable market position. Being under supplied has an Easton the first instance, probably a net positive on your ability to book contracts and to to secure durable orders and customers you want. And it has pricing benefits. People are going to have you to do as many as you can, build as much as you can. But if you're in charge of building S or mine now, you probably have the institutional memory of the early 2000s. Yeah. And we've talked about this before with regard to like electric transformers also. It's a similar situation where like folks who've been in the industry a long time do remember a historic period wherein there was this huge order book boom and then the market fell out from under them and they ended up over supplied. And so there's been reticence to expand capacity too much for that reason. They are expanding capacity, but maybe not fast enough. Anyway, what's interesting about it is that, but also, you know, even with that history, we are the most under supplied we have ever been, at least since the data starts at the beginning of this century where right now, even today for 2028, the order book for 2028 today is over 100 gigawatts relative to about 60 gigawatts of production capacity, which helps to explain why my new, you know, the expression everything is computer. I like everything is turbine because now we're seeing right, like if you're a jet engine company, you are pivoting to provide turbines for the grid, right? This is boom supersonic and all the air derivatives and like everybody who's got a turbine is trying to turn it into a AI data center power supply. And not just that, there's companies are turning things that are usable if, frankly, somewhat imperfect solution for large scale, always on grid connected power into power, right? Air derivatives are traditionally used for very specific applications and they're not being used necessarily to power things all the time, constantly for a decade, great. They can, but that's not typically within the design spec. The design spec would be for combined cycle turbines that are great integrated and that are part of a big, liquid, well supplied power market in which they play the role that they historically played. Yes, so it's an interesting times for all of these things, right, in terms of what this shortage for now with this order book mismatch brings to the market, who it brings to the market, the kind of approaches that people then take in terms of how they buy and sell power and everything. Yeah, it's different times. It's nothing like we've experienced an hour career, but for those who've got a little bit more tenure than us, it is a keenly familiar. The grid is changing fast. Data centers, electrification and extreme weather are driving a surge in energy demand. Utilities can meet the moment with existing resources by activating energy customers and their distributed energy resources to create predictable and flexible capacity with Uplights integrated demand stack. Uplight coordinates energy efficiency, rates, demand response, and virtual power plant programs into one cohesive strategy to strengthen grid resilience, improve energy affordability, and make progress toward clean energy goals. Learn how Uplight is helping leading utilities harness over eight gigawatts of flexible load at Uplight.com. Catalyst is brought to you by Antenna Group, the OGs of PR and Marketing for Climate Tech. Is your brand a leader or challenger? Are you looking to win the hearts and minds of customers, partners, or investors? Are you ramping up your new Biz pipeline? Are you looking to influence policy conversations? Antenna works with leading brands across the energy, climate, and infrastructure space to do all of this and more. If you're a startup, investor, enterprise, or innovation ecosystem that's helping drive climate's age of adoption, Antenna Group is ready to power your impact. Visit AntennaGroup.com to learn more. Okay, so relatedly then, of course, we're under supplied. So what do you expect? Prices are going to go up. So you've got some good data on that from you folks at Halcyon. Which you and I actually talked about the last time you're on the pod a little bit, but I want to run through it again because it is interesting. You've got good data on the average capital cost of various types of natural gas turbine power plants as they are planned. I'd say these are ones that are not operating yet, but the expected capital cost and how that trends into the future, which I find interesting. So kind of walk me through that, particularly the breakdown of the different types. One of the exercises that we do that manifests itself as a data series that people can buy from us in the subscription is just going through the regulatory corpore in the US and pulling all of the data from the CPC and the certificate of public convenience and necessity or an equivalent that is basically the utility going to the state and saying we need to build this X. And in this case, we're looking at gas plants and all of the data that gives you an idea of what these things are supposed to cost is within there. It's not broken out in a neat and tidy fashion where there's this like, here's this tabular spreadsheet with all the numbers in it. It tends to be hidden away in proceedings and responses and bottles and everything else. But the upshot of this is that we can map out on the order of more than 160 active plants, like close to about 80 gigawatts worth of actual capacity in which the cost of a combined cycle has doubled. And 2026 deliveries, things that are going to come online this year are in the range of, let's say, $1,200 per kilowatt. The projects that are looking to come online in 2030, 2031 are a little bit shy of 2,500. So close to doubling in that time period. And the reason I'm going to say to useful is this isn't based on the announced capital cost for right now. It's based on the price as it moves into the future. So it's an updated live number that reflects the actual market conditions underneath these things once they've been announced, not just whatever deposit you put down with your turbine supplier, but the actual ongoing costs to make this thing into a real asset. By the way, I also wonder, I don't know, you could probably tell me because you've looked at this data, but does this include like EPC cost, for example? Yeah, this is the delivered cost, like the allowance for work during construction, all the sorts of things that flow into it, minus the things that we know are discrete and separate. If you needed to build 80 miles of feeder for it, we script that out because that's not like part of the actual stuff, the kit. So we should come back and look at this data set again in 2030, because I do wonder whether they're actually underestimating the total costs. Because EPC and stuff like that in particular, that's also super inflationary. And it's really hard to find EPCs right now because they're all booked out. So I wonder actually whether it's again, end up being even more expensive than they think it is. Well, that's why we revisit this every month, because it moves. It's interesting to add new assets. It's almost maybe even more interesting to watch movement within existing assets based exactly on that. Like the EPC target went up because we did, right? Any number of reasons, tariffs, cost of labor. Well, it's the same, any number of reasons. But part of it is the same reason why gas turbines themselves are. It's an under-supply problem. Exactly. So you can see that coming through and continuing to go. And yes, we should be revisiting this, essentially constantly. And we're already seeing, we see people getting verbal quotes that are higher. I've had actual developers come up and say, your numbers aren't going. I was like, well, show me yours and I'll show you mine. And then they don't. So we haven't actually gotten anything more concrete than this. But this is what's written. This is what is essentially disclosed by law. And it's a pretty fertile ground to get an idea, briefly any other types of turbines. So simple cycle, not quite up so much, but up by about 50% from like $1,500 to about $1,500 to kilowatt. And then we start to see reciprocating internal combustion engines as well, or rice turbines. And those are really expensive. Those are like 2,500 to $3,000 to kilowatt already. But interestingly, we don't see a long delivery pipeline for those. The delivery pipeline for those only runs out a couple of years. We don't see anybody planning rice turbines or rice installations in the 2030s yet. Because I think they're mostly used for either bridge power or backup replacement for the diesel cents. In theory, but they're increasingly being deployed at a scale that suggests that they're being used for something closer to bulk power. Right. Yeah, maybe that is like I said, everything is turbine. Everything is turbine. Okay. Well, let's stay on the theme of like all this power build out stuff. I really like this next one slide 32. So so much capex. That's my version of the slide title. You're comparing the total amount of capex spending on this is going to be predominantly data centers, which just says tech capex in 2025 to other historic booms in capex spending in the economy, which is a good way to compare. What do you find? So I'm going to give credit first of all to Michael Semblegast and his teammate, Jake Morgan, an asset and wealth management. They built this slide first, not me. I did in the past, I'd done some examples of interstate highway and broadband capex as a comp, but I'd not done this full suite that they've got here, which goes from all the public works in the 1930s, like the Hoover Dam through the Manhattan Project. And we could call our wave of electrification in the US in the late 40s Apollo project, the highways broadband build out and the tech capex. And you know, things like the Manhattan Project, electricity, the Apollo project, these are like nests then or barely above one half a percent of US GDP at their peak, even the Apollo project, even the interstate highway project is like six tenths of a percent. Building out broadband capex in the year 2000 of its peak was like 1.2% of US GDP. And tech capex right now is just under 2%. So basically higher than anything else. And to your point, this is the capital expenditure to build compute, essentially. This is capex for building just the actual computational elements as well as the buildings that contain them and the power stuff that's within the fence of the company's capital expenditures. It is not power and transmission and water capex to go with it. So it's a pretty fast, it's a pretty fascinating big number relative to everything else. And I will add all these other data points, as you've said, where you can go into history and you can figure out what year was the peak, right? And so the peak of broadband capex was the year 2000 when it was just over 1% of GDP. That's comparison against 2025 actual capex of tech, which may or may not probably isn't the peak. And in fact, the 2000 example for broadband is instructive because the NASDAQ bubble burst in March of 2000 and capex kept going. So this is not a new observation. Michael Burry made this observation recently on Michael Lewis's podcast that the capital expenditure actually wags what might be happening in the purely financial market. So yeah, this could keep going for a while. The capex is committed. Sometimes it's already underway and a lot of it, a lot of it will keep going. This is probably not the peak. Like most of the estimates based on what companies themselves are saying for their estimated capex have a higher number for next year. And then again, you attach the relevant quantum of investment in the electricity sector to it and it's a lot more money too. I mean, that's like, it's hard to say in many cases specifically, is this new capex specific for energizing this data center? But certainly the prime mover of demand growth and of building new infrastructure in the US is for energizing data centers. And so the utility capex that goes with this is also in the tens of not hundreds of billions of dollars. Okay, good segue. Let's get back into energy then. The energy results of this, right? So let's go to Texas. We're going to ercot slide 91 is on the queue. The Texas interconnection queue. This is the large load interconnection queue, not the generation cube, although the generation queue looks I think kind of similar to be honest. Everybody knows this, right? Like Texas, lots of people are trying to build data centers in Texas. No big surprise there. The queue has gone up a lot. No big surprise there. It is pretty astounding how quickly it has gone up how recently. So the data suggests that the pipeline of large load interconnection requests in ercot in Texas was what 40, 40 ish 42 gigawatts as of January of 2024. So two years ago, it went from 40 ish 42 gigawatts to 226 gigawatts as of November of 25. Now it's even a little bit higher. Those are stupid high numbers. This is a reminder. I just want to like frame this up a little bit. As of what, maybe two years ago, there were about 30 gigawatts of data centers in the US in total. So this is going from 41 to 226 in Texas alone in two years in the queue. Now that's not all going to happen, obviously, but nonetheless. This is a great one. Ercot kindly publishes this every month in a somewhat unstructured format, but high enough frequency that it's worth extracting and putting into this fashion that I got here. This is an awful lot. So 226 gigawatts. This current state peak load is in the range of about 85 gigawatts. So that's like two and a half Xing, the existing state peak load, if all of this were to happen at once. It's gone, as you say, really, really rapidly. It's increasingly co-located. Like a couple of tens of gigawatts of that are actually co-located in larger connection load, which is interesting. And that keeps ticking up. But you know, sort of sounding like financial disclosures here, not all of these assets will eventually. I don't think that Texas is actually going to be building 226 gigawatts of just large load in the coming, let's say, seven to eight years. It's the nature of interconnection queues. A lot of it is speculative. And it's especially the nature of bubbly interconnection queues. Like clearly most of this won't get built out. I think it is indicative, though, of one thing that is definitely happening, which is just like people have the perception, Texas, you can build stuff, especially big stuff. A lot of data centers want to be big. And so there is a mad rush of developers, hyperscalers, REITs, Rick Perry, basically everybody trying to lock up sites in Texas where they think they can go interconnect gigawatts. And that adds up to hundreds of gigawatts in total. So there's something else here that I think you and I and many of your listeners will be very familiar with, which is a highly speculative supply side queue. We're very comfortable with the fact that, of course, wind and solar developers plan for 10 and they're going to build two, right? Or that ratio might even be too high. You've got 10 assets that you're planning and you're going to build one of them. That you're highly speculative in terms of where you're going to go, what you're planning to do. The size of any asset itself is also fairly prospective. It depends on what you're going to be able to get, right? And you'd be silly not to max out the possible interconnect on the site and you'd be silly to not try to do as much as you can for optionality sake. What I think we're not used to is a demand side interconnection queue that has some of those same speculative elements. Like back in the day, if you're building a hospital in suburban Atlanta, you're not going out and picking seven or 10 possible sites for that hospital. And you're definitely not picking seven to 10 sites scattered across four or five different states. Like if you are building a hospital, it's because you have a human need for medical services in a particular place. You're not viewing it as completely fungible between maybe we'll go to Tennessee and build the same hospital or maybe we'll go to Texas or maybe we'll go stay in Georgia. I think it's a different thing though, actually. I don't think what's happening is the developers are saying, I need a data center and I'm going to pick seven to 10 sites in which everyone wins, wins and I'll build it. It's actually, I think what's happening that a lot of developers, speculators, et cetera are saying, if I can develop this site, I can monetize it. I could sell this thing or maybe I can lease it if I'm a colo. So I'll do as many of those as I can do that I think are good because right now there's a really valuable market on the other side. And so everybody's doing that in Texas. Yeah, right. With the, my one other wrinkle being that like if you're the pure, if you're the pure respecting the 10 element of this and that's in the good way, right, you're in the land business side of this, right? You're in the site control part of this business. That's true. If you are then building the data center on one of those sites though, if you're building the compute, you're fit, you could be more fungible between that, between where you're exactly you're going to go depending on other factors. To some extent, to some extent there's, there's, there's more like spread across different places based on what you're planning to do. I need to build this compute and I've got that in this period of time and I'll talk to whoever has slight control that will help you do that. All right. So then the direct result of this is the next slide, slide 92, which is the no one knows anything slide. So we're staying in Texas. We just talked about this crazy big load interconnection queue. So of course the question is then how much new electricity demand is there going to be in Texas as a result of that? That is the operative question, whether you are a grid operator or the market itself or whatever. And you have these great contrasting data sets of the load forecast from two parties who you would think would be pretty aligned because they're both trying to answer exactly the same question and they work hand in glove with each other and yet. So walk me through the data. Sure. So this is the, this is one of the no one knows anything slides. I've got a couple. You know, my, my perennial favorite before this was markets respond to incentives. This is the new one for our current age is no one knows anything. So yes, the transmission service providers who are responsible for building the grid and integrating the energy required to energize an electrify what happens in Texas are fundamentally serving the same market that ERCOT, the grid operator is operating. However, ERCOT says, you know what, we could go from like little under 500 terawatt hours in 2024 to like a thousand by 2030. Right. So let's call it, you know, up 110% in that time period. The transmission service providers and the other hand are like, sorry, we expect to go all the way up to about 1600 terawatt hours. We're going to go 240% up from where electricity demand was in the state in 2024. And part of the reason is that they're looking at different information. The TSPs are looking at everything that people are asking them to build and ERCOT is looking at everything that it thinks can get, that it thinks will actually happen. But also the incentives are there. ERCOT's incentive is to keep things operational as high to the highest degree possible and at the lowest cost distributed across all of the people who receive service in Texas. The transmission service providers are paid for building assets and will happily, if possible, build whatever asset base they're being asked to build. So the true number is either somewhere in between or much closer to ERCOT's figure. ERCOT has the reason for demand, supply, balancing and upkeep and everything else to get it really, really accurate in energy terms. But the transmission service providers have every incentive to go as big as possible because that's how they get paid. They get paid to build assets. So your view here is that it's less like they just have different views of the future and their views very so substantially from each other and more just like the transmission service providers have an incentive to maximize the number and it's not a real forecast. Well it's nobody, it is driven, it is a forecast. It's driven by what people are asking them to do. The question is how much discounting are they doing and that I think is where there's a significant difference between ERCOT and the TSPs. The forecast out to 2030, four years away, it's not far in the electricity supply terms, it's like the blink of an eye, it's like no time at all. And there's a difference between these two forecasts of about 500 terawatt hours. Contextually the US total electricity demand is in the range of 4,000 terawatt hours annually. Right? Coming up like closer to 4,500. But, but. So call that more than 10% of all US electricity demand as just the delta between these two forecasts in Texas alone. Yeah, yeah, we're put it this way. Do we think that in 2030, Texas is going to consume as much electricity as a third of the United States consumes right now? Like it's a potentially. As compared to roughly 10% or like 11 or 12% last year. That's right. So like it's, you know, like it's sometimes these things are more helpful when we ask them in this fashion, in this comparative fashion, what would need to be true for all of that to happen? Right? And, you know, it's a huge number, but again, it doesn't exist in a vacuum. And this is back to my sort of earlier point about how developers work is there are other grids that are similarly aggressive in their expectation of what demand might look like based on requests that they're getting without any ability to kind of zero that out against similar or identical potential demand that might be built somewhere else and not happen in wherever it is. So if you're, you know, like this, there's kind of no way to cross reference all of this stuff yet because of the nature of the way they're regulated state by state. Okay. So the next one, I want to jump back actually to slide 35 because, because this one, I think it's a reflection of like, I guess it's a reflection of a US centric mindset that I have that I'm very surprised by this that or I don't believe this data. But the data is from the IEA and it's a projection of how much of the electricity demand growth through 2030 is going to come from various different sectors. And we just talked about in the case of Texas, but it's also true the case the US like this like insane boom and electricity demand coming from data centers. So what's surprising about this other data set is that data centers are ranked fifth in terms of the source of new electricity demand. I presume that's because we're, this is a global perspective. Not a US perspective. Right. That's right. So this is a global perspective globally. Electrification of industry is going to be like 30% of the demand growth from electricity between 2024 and 2030. Even electrified transport, which we in the US are sort of being trained away from thinking about as a big driver demand is a bigger driver of demand around the world than data centers is or would be. Even appliances, there's a lot of the world that needs to add its first dishwasher, right? Even its first refrigeration. Space cooling. So just air con and buildings is going to be like 10% of the total growth and data centers are right around 8%. What I would say is consider this very much a moving target. Like, I will be very interested to see what this print looks like a year from now, two years from now, three years from now, right? But the other thing will be to be considered is, is there a trade off? If there's a sort of finite quantum of new electrons that are going to be consumed between now and 2030, is it going to come to the point where? Well, yeah, more of it went more as being consumed by data centers and less in absolute terms by space cooling. That would be complex. That would be something for the rest of the world that would be akin to a trade off that we really haven't had to do in the US in quite some time, at least not at a national level. Yeah. Yeah. It's a moving target. Data centers are going to, I mean, even on a global basis, I think they're going to move up this ranking before too long. I would agree with that, certainly. They're going to move up, but where they land is a really big question, and how they interact with the rest of these different places that electricity will be consumed. It's going to be really interesting to watch. Yeah. And the fact that in some markets, it's to some extent, it's a near zero-sum game in the sense that there's only so much supply, and we're building out as much supply as we possibly can. So, like every new data center is a new electrified industry facility that isn't going to happen, probably. Nat Bullard is a longtime climate tech analyst and writer. He's a co-founder of Halcyon, which is an AI-assisted research and information platform. This shows a production of Latitude Media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. This episode was produced by Max Savage-Levinson, mixing and theme song by Sean Marquand. Stephen Lacey is our executive editor. I'm Shale Khan, and this is Catalyst.",
    "release_date": "2026-01-15",
    "duration_ms": 2785000,
    "url": "https://traffic.megaphone.fm/PSMI1883555056.mp3?updated=1768410615",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2026-01-16T01:55:33.812798"
  },
  {
    "title": "The VC case for 'full stack deeptech'",
    "description": "For \u201cdeep tech\u201d or industrial tech investors, a captivating idea on paper doesn\u2019t always translate into a sustainable or viable business. Even a remarkable technological breakthrough isn\u2019t guaranteed to survive the long sales cycles of the industrial world.\n\nSo which companies are worth the investment?\n\nIan Rountree, founder and partner at the venture firm Cantos, wrote a bare-bones thesis on X that offers guidance on this question. In it, Rountree lays out a stark list of the companies he invests in\u2014and the ones he passes on.\n\nIn this episode, Shayle and Ian unpack his post and explore how it applies to the current landscape of hardware and industrial startups. They cover topics like:\n\n\n  \nWhy selling technology to large incumbents like automakers or utilities can be a death sentence for startups\n\n\n\n  \nThe pitfalls of \"commercializing science\"\u00a0\n\n\n\n  \nWhy capital risk to sell an end-product can be better business than licensing technology\n\n\n\n  \nWhy \"weird\" companies\u2014\"N of 1\" startups\u2014can generate huge amounts of talent and capital\n\n\n\n  \nWhy selling commodities (like electrons or minerals) can actually be a safer bet than entering a new market\n\n\n\n  \nReal-world examples of full-stack success in the mining industry, including Earth AI and KoBold Metals\n\n\n\n\nLatitude: Earth AI\u2019s play in the hunt for critical minerals\n\nCatalyst: Calibrating hype with Akshat Rathi\n\nCatalyst: Climate tech startups need strong techno-economic analysis\n\nOpen Circuit: Pain, resilience, and bargain hunting for climate tech investors\n\n\n\nCredits: Hosted by Shayle Kann. Produced and edited by Max Savage Levenson. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by Uplight. Uplight activates energy customers and their connected devices to generate, shift, and save energy\u2014improving grid resilience and energy affordability while accelerating decarbonization. Learn how Uplight is helping utilities unlock flexible load at scale at uplight.com.\u00a0\n\nCatalyst is brought to you by Antenna Group, the public relations and strategic marketing agency of choice for climate, energy, and infrastructure leaders. If you're a startup, investor, or global corporation that's looking to tell your climate story, demonstrate your impact, or accelerate your growth, Antenna Group's team of industry insiders is ready to help. Learn more at antennagroup.com.",
    "summary": "Like big companies love to pilot something. And I like that concept. Right. Like because I think it's like definitionally more capital-intensive to do that. Like, just do this better. And that's like a trope. Like that's interesting.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shail Khan, and this is Catalyst. If you make progress quickly, great. Makes it easier to raise your next round and therefore do all the other things you need to do. But if you don't, the lack of momentum will kill you. I would rather need more money, but make time my friend, then treat them as pure trade-offs. Coming up, an ode to the full stack deep tech startup. What if utilities could meet surging electricity demand with energy assets already in homes and businesses? Uplight is making this possible by turning customers and their smart energy devices into predictable grid capacity through an integrated demand stack. Uplight's AI-driven platform activates smart thermostats, batteries, EVs, and customers to generate, shift, and save energy when the grid needs it most. Learn how Uplight is helping utilities unlock flexible load at scale, reduce costs, and accelerate decarbonization at Uplight.com. Catalyst is brought to you by Antenna Group, the communications and marketing partner for mission-driven organizations developing and adopting climate, energy, and infrastructure solutions. Their team of experts helps businesses like yours identify, refine, and amplify your authentic climate story. With over three decades of experience as a growth partner to the most consequential brands in the industry, their team is ready to make an impact on day one. Get started today at AntennaGroup.com. I'm Shale Khan. I lead the early stage venture strategy at Energy Impact Partners. Welcome. All right, so this one is a little out of the ordinary for me for two reasons. First, despite the fact that I am a venture capital investor, I actually generally don't have other VCs as guests on this podcast, at least not very often. I prefer usually to talk to operators and researchers who have a much deeper knowledge than anybody in my shoes. Second, I'm not generally in the habit of having a whole conversation based on a tweet or an ex post or whatever you want to call it, but I'm in the business of exceptions. So here's one. My friend Ian Roundtree is also an early stage investor. He leads Canto's, which is a pre-seeding seed fund that's been doing deep tech since long before it was cool. I think he may have been part of coining the term deep tech and is now part of coining whatever the next term is going to be. Anyway, Ian posted something about his investment thesis just a few months ago that has been basically stuck in my brain ever since then. So he said, and I quote, we invest in two archetypes at Canto's. One, full stack deep tech selling an end product or even commodity, not selling technology. Or two, weird, end of one, never seen anything like it before. Selling technology to incumbents, pass commercializing science, pass, and company doing the current thing, pass. I've been hung up on it because it resonates so strongly with my experience. And for the past now, 19 years working with startups in the energy and industrial world, first as an analyst. And now for the past eight years or so as an investor. I think it's correct, but it also describes a world that honestly excludes the vast majority of startups in this space. So it's worth unpacking. Ian, welcome. Thanks, Shale. I'm excited to finally have you on after many times of having podcasts worthy conversations with you, not in front of a microphone. And I rarely do this, but I want to talk about a tweet, an ex post, whatever, that you posted a few months ago and have pinned since then. So presumably you like it as well. But it just has stuck in my head ever since then. And I could not agree more strongly with it. And I think it actually, I want to unpack it because I think it contains some insights into the types of companies that really seem to succeed startups anyway within this world of deep tech, hard tech, whatever the hell we want to call it. You list out the two types of companies that you do look for to invest in. And we're going to talk about those two types of companies in a minute. But after doing that, you list out three types of companies that you pass on, that you don't invest in. I want to talk about that first, actually. The first one is companies selling technology to incumbents, which I want to harp on because in these deep and hard tech categories, in industrial categories, let's say, I feel like it's the majority of companies actually, the more majority of startups have a technology that they wish to deploy and the markets that they are entering are comprised of big incumbents who control a lot of the infrastructure and the distribution and so on. And so the obvious thing to do is try to sell them the technology. And so you do see a ton of that. Tell me if you disagree. Why is that a blanket pass for you? Well, I've learned this many things the hard way. I started Cantos. It'll be 10 years this spring. And it seems like the easy thing to do because it scopes down your startup to, oh, we just were going to use this amazing technology and we're going to productize it in the simplest way and that'll make it easy to sell to said big customer. And then it turns out that there's just so much institutional inertia, sometimes cultural, sometimes it's actual switching costs of having to rip out whatever they're using now to move to your product such that it ends up taking a lot longer than you think. And this has implications for capital intensity too, but we can address that later. It's more that it slows you down when speed is so critical to a startup. I think speed is actually the key point about this because there's another piece that's not described in here which people talk about a lot which is like, okay, your thing, whatever your thing is, especially if it's going to display some current thing has to be 10x better, whatever that means, faster, cheaper, et cetera. But let's say you do have a thing that is 10x better in whatever way. You would think, great, it's 10x better than the current thing. I'm going to go sell it to whoever buys the current thing and uses the current thing and I should win because it is that much better. And I think in the long arc of history, if you had infinite time and infinite cash to burn to get there, you could win that way by selling to incumbents. You can't if you are time limited on startup speed. Yeah. And I mean, to add color to that, the main reason that startups are time limited is because you raise serially in whatever. You raise 18, 24 months of runway. And so let's say you raise 24 months of runway, you need to start fundraising, let's say, six months before you run out of money. So you have 18 months to do whatever you need to do to unlock the next series of fundraising. And if your customer's sales cycle is longer than 18 months, you are de facto debt. And so you just you have to make progress faster. And you know, we could get you this is maybe a problem with venture capital as a whole. But the game on the field is you need to make discernible progress within 18 months or you don't raise your next round. A lot of times sales cycles do take longer and there are understandable reasons for this. You know, the stakes are very high in some industries. Lives could be on the line. The customer wants to do their diligence and they're using something good enough today. Question mark. And you know, maybe they go golfing with the sales rep from that company and you have to overcome all this to get your product in there and start to scale enough such that you can raise the next round and you get to live to fight another day. And one of my very first investments at Kansas in 2016 was a startup that had incredible machine learning and interoperability software that was selling into the automotive industry and to industrial robots absolutely best in class. And we would hear this time and time again from the reps at big automakers. And the company died despite having some flashy investors and incredible talent behind it because Ford and Toyota sales cycles were too long to raise the next round. Yeah, I mean, the auto OEMs are notorious for having the longest sales cycles of anybody, even for stuff that is not right. That's software you think could be faster sales cycles there. I mean, the other dynamic too, before without spending too much time on this piece is pilot purgatory, right? Like big companies love to pilot something. The timeline between pilot and full commercial rollout is never certain. It's not well defined in general. And so again, just from a time perspective, you get sometimes they're literally different teams. Like I always, I always encourage entrepreneurs to understand the counterparties incentives. And if they're like, whenever someone's behavior in a business context doesn't make sense to me, I try to learn how they're paid. And usually the answer comes to the fore that way. And so if you can just understand people's incentives, you can navigate around that and sometimes quicken the sales cycle or assess whether you are pursuing a sales cycle that is compatible with your startup in the first place. I will say I've learned this lesson the hard way as well in big industrial categories, like the chemical industry, the mining industry and so on. It's easier said than done to not do this. So we're going to get to the flip side of this, which is like if you're not selling your technology incumbents, then you have to be full stack and that has its own challenges. But I think you and I are aligned that like despite those challenges, it may be the only way to succeed at a venture pace and a venture scale. Yeah, well, I mean, speed is, I would argue the most important thing, but there's also like ultimate value capture and there've been a bunch of cases, whether it's selling some amazing chemistry into the battery supply chain or selling really advanced software built for an industry or a subsystem that goes into a larger plant or something where you have objectively superior technology and let's say it's 10x better than whatever there is today. And they acknowledge that this is great and they want it and they give you an offer to pay something that is way lower than you think it's worth. And when you push back, they say, sorry. Yeah. Like if you're just not in the pole position, then you're not often going to get remunerated for the value you're creating. Yeah. I mean, speaking less to the value capture and words of the timeline thing, I think I'm thinking about a company like Seal and Anno, which is run by a friend of mine, Gene Burschowski. I'm not an investor there, but close to Gene, who's amazing and like a total rock star CEO, you know, they're doing Silicon Anno battery materials and it is clearly better. Everybody I think agrees it is clearly better and they've had to go, they're selling to auto-amps predominantly and it's been and they're getting there now, but it's been, I don't know, 13 years or something like that. And like it's a very rare founder who can keep a company alive and funded that long for something like that. Yes, definitely. Okay. Let's move on. So selling technology to incumbents is hard for all the reasons we described. Second category that you said is a pass, commercializing science. I guess I want to hear what you mean specifically by that. Yeah. I mean, without getting into the semantics of like do we go at hard tech or deep tech or whatever, there is a category of company that I alluded to before, which is you have incredible technology and you think therefore it is incredibly valuable. But that's not actually how business works. Like the only point of a startup or business in general is to create profit margins above and beyond its cost of capital. And technology can be a very interesting means to that end, but it is only a means to that end. And just because your technology is mind blowing and you know, is cited in a bunch of papers or whatever does not necessarily mean it is valuable in an economic sense. And so if you were like, you know, commercializing your PhD, for instance, you can often look like you have a hammer and you're just looking for nails. Whereas in a business context, you have to reverse it and you kind of have to be obsessed with the problem and completely agnostic as to the solution. And I find that there are a lot of deep tech founders, especially PhD entrepreneurs and particularly when they are commercializing their own thesis, then they kind of have it backwards and they're waiting for people to tell them how awesome their technology is. When really you have to do a little more work to go show them. And so that's part of it. The other part is the timelines where I would much rather have a novel application of a existing or relatively new technology rather than wait for something to become market ready in the first place. And you know, I've lost a lot of money just like waiting for a startup to advance through technology readiness levels. So I feel like there's two pieces there to what you're describing. One piece is this a hammer in search of a nail kind of a problem. It's like somebody who's attached their personally attached to a particular technology that it turns out actually might not be like product market fit is elusive. And then the other is more like a TRL scale for lack of a better term problem of like if you're investing in like breakthrough science that hasn't been sufficiently proven, then it just takes too long to get there. Let's spend a minute on each of those. On that second one on the like it takes too long to get there. How do you think about things like fusion, for example, nuclear fusion, right? Which is like anybody who's investing early in nuclear fusion, even if they were doing that 10 years ago or today, like it's sort of inherently breakthrough science that hasn't been commercialized. I'm to the short answer is I don't. That's just not. I would do. I used to spend a lot of time thinking about fusion and I would like, I'm not technical at all, but like interested enough that I would like to bait people on. Is it DT or is it PB 11 or whatever? I loved like getting wonky like that, but it turns out in my business context, I don't have the time to wait for that to become commercializable. And there are some very interesting initiatives to bring fusion energy about. It is expressly the type of science that we at Kanto's do not invest in. Yeah. Although I really hope for humanity's sake, we pull it off. And then on that first one, the hammer and search of a nail thing, I also have seen that a lot. Like one, I sort of somewhat recent example for me is we ended up incubating a company called Voia Energy, which is still kind of in stealth, but is known to the public. And they're using metals as fuel basically in a particular way. And we kept running across. As we were incubating and we were working with the founders on it, we were taking, we're doing an electrochemical approach and we kept looking around and finding a few other companies were all using combustion. And I was trying to figure out, and it seems clearly worse to me for a variety of reasons to do that. And so we were asking ourselves the question, like are we missing something? Why is that the way that everybody is doing it? And a lot of it comes back to there's a particular professor who is a professor of combustion who has been the one to create the diaspora that exists such as it is today around this particular area. So it's exactly that thing of like, this is a combustion person. That is what they do. And so of course, that is what they are going to spin out of their lab. And that's where the starters are going to come from and the researchers and so on. Do you see this a lot? The problem in like, you do a lot of stuff in bio world, is the technology and solution in search of a problem statement, a bio thing often? Oh man. I started by saying we're doing a lot less of it now. Time has changed. Yeah. Do we do very much believe that like in our lifetimes, the intersection of computation and biology will probably be one of the more prolific areas of innovation. It has been tough to invest there for a while for a variety of reasons. But yes, there's a lot of this like commercializing science. Now that is an area interestingly where the capital markets have really figured out how to underwrite to let's say technology readiness levels. Yes, I find this to be frustrating not being in the pharma world, right? Because I look at the pharma world and I'm like, oh, like drug, the process of taking a drug from very, very early stage through to the market is like really well understood. The financing seems really straightforward. Why can't we just like replicate that across a variety of different markets? It just doesn't seem to work the same way. It's not standard enough. I mean, I think, I've thought a lot about this. It's just not standard enough. You need enough in that industry, you can kind of look at, okay, well, we're in, we have this kind of read out in our phase one trial and we're in this indication. We know this indication has this many patients and usually you pay this much for it and we can work out like big pharma when they acquire something. They've like run the discounted cash flow houses on it, not based on technical milestones. And it's just, we don't have a process that is as scientifically and regulatory defined and widespread enough that an entire capital market can develop around it. Now, that said, even though there are dedicated investors for biotechnologies, it has been a really tough area for public and private companies for a while, like a third of publicly traded biotechs were trading below cash because they just couldn't, if you didn't have a team that understood this, then like a broader universe of investors couldn't access that asset. And so even when you have very developed capital markets and dedicated funds that have been performed over decades, it's sometimes still not enough to weather certain storms. I still think in principle, at least like leaning in this direction on some other deep tech stuff would be interesting. You see this occasionally in other places, not to bring up fusion too much, but one that comes to mind to me is Pacific Fusion, which raised a quote unquote billion dollars out of the gate, but it's actually like a staged series of capital infusions contingent on milestones. It's like a billion dollars lined up if they can achieve X, Y, and Z over time. And I like that concept. I want to see that. I mean, you're an investor in like Salugen, right? If you had looked early days to what Salugen was doing, so this is in the chemicals industry, could you have lined up a series of technical milestones ahead of time? Could you have done this a priori and then said, okay, we're going to sort of predefined the capital roadmap here? Or was it just too much uncertainty? Were unit economics too much of an open question? Like, what stops us from doing that? Yeah. I mean, well, in Salugen's case, it was special because we met at Y Combinator Demo Day and they had maxed out their credit cards to buy $15,000 worth of Home Depot components and had a small bio reactor using their technology and were making hydrogen peroxide at that time and selling to float spas in the area. They had thousands of dollars of revenue and it was small, but you could at least, like they were already- Right. There wasn't a long road back to first dollars of revenue. Yeah. And so it's just been scale up. Like that, I would define, you know, they had this cool science out of their PhD work at MIT and then it was just like scale up from there. It was more engineering after the seed round and we could do the TEA and we knew that if they got to a certain scale, it would be this profitable. And then there were some question marks around which chemicals can we expand to and like how much of this is going to be specialty versus commodity. But that was a little easier than like, trust me, trust me, I just need $200 million and then we'll make money. Which we see a lot of. And occasionally works, right? It has. I mean, to be clear, like my opinions come from my own experience and are in the context of like my firm and my background and my capital base, it doesn't mean it's the only big money. Okay. So that's two sort of tough categories, selling technology and incumbents, commercializing science. The third one, I don't think we need to talk about a whole lot because it's kind of self evident, but you said like, Enth company doing the current thing. I share that view as well. It's just, it's much, much more difficult for a variety of reasons to get a venture grade outcome if you are in a really crowded market. It doesn't mean like some somebody breaks out of those crowded markets sometimes, but the bar is so, so much higher for you. It's harder to raise capital. Yeah, exactly. It's hard to attract talent. It's harder to separate yourself from the pack, basically. Well, this gets back into the commercializing science thing a little, which is there's a type of startup that I have invested in in the past that's like, well, here's all the technical reasons why our tech. We're the best because X, Y and Z, like we could create a chart and it's going to show all of our competitors and we have the full X and they have the personal. But sometimes you do legitimately have something that is better than the incumbents. Like, you know, in, I'll pick on one of our, one of our own portfolio companies, we invested in a company that's doing amazing processing of radar technology. Like mind blowing can take off the shelf data from your run of the mill radar and resolve more or less a three dimensional image. They call an RF camera, like mind blowing cheaper than a camera can see through fog. In some cases, see through walls, put this thing on cars, on drones has so many implications. And there's a much better funded company incubated by BC called Chaos Industries, which I personally think has worse technology. But they're doing a lot better because they've productized this. They're very good at selling it. They're very good at marketing it. They have a lot more capital and they're landing contracts with the government left and right. And you know, my hope is because we've invested in a much better technology, we can catch up. And like, I've seen this movie before and sometimes having the best technology doesn't win your race. It gets back to the old trope and venture of right, like a combination of team, tech and timing. But like, you can have the best tech and you don't have the other things going for you. It isn't sufficient. It may be necessary. It's not actually, I don't think it's all necessary either. It depends on the situation. It's certainly not sufficient. When it gets into like the intersubjective nature of capital markets, you can be objectively right. But if everybody else is wrong for long enough, you are de facto wrong and they are de facto right. And so you have to account for it's tempting for self-styled intellectuals to sit and think that like, oh, I've just outsmarsed everybody. But like the market can afford to be wrong longer than you can afford to be right. Yeah, that's the other point, right? Like what does it take to prove your rightness, how long, how much money, et cetera? I think back on the heady days of all the solar technologies and this thin film technology is predominantly in the late 2000s, early 2010s. And you know, I think the common story that everybody tells about what happened there is that VCs invested billions of dollars into a bunch of different thin film technologies. And then meanwhile, China scaled up crystalline silicon, which is the dominant technology and cost got so cheap that there was no way to compete. And that's true. It is also true that in principle, some of those thin film technologies could be cheaper, right? Like they could still be cheaper that crystalline silicon is even at a silver cost today. But to get to the point where they would be cheaper requires a lot of scale and a lot of capital. And as crystalline silicon prices were crashing, nobody was willing to fund that effort. And so you know, you may have been right, at least some of those folks may have been right. But yeah, it means nothing essentially in the grand scheme of things because they were never able to prove it. Yeah, I used to kind of think that you could like, if you had an amazing enough technology, you could sustain years of no commercial progress assuming you could fund it such that because you were taking no market risk, I was, you know, patently obvious that your technology was better. Once you came to market, you could jump up the commercialization curve faster. And rather than needing to start small, you could just start with a giant contract because your stuff is so awesome. One of the things I didn't see there is that the learning curve matters. And once you start doing a thing, you tend to get better at it. And if you're commercializing something at small scale, you're kind of working out all the kinks that if you're sitting in your lab, making your technology better, then you bring it to market, you only get to learn then. And it's kind of like too late. And so I'd like, I actually like startups that start small and they just get that little compounding and learning every little like sales motion, getting to know your customer better and internal operations and how does technology hand off with manufacturing and closing that design for manufacturing loop and all those little things. Like, if you're just developing the technology, you have to like punt them and then do them all later and it's maybe too late and you've got all this technical debt or something. This compounding is a wonderful thing. The grid is changing fast. Data centers, electrification and extreme weather are driving a surge in energy demand. Utilities can meet the moment with existing resources by activating energy customers and their distributed energy resources to create predictable and flexible capacity with Uplights Integrated Demand Stack. Uplight coordinates energy efficiency, rates, demand response and virtual power plant programs into one cohesive strategy to strengthen grid resilience, improve energy affordability and make progress toward clean energy goals. Learn how Uplight is helping leading utilities harness over eight gigawatts of flexible load at Uplight.com. Catalyst is brought to you by Antenna Group, the OGs of PR and Marketing for Climate Tech. Is your brand a leader or challenger? Are you looking to win the hearts and minds of customers, partners or investors? Are you ramping up your new Biz pipeline? Are you looking to influence policy conversations? Antenna works with leading brands across the energy, climate and infrastructure space to do all of this and more. If you're a startup, investor, enterprise or innovation ecosystem that's helping drive climate's age of adoption, Antenna Group is ready to power your impact. Visit AntennaGroup.com to learn more. All right. Let's talk about what to do. We've mostly been talking about what not to do. You have two archetypes here. I want to spend most of our time on the first one. Would you describe as full stack deep tech selling an end product or even commodity, not technology? So, say more. What is full stack? When I say full stack, I'm some tweeting a 2015 post from Chris Dixon at entries in Horowitz. It's largely been a crypto investor. But back in 2015, he described a full stack startup and was largely alluding to companies like Uber and Lyft and Airbnb, who rather than trying to sell software to the taxi and leave the magazine industry or to the hotel industry, were leveraging software to influence the real world while not necessarily building hard-rhythmselves. I thought that was really interesting and largely apply that same thinking to more industries and the types of technologies that we invest in where you are building some software but also some physical component. There's nuance to this. I mean, it's sort of a full stack is similar to vertical immigration. But it's not exactly the same thing. There's heavy overlap, but it connotes the combination of technology and non-tech product or services specifically. Yeah. I think the key thing for me actually more than like defining full stack is the selling an end product or even a commodity, not selling technology. Whatever the thing is that your thing unlocks, like go, if I'm vertically integrating in a direction, it's downstream to start at least. Right. Sell to the end customer of the thing. Whatever your technology makes better, figure out who the end customer is for the thing that that made better and then sell that product. But that is expensive. Like let's be clear about the trade-off there. Right. Like because I think it's like definitionally more capital-intensive to do that. Do you disagree with that? No, it is. There's clearly the downside of going full stack is that you typically need a little more money to do it. And like the time as money added is appropriate, but there's a multiplier on time, as we talked about earlier, for startups where there's such an importance around making progress quickly when layering in the intersubjective nature of capital markets that like if you fail, like if you make progress quickly, great, makes it easier to raise your next round and therefore do all the other things you need to do. But if you don't, the lack of momentum will kill you because it'll become multiplicatively harder to raise capital the less progress you make. I would rather need more money but make time my friend than treat them as pure trade-offs. Can you give me like a canonical example for you of like a company either you are or not involved with that is full stack in a category where they could otherwise be selling tech to incumbents? Yeah, so I mean, a great example is mining. And there's a few companies in this space, full disclosure, one of our largest investments is Orthaei. But you also have Cobold, which rates a lot of money and Varei. And there's a couple of others in this space. So mining is an industry that is massive and there were just not that many startups in. And I thought that was interesting because it's such an important industry. This is definitely a commodity industry. And it is increasingly important for semiconductors, for defense tech, for electrification of the grid. Yet we're not seeing much innovation there. There had been some startups along the way that said, hey, we're going to use satellite imagery, AI to help people mine better and know if there's a deposit near your existing mine or something like that. And it turns out it was very hard to sell that technology into mining companies. And two, they weren't willing to pay much for it. And so the company we invested in, or the AI, and I believe Cobold may have had a similar journey, but although I won't speak out of turn, said, well, hold on, how much is it to acquire these mineral rights and which is a drill? Like, let's just hire some geologists and go apply for the rights. Earth AI went out and bought rights and drilled their own hypotheses. And we now own deposits that we think are very valuable. There's a lot more work to be done to prove them out. You've seen Cobold go out and acquire a deposit. They're using AI to maybe find and definitely mine a little more efficiently. And this is one where like if you had taken the typical startup path of just selling software, maybe you make a software license, but if you are willing to go out and take a little more operational risk and need to raise a little more capital to be sure, then you can end up owning literal gold in the route. And that's a lot more interesting to me as an investor than a mining SaaS company. The other category that I think of as you described this that I think it's equally applicable to is the like AI for materials discovery category where there's a bunch of companies doing that now where like my question to every one of them and I've talked to them is like, okay, let's say you can you build your magic model and you can use that model to discover novel materials. Are you going to license the model and the technology to a big company? Are you going to try to sell the IP around the materially discover or are you going to do what you probably should do which is make the material? Whatever you discovered, if it's better, you should just make and sell that thing. And that ultimately, that's your business. You're not an AI discovery company necessarily. You're an AI enabled materials company. And some of them are doing that. But it's kind of the same story as I think you're describing in the mining industry. Yeah, totally. I mean, one of the initial thread that I started pulling on that led me to my preference for full stack hard tech startups was I invested in a construction software business that was doing AI for planning. And there were a couple of these out there and we were sort of selling against some of those other companies. And the general contractors are really slow to adopt. And I was like, could we just buy a failing general contractor? Like, just do this better. And it turns out it's not just the technology that improves every single time we've invested in a capable team or watched a capable team go after an industry with a lot of inertia. There are a thousand little things that can be improved upon that if you're not taking that on and you're just selling into the industry, you're not going to help innovate those areas. You mentioned even commodity thing. Let's talk about commodities for a second. A lot of the world that I treat, I'm like, you know, focusing energy and industrials. And like, when you boil that world energy in particular down to its core constituent parts, it's mostly commodities, right? Electrons or MMBTUs or whatever. And notoriously commodities are difficult markets. Commodities sectors are difficult markets to build startups. And that's like a trope. I think there are a bunch of ways that you can do it. But you said in here even commodities. So like, what is it that makes a company who is going to end up, if they go full stack, they integrate downstream, they're going to sell a commodity. What does it take for that to be attractive to you? Yeah, I love commodities. For a long time, it was used as a pejorative in venture capital. I love when a company is selling a commodity. When the technology that is embedded in their full stack business actually gives them a cost advantage, that's incredibly powerful. And taking a step back, like what we do as investors is in a way price risk. And so if we're thinking about the types of risks that a startup takes, there's technical risk, there's execution risk, and there's market risk. And you always have execution risk. So let's focus on the other two in hard tech and deep tech and physical world. You're typically taking a bit more technical risk than some of our peers who focus on maybe software and AI applications. And not foundation models, a lot of technical risks there, of course. But if we're taking more technical risk, and we are on balance, basically paying the same prices at seeds or is a series B, whatever, as companies that are not taking more technical risk, then unless you are offsetting that with a equivalent decrease in market risk, you're probably just going to make worse investments. These are the physics of finance. And so why I love commodities is because it offsets the technical risk we're taking with less market risk. Because I know if I'm selling a commodity, i.e. my product is molecularly identical to the competitors, but I have a cost advantage. I know you can sell that. I love one like in my memo, I can just look at the spot price of something on a liquid market. And I know that's the mark that we're shooting for. Whereas if you're developing something new, or you're having to convince a customer that they need it, even if you know it's objectively better, then there is inherently market risk there, and that just makes it a riskier asset. I've heard of the node, CoSla, say something very similar, like he'll take technical risk, but he won't take market risk. The other thing that I would say here about commodities is like, I think people- But he invested in impossible foods for getting that there was market risk there. Yeah, I mean, many things have- I think nothing is as simple as that makes it sound, right? Like there's market risk where you think there's not, but also commodities are not commodities in the way that you think they are. Like I mentioned that electrons are commodities. They're kind of not, right? Like, yeah, an electron is a fungible thing, and it's similar to every other electron, except where and when it is delivered is very, very important. And so there are lots of non-commodity businesses built on selling electrons because they're able to sell electrons in the right place at the right time, or both. And I think that's true of most commodity markets, right? Like very few of them are truly indifferent to time and place. And that additional factor matters in terms of whether something- whether it's truly like a knife fight to the bottom on cost, which is why people don't like commodities from an investment perspective. I don't want to go on too much of attention, but there's an interesting corollary here to the type of real-world full-stack business that we both like and sort of decentralization of infrastructure. Like if there is advantage to defy economies of scale and co-locate your product, your, you know, behind the meter, energy generation, whatever, then you are operating your own sub-facility, but you're still taking some market risk by virtue of customer concentration, you know, because you can presumably as a startup only afford to have so many of these co-located things. And, you know, if they're with the same two or three customers and they change their mind or someone gets fired and the new gal doesn't like you or whatever, that's risky. So I've invested in some of those, but you just have to be clear about like there's, you know, whether the counterparty is going to slow your sales cycle down. Yeah, there are also interesting examples that are like companies in a commodity space with a differentiated input and or companies in commodity space that like their execution and ability to learn in that market allows them to enter into a new market that's non-commodity. So let's say you want to give an example, Crusoe, right? So Crusoe was flare gas to Bitcoin. That was that was the start of the company. They went and locked up assets that were methane flaring. It's wasted energy. They turned that they, you know, put little data centers there and mine Bitcoin. That's that's a commodity, right? Bitcoin's clearly a commodity. And they did that whole bunch and then that allowed them to sort of like build up more expertise. They built, they started going beyond Bitcoin and into cloud and they had their own little cloud managed services thing. And they had all this experience building small data centers and then like the right place at the right time came around and to give them credit, like they fully went after it and took full advantage of it. And then now they're building, you know, Abilene Stargate campus for OpenAI and Oracle and a bunch of others and they just raised money at a $10 billion valuation. Another totally different company. Yeah. I think Corrie started mining crypto too. I don't know if it was because they were flaring, but no, no, it was different. I mean, Corrie was just just doing crypto mining. A lot of crypto mining companies are now, you know, AI data center companies that that pivot is not unique to Crusoe, but I think everything is computer. Yeah. But they've been able to particularly take advantage of it. What's interesting to me about it is like they were, they were certainly in a commodity business. They had to differentiate the original version of the business had differentiated input, right? It was flare gas was their input to create Bitcoin. And that's cheap energy, right? If you could take advantage of it. That's what I mean. If you have a car, you have a structural advantage in producing a commodity, that's like the holy grail. Right. Right. Like if I can make dollar bills for 70 cents, I'm a trillionaire. So I have a different frame that I often use to describe to categorize startups. And my version of it is wave makers and wave riders. So like there are companies that make a wave, they do a thing that but for the existence of that company, the would not have happened, certainly would not have happened in that timeframe or their wave riders, right? They correctly predict a trend a few years ahead of time. And they time the development to their technology or their company correctly to that trend. And then they're able to ride that wave, you know, Tesla being the canonical wave maker, like the EV thing wouldn't have happened at least on that timeframe that it has but for Tesla. And then for every Tesla being the wave maker, there are hundreds of wave riders, EV charging companies, battery technology companies, etc, etc. Right. And it's useful frame in the first place, but it's also valuable for me because I think the the the onus on the, you know, what matters the most is different across those two categories. There's a certain type of particularly on team. There's a certain type of founder and founding team that is required in the wave maker category because it is so hard because you are swimming against the tide. And you have to bend the arc of the universe to meet your your needs. I think it'd be similar in the full stack deep tech concept, right? Like it's harder to do you need more money, you have to do more things. Definitely. Right. And so what do you how do you think about like the archetype of the founder that can do the full stack deep tech thing? Our preference for this type of business has also increased the onus on the entrepreneurs. And you know, when we when we thought that you just needed incredible technology and you know, sort of good enough business sense, I was not not that we were lowering our bar for entrepreneurs, but I was sort of looking for different things. And now we flesh it out. I actually have internally like a wave of scoring founders along six axes and how we tease out each of those. And a lot of them are around the qualitative. It's sort of, you know, it's talent gravity. It's narrative ability. And if you're going to raise the capital required to vertically integrate a business and go full stack, then you just have to be that much better at fundraising. And I think you have to be really honest about the fact that, you know, a lot of people don't like the reason that I'm on this side of the table, not operating a company is I know I don't have those qualities. I have enough of them to run a small investment firm, but not to build a full stack business that's going to come at a giant advantage incumbent. And to do so, you probably need to raise, you know, nine figures of capital, probably ultimately billions of capital across equity and other forms of capital. So the type of person you need to make a wave is like you said, like, you know, bends the arc of time toward them. We'll say that a founder has to have such a powerful gravity well that like, we feel ourselves being physically pulled toward them in a meeting. And if we don't get that sense, we sort of say, okay, well, maybe this isn't a wave maker. And we've just found ourselves looking for people that are both incredible technologists and also have this ability to communicate things in a hyperflament way. They can kind of go up and down the stack of understanding, communicate it to an expert, communicate it to the general audience and get people so fired up that they bend the fabric of space time to their will. Yeah. All right. Just to close out the other category of things that you do look for to invest in, which I also share, you described as weird n of one never seen anything like it before. So here's my question for you. Apart from those things, I mean, I'm drawn to them too. But apart from just them being cool, like, how would you articulate? Why are the weird n of one never seen anything like it before? Ideas, good venture investments on balance to continue the astrophysics parallel? Like, if you're in a area with other astronomical bodies, you're sort of like competing with their gravity to pull in talent and capital and PR, then that's harder than if you are the only body to reach some critical mass in an area with a lot of, you know, a lot of matter. And if you're like the, you know, we saw an early stage company recently that I thought was weird and interesting. We ultimately didn't invest where a woman had left Wall Street to build, as she calls it, to beers for dinosaurs. And she's using, you know, some satellite imagery and AI and a field team to go out and find dinosaur fossils, which have been selling for a higher and higher prices. And I thought that was really interesting. I mean, I've been doing this for 10 years. I see like a thousand startups a year more. I never seen anything like this. And if it's an area that is sort of big and interesting enough, and I don't know if fossils are, I didn't go that deep, but then she's going to build like the company. And when people think of dinosaur fossils, they're going to think of her company. Whereas if you've got, you know, you and I both invested in long duration energy storage businesses, but if you've got like the 20th Elders business, it's going to be a little harder to stand out and the venture capitalists you need to raise money from have probably made a bet already. And a lot of them don't want to make a competing bet and it just complicates things. Yeah, but I want to separate two things. I mean, there's a part of this is just the flip side of nth company doing the current thing, right? Like if you just first to a market, I guess would be another way to put it. That's a piece of it. But the other piece of what you described is weird, never seen anything like it before. Like it's something above and beyond just you are the first in this category. It's I think to me, an enormous amount of startup success ultimately comes down to narrative. How well the founders able to construct the narrative that raises capital and attracts talent. But also just like if the narrative, if the story, if I can tell myself the story of what this company is trying to be and my eyes light up as I'm trying to do it, because it's so interesting and so weird or so impactful, if it would have such a big impact on the world, I'm drawn to it. And that's a part of the magnetism that we're talking about before that can come from the founder, but can also come from the vision. I think of a company like Colossal Biosciences, who's the tag, I don't know anything about them apart from that they're trying to bring back the woolly mammoth. Like that's interesting. It's weird. Yeah. It's weird. So yeah, I guess I just want to I want to clarify that it's not just being first, it's not being the sole body in that part of the universe. It is also about that part of the universe being exciting, I guess. Right. It's not valuable just because it's weird for us. Weird as a filter for might be unique and unique in a valuable way. Look at this defense tech wave. And now it's like the hottest area outside of AI. And that was more or less created by Andrew and I'll give a lot of credit to Palmer, one of the four founders of that company for like completely reversing this trend. And that is I don't know if I would have called Andrew weird in the beginning. Maybe I would have like now it's sort of hard to look back because it's so big and spun off all these other defense tech unicorns that yeah, when I first heard about it, I thought it was weird. And then weird sometimes means controversial to be clear like to lean into these areas you necessarily are going to have to disagree with people. And if you're not comfortable doing that, then it's going to be difficult to build a weird, unique end of one company. All right. And this was unsurprisingly a lot of fun for me. Thank you again for joining. Of course. Yeah. Well, this is just one of our normal conversations we have recorded this time. That's right. Ian Rountree is a partner and the founder of Cantos. This shows production of Latitude Media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. This episode was produced by Max Levinson mixing in theme song by Sean Markwan. Stephen Lacey is our executive editor. I'm Shale Khan and this is Catalyst.",
    "release_date": "2026-01-08",
    "duration_ms": 3162000,
    "url": "https://traffic.megaphone.fm/PSMI3511689872.mp3?updated=1767839976",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2026-01-16T01:59:57.335982"
  },
  {
    "title": "How AI is changing weather forecasting",
    "description": "Weather forecasting drives billions of economic decisions \u2014 from grid operations to evacuation planning. Better forecasting could improve supply chain planning, disaster warnings, and renewable integration. The industry has decades of satellite observations and ground measurements, making it ripe for AI-driven advancements.\n\nAnd it\u2019s already happening. But how exactly does AI get used in weather forecasting, and how does it actually lead to improvements?\n\nIn this episode, Shayle talks to Peter Battaglia, senior director of research at Google DeepMind\u2019s sustainability program, which launched a new AI-powered weather forecasting model in November 2025. They cover topics like:\n\nWhy precipitation is so much harder to predict than temperature\u00a0\n\nHow the weather industry works, with governments creating global models and private companies refining them for specific use cases\n\nWhat AI models can see that traditional supercomputer simulations can\u2019t\n\nNovel sources of data like cell phones, door bells, and social media\n\nResources:\n\nLatitude Media: Where are we on using AI to predict the weather?\u00a0\u00a0\n\nLatitude Media: Could AI-fueled weather forecasts boost renewable energy production?\u00a0\u00a0\n\nCatalyst: Specialized AI brains for physical industry\u00a0\u00a0\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by Uplight. Uplight activates energy customers and their connected devices to generate, shift, and save energy\u2014improving grid resilience and energy affordability while accelerating decarbonization. Learn how Uplight is helping utilities unlock flexible load at scale at uplight.com.\n\nCatalyst is brought to you by Antenna Group, the public relations and strategic marketing agency of choice for climate, energy, and infrastructure leaders. If you're a startup, investor, or global corporation that's looking to tell your climate story, demonstrate your impact, or accelerate your growth, Antenna Group's team of industry insiders is ready to help. Learn more at antennagroup.com.",
    "summary": "AI will improve weather forecasting. You have like weather and stream flow and other types of things like that, all fluids. Right. Like we're getting more precise. Like that wasn't that that's not new.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shail Khan, and this is Catalyst. We don't really understand how the AI models forecast it, but they are capable of treating the hurricane as almost like a large macroscopic scale object that is moving. They have like spatial awareness in a way that the old models didn't. Yeah, it's a really interesting area, I would say, of the science of how AI works to understand exactly how they see the world in that sense. Coming up, where the winds are blowing, using AI for weather forecasting. What if utilities could meet surging electricity demand with energy assets already in homes and businesses? Uplight is making this possible by turning customers and their smart energy devices into predictable grid capacity through an integrated demand stack. Uplight's AI-driven platform activates smart thermostats, batteries, EVs, and customers to generate, shift, and save energy when the grid needs it most. Learn how Uplight is helping utilities unlock flexible load at scale, reduce costs, and accelerate decarbonization at Uplight.com. Catalyst is brought to you by Antenna Group, the communications and marketing partner for mission-driven organizations developing and adopting climate, energy, and infrastructure solutions. Their team of experts helps businesses like yours identify, refine, and amplify your authentic climate story. With over three decades of experience as a growth partner to the most consequential brands in the industry, their team is ready to make an impact on day one. Get started today at AntennaGroup.com. I'm Shell Kahn. I lead the early stage investing practice at Energy Impact Partners. Welcome. Here's a statement that I suspect would be pretty non-controversial. AI will improve weather forecasting. It's obvious, right? It seems like it must be true. I certainly would have agreed with that statement had you asked me before this conversation you're about to listen to. But to me, the interesting question is why exactly? Through what mechanism can AI improve weather forecasting? So that matter, how do we actually do weather forecasting today? And if it does get better, what are some of the likely outcomes that it will enable? It's an interesting set of questions for me for two reasons. First, weather forecasting itself is important to a whole host of other categories I care about. Obviously, resilience, but also energy and a variety of others, agriculture, etc. But also, it's interesting because I think it's exemplary of a whole host of next wave applications for AI. LLMs are, of course, finding their way through everything that requires language. Now there are world models starting to show up to try to revolutionize robotics and things in the physical world. What about things like weather where we have used some machine learning historically, but can we do better with transformers and the new architecture of AI that we're seeing in other categories? Let's find out. My guest today is Peter Pataglia. He's a senior director at Google DeepMind where he is leveraging the big brain inside the DeepMind to improve weather forecasting. Here's Peter. Peter, welcome. Thanks. Happy to be here. All right, let's talk weather forecasting. I want to start maybe by having you school me a bit on something that I realize I don't know, which is how do we do weather forecasting? Like currently, and maybe a little bit of history. Have there been major shifts technologically in how we forecast weather historically? So maybe walk me through the history, such as it is, of how we forecast weather. And then like, what do we actually do today? Yeah, so I have to admit, I'm actually a relative newcomer to the area of weather forecasting myself. So we had gotten involved in this several years ago, and it was sort of built out of a research program that was trying to model complex simulations, including fluids. And the Earth's atmosphere is a fluid. And one of the big challenges that we were sort of interested in exploring was modeling the atmospheric fluid, which is weather forecasting. So I should say that I sort of have gone through this journey of learning about weather forecasting. So the stuff that I'll say hopefully hopefully it's accurate, but forgive me if I make mistakes. So I think my understanding about the field is that really a lot of the historically weather forecasting was very important for agriculture and sort of other use cases that were very important for kind of day to day life. But I think it was about maybe 100, 150 years ago that you had agencies or euros that were starting to do like marine forecasting or like kind of more systematically collecting observations systematically and treating it as a science. But then probably about 50 years ago or so you started to see the emergence of like large government public weather agencies. So I think NOAA in the US was formed in the 70s. I think ECMWF, the European Center for Medium Range Forecasting was also formed in the 70s. And these are two of the big prominent weather bureaus. But most governments have a weather bureau and it's sort of weather has traditionally been viewed as a public good. So this is something that they collect tax money and then fund their weather service. And the idea is that a lot of the, you know, it's not only is forecasting the weather useful just again, like what are you going to get aware of? Have an umbrella or wear a coat? But for things that are more like, you know, there's a dangerous storm coming flood extreme heat, extreme cold, those types of things and also a lot of sort of decision making, like agriculture, energy, transportation. And I think in general, weather forecasting is traditionally being understood to be something that's a very good investment on a dollar. So the public tax money that's invested in the national weather bureaus has a significant economic returns on those investments. So that's kind of the, I think my understanding of the history of the kind of standardized or official weather forecasting business in a sense. Maybe one more thing I can say about the way to think about the industry. I think my understanding is you can kind of think about the weather forecasting industry is divided into, it's almost like a pipeline really. There's this sort of government official weather bureaus that are issuing these like global forecasts that are predicting all sorts of weather variables, but typically more at like a coarser spatial resolution. And then you have this full, this like big post processing chain where they take the like sort of base forecasts and then they specialize them for different use cases. So for example, like when you look at the app on your phone and you see, you know, the chance of precipitation, that's not coming directly from NOAA. That's coming from other intermediaries that are like taking, you know, local weather station data and other historical information and trying to kind of like tweak it and improve it and make it especially useful for your use case. And you see that sort of an energy and all sorts of other applications of weather forecasting. Is there a corollary to how that has worked historically to like what's what's happening with LMS today, not to jump into the AI stuff too early, but just in the sense of like the is the global forecast. So let's say NOAA's forecast. Is that a big mega model that spits out this one big forecast? And then what people are doing in the post processing world is saying, okay, I'm going to take that model, but then I'm going to like fork it is the wrong word, but I'm going to, I'm going to fork it and add a bunch of additional data into it to try to make it better at a smaller spatial resolution. Like I'm just trying to picture what it actually is. Yeah. So I mean, I didn't say much about that. Where your actual weather fork has comes from in terms of like technically. So maybe if I say that, then maybe it will sort of open that the answer to that question. So it's true. Again, fluids like the atmospheres of fluid and in physics, we have fluid equations called the Navier-Stokes equations. And that's they govern fluids like at all scales like from the largest scale structure of the universe, which actually turns out to also be a fluid down to like what's happening and you know, in your blood basically is there's turbulence that determines sort of how your blood flows and it has kind of important implications. Now all things in between that, right? You have like weather and stream flow and other types of things like that, all fluids. And what happens like engineers have figured out that well, so this the flu fluids are very complicated to simulate. So in order to simulate them accurately, they need to approximate the solutions so they can run them on very large computers. Because they're so complicated they would never run on a computer natively. You have to sort of break up the computation and approximate certain things in order to actually model everything that's happening for example in the atmosphere. So that's called numerical weather prediction. The numerical is just saying that they're making a numerical approximation to these Navi-Stokes equations. And traditionally it's been run on supercomputers. So like a lot of the big supercomputer centers have either, you know, do a lot of weather forecasting or even built to do weather forecasting. And in many ways it's been sort of a triumph of science and engineering that we've been able to like decade on decade predict not you know, one day, two days, but like 10, 12, 15 days into the future. It's hard to even sort of imagine like the scale of that type of predictive accuracy was sort of unimaginable 100 years ago. People just didn't think like in two weeks we can kind of know what the weather is going to be. That's crazy. That relies on knowing like what's happening on the other side of the earth as the sort of prevailing winds carry the you know moisture and the temperature and all that kind of stuff. So. And I imagine there's like an exponential increase in complexity the further out into the future you get just because like the there are a variety of possibilities of what actually happens today. And each one of those needs to be taken into account when I'm trying to predict what's going to happen tomorrow and so on and so forth as you move into the future. That's the butterfly effect, right? It's that a butterfly might or might not flap its wings. And then that will determine like a week later whether there's a hurricane or not. Right. So the idea is that little tiny changes or little tiny effects or lacks you know, missing effects will cause could cause huge changes in weather over time. And that's exactly right. So the fluid the atmosphere fluid is thought to be chaotic which means that that's sort of the definition of chaoticity. It's that little tiny changes can have huge large impacts later. That's what makes it so hard. And there's coupling across scales again the butterfly flaps its wings but then you have like up to the top of the atmosphere stuff is happening now. That is exactly why it's very very difficult to sort of you know, find solutions to the exact equations that govern the atmospheric fluid. It's so we have to make approximations and we use supercomputers and we have all kinds of tricks. I should also say like another important thing to recognize is that when you generate, when the weather forecast has been generated, the actual prediction of the future is only half of the process. That's the second half. The first half of the process is figuring out what the weather currently is. So if you we have satellites and we have weather stations and balloons and ships and all sorts of information that are taking measurements of like what the weather is all over the earth. But again, using the butterfly as an example, you would have to know like where every butterfly is in principle to act to perfectly forecast the weather. So if you sort of think that through you realize that it's not really weather forecasting is always going to be fundamentally uncertain to some level. We're never going to be able to make perfect observations of the weather everywhere on earth with the precision required to perfectly predict the weather a week out. And so when weather forecasting that's why you have a chance of rain versus like it's definitely going to rain and you have like a range of temperatures, especially as you go out in time. And that's again sort of what makes weather forecasting so hard. So the first step in weather forecasting isn't actually predicting. It's taking all the satellite data and all the stations and all the different observations and estimating the current state of the weather across the earth. And once we have that estimate, then we can make the prediction with the supercomputer. And that's that that second part is what our team and a lot of the teams in the field who are working on AI based weather forecasting have been especially focused on. But my guess is that over time we're going to see other parts of the weather forecasting process being, you know, having having more and more AI methods that are coming in and trying to advance them. Before we get into the AI methods, I, it seems like we have generally even pre AI, we've been getting, I mean, you tell me if the curve has been linear or exponential or flat, but like it seems like there's been, I don't know, fairly linear improvement in our weather forecasting ability for decades. Like we're getting more precise. We are also getting better predicting further out into the future, as you said, like, you know, a week, two weeks, et cetera. To the extent that that's true, I'm sure it's all these things. But like how much of the improvement that we have seen historically has come from, I don't know, a, as you said, just like having better ground truth data on the current state of the weather, be more compute, as you said, has been running in supercomputer. So we get power more and more powerful computers. We can just run more and more complicated Navier-Stokes equations or see additional tricks, basically, that allow you to like do better predictions without adding more compute. Yeah, that's a great question. So I'll just admit, I don't know the answer to that. I think all three contribute. So I can say on the first one, data, yeah, we like, there's better satellites that are flown and there's more better systems for collecting balloon observations or these different sort of things. So we're definitely getting better data and we know that that improves the quality of the forecast. We're also getting better models. That's definitely true as well. We're building bigger supercomputers. They can operate at finer resolution. Just I think in the last less than 10 years, the ECMWF, which has the best weather forecast, they increased the resolution, meaning that they had finer detail and space in their forecasts and that allowed the forecast to be more accurate. So you see both like adding just raw compute power, but also improving the quality of the models and the approximations can also, you know, has also made a pretty dramatic impact. And I think that sort of blurs into your third category of like other tricks. I think in general, you have, you know, like without getting into the details of how the numerical models work, you can kind of think about them as a backbone that's making a sort of general prediction at a course scale. And then you have a lot of other parametersations and trickery sort of under the hood and making finer and finer green predictions and also updating the backbone to be consistent with the fine green. And those are all being advanced sort of in parallel and like the teams, these, you know, engineering teams and scientific teams are sort of working together to make these better. The last thing I would also say too is again, going back to that like post processing part of the weather forecasting pipeline. It's not just again that like these large, you know, NOAA and ECMWF and these other large agencies, it's not just that they are improving the forecast, it's that other parts downstream and post processing are improving what they're doing. So actually the first advent of like AI and machine learning in weather forecasting or at least some of the earliest was not like trying to overhaul the whole weather forecast process itself, but making, you know, using more and more like statistical methods and linear regression and nonlinear regression, neural networks and other types of earlier machine learning techniques to improve the not the base forecast, but like the specific application. So maybe we can calibrate your, you know, chance of rain better if we have a slightly better downstream model. The grid is changing fast data centers, electrification and extreme weather are driving a surge in energy demand. Utilities can meet the moment with existing resources by activating energy customers and their distributed energy resources to create predictable and flexible capacity with uplights integrated demand stack. Uplight coordinates energy efficiency rates, demand response and virtual power plant programs into one cohesive strategy to strengthen grid resilience, improve energy affordability and make progress toward clean energy goals. And how Uplight is helping leading utilities harness over eight gigawatts of flexible load at Uplight.com. Catalyst is brought to you by antenna group, the OGs of PR and marketing for climate tech. Is your brand a leader or challenger? Are you looking to win the hearts and minds of customers, partners or investors? Are you ramping up your new biz pipeline? Are you looking to influence policy conversations? Antenna works with leading brands across the energy, climate and infrastructure space to do all of this and more. If you're a startup investor, enterprise or innovation ecosystem that's helping drive climate's age of adoption, antenna group is ready to power your impact. Visit AntennaGroup.com to learn more. So okay, so we've been improving. We've been in more recent years applying sort of the earlier versions of ML to continually improve. I guess I'm curious to your perspective what the biggest gaps are. Obviously, we don't have the ability today to generate a perfect forecast three months into the future. You could always get better. But apart from just that element of it, are there any areas where you feel like actually there's a real, it's really hard to do X. Is it like precipitation is a bugaboo or something else? Yeah. I mean, I think there's sort of two ways to answer that. You're always going to be limited by the quality of your data. So if you don't have good data about something, it's going to, you know, you're just bad at, you know, garbage in, garbage out sort of thing, right? So these models take an estimate of the state of the current weather and then predict what's going to happen if your estimate isn't very good because your raw observations weren't very good. You're not going to get a very good forecast. So in improving, just collecting more data and using the data you have collected to form a better estimate of the current weather, that's definitely going to always improve things. So that's sort of a known gap, right? Now, we don't know exactly what the ceiling is. We don't know, like, if we do this satellite or that station observation, how is it going to improve things? We might have an idea, but we don't always know. And sometimes we have to just test it out. But the other thing I would say is that you have different features of weather, which are harder or easier to predict. So an obvious one is temperature. So temperature is sort of very smoothly. If you look at a map of the temperature across the Earth, it's not up a mountain. It's going to be colder and in a valley it'll be different. But it's sort of very smoothly. What doesn't very smoothly is precipitation. So a violent thunderstorm that sort of emerges out of nowhere and there's high wind and low pressure and all this kind of stuff, that where exactly that front will be, where exactly the precipitation will happen, what exactly the wind and these kinds of things are much, much harder because the detail of everything's happening at a finer scale. Where even if you look at a radar map, you can see this. It's not like precipitation sort of very smoothly over the Earth. You see there's a little thunderstorm right there or there's a rain and then a few miles over, nothing. So that type of very high resolution, complex patterns of precipitation, for example, wind as well, those are much harder to predict because you're effectively predicting a lot more information. You can't just sort of summarize it by saying, oh, every 25 kilometers or whatever, the temperature is this and then everything else is just kind of interpolated in between. You have a lot of stuff happening at a much finer scale. A lot of our models even capture and then we have to do a secondary step to try to resolve those finer details. All right, so let's talk about AI then. You mentioned this is one thing, right? Whenever we talk about AI, quote, quote, fingers applied here, right? There is ML as a subset of AI, a related entity. We've been doing ML already. So I guess the first question that I have is, as you think about leveraging AI now and into the future for weather forecasting, what version of AI are we talking about? Like what version or versions are you actually? What are the actual capabilities and or model structures that are interesting here? Yeah, that's a good question. These days, AI is a pretty capsule term. I find myself just using the word AI just to mean a lot of different things because I think it's kind of easier and usually people kind of know. But the way I would say it is the difference between AI and so machine learning is sort of the statistical inner core of AI. It's trying to capture, it's taking data and trying to capture the patterns through a training process and then kind of use some inductive assumption. Like what we've seen in the past is going to be similar to what we see in the future. Modern AI, I think, is a broader family of things. It sort of involves agents and interactions with them and a lot of language models are often sort of associated with AI. What we use in our weather forecasting models and a lot of folks out in the community are using it as this field is advancing and this AI based weather forecasting is developing, we're still mostly using fairly traditional machine learning, supervised learning. So supervised learning just means you take a data set that has a pair of examples, an input example and a target example and you train a model to try to take input examples and accurately predict the target examples. And so if you think about weather as, again, like I said before, you're estimating the current state of the weather and then the next step is to predict what's going to happen next. It can be a supervised learning method can be trained to do that and that's what we're doing. And our models and most folks that I see are doing as well. And then the only extra step is that just makes one prediction but then we feed the output of the model back into itself and then we have it make another prediction so the output becomes the input and then it generates another output and if you just sort of chain those steps together you get your first input and then you get a sequence of outputs that represent future steps in time. And so supervised learning we use a lot of these days with in terms of AI architectures, transformers and graph neural networks are what we use, people use convolutional neural networks but I don't feel that neural network architectures these days tend to be the sort of exciting part it's usually more of like the training and the sort of data how you handle the data and that kind of thing. So you mentioned transformers because I guess like if we had been having this conversation five years ago, right, I imagine that you still would have told me about supervised learning for example, right? Like that wasn't that that's not new. Transformers had been invented by that point but like had not been, you know, broadly applied the way that they are today. So what is it that like this new wave of AI unlocked by things like by by transformers and convolutional neural networks and so on like what is that enable above and beyond what you would have been able to do five years ago? Yeah, that's a good question. So the way I look at it is so transformers are very similar to graph neural networks. They both of them are so actually let's let's take this back. So we used to use often convolutional neural networks and the idea here is it learns a little function that's sort of local in an image and then it sort of applies that same function everywhere and then you stack up sequences of these layers and that eventually lets you like one the information on one side of an image communicate with the information on the other side of the image because it's a hierarchy. A transformer architecture allows you to make a direct connection between the information on one side of the image and the other side of the image the same way that graph neural network does and I like to think about it like graph neural networks because what it's like saying is well in a graph you have nodes and you have edges or connections between the nodes and a longer connection between nodes is for nodes that are farther away and shorter connections are for nodes that are closer. So if you use the graph neural network analogy to describe the older convolutional networks, it's like the graphs are all small. They're all kind of everything is kind of close. It's like nearby in an image. Graph neural networks allow you to choose how far away you want information to interact and in transformers it can be understood as a graph that has connectivity across any spatial scale. So in language models the way a transformer works is it says when I want to make predictions about the next word I want to be informed by the most recent word but also every word that has happened in the text before and that's important because in language the next word is not predicted by just the previous word. It actually is predicted by stuff that happened earlier in the sentence or in the paragraph or in the book and so the ability to make to have information across large spatial scales that interact with one another that ability allows you to it opens sort of new patterns of computation and allows you to represent functions that have traditionally been harder to represent but it allows you to make better predictions of the next word or the next, in our case you can kind of think about it as the next spatial point or a far away spatial point and that allows the model to be more flexible and capture richer functions. There's a good comparison there. What do you think of as being I guess you just described something that is similar about what you can do in weather forecasting thanks to a transformer architecture to what you can do with large language models which is what most people are going to be most familiar with in the new wave of AI. What's different? So that's a great question. I think what's interesting is that the way that so in language the text is understood to be as treated as a sequence it's like token, token, token. We are also modeling sequences in weather but we're not allowing our models to look too far back in time. Because weather is actually different from text in a fundamental way, in fact most physical processes are, they are what's called Markoff in that the most recent state of the system determines the subsequent state. Like I said in text that's not the case. Right now I'll just pause. You didn't know what word I was going to say next. It kind of depends on the context a bunch of words behind it earlier. With weather forecasting in principle if you know exactly what's happening right now you can fully predict what's going to happen next. You don't need to look further back in the past. So we actually use transformers not to model the spatial, the interactions in weather over time like the sequence of text but in space. So in text you actually don't have a sense of spatial structure. You just have one sequence of text. It's this word, word, word. I mean you read you to see word, word, word. In weather you have spatial structure. You have weather all over the earth at the same time. And it's all, especially the close weather, it sort of determines and can be used to predict what's happening next at our current location. And so we use transformers and graphinal networks to capture the short and long range spatial dependencies. And those interactions between what's nearby and what's about to happen next are what determine weather and that's how we sort of make these predictions. But one thing I should also add is that similar to how I was saying earlier that it's kind of impossible to measure everything that's happening on the earth in the fine detailing weather. You have to make approximations. These models do too and this actually brings us to a very fundamental difference between how AI models are making their predictions and how traditional models are. So AI models can take the statistical structure of weather patterns. So for example if I'm looking at a hurricane that's traveling over the earth, right, in a traditional model the way it simulates that is in very fine detail it kind of figures out like, what's the pressure and temperature and the wind and the moisture and what are those things? What's going to happen next is determined strictly locally. AI models, because they can look at a much larger spatial range, they can use what traditional methods use or they can use other approaches because you know when you look at a hurricane it almost looks like an object sliding over a globe. Right, that's not how a traditional model models it. And we don't really understand how the AI models forecast it but they are capable of treating the hurricane as almost like a large macroscopic scale object that is moving because they can see all the structure of the hurricane and they can see you know sort of what's happening in the recent past. They have a spatial awareness in a way that the old models didn't. Yeah, and we don't know. It's a really interesting area I would say of like sort of the science of how AI works to understand exactly how they see the world in that sense. It seems like to me on one hand a much harder problem than an LLM because you've got the entire physical world and the data is sparse as you said and there's lots of complex interactions. On the other hand it's determinative in a way that LMs are not right like there is no correct next word necessarily. It's a best guess as to what the best next word should be but in the case of weather forecasting there is a correct prediction to make and there is a universe of historical data that you can draw upon to do it. So I'm back and forth on whether this is a harder problem or an easier problem than like making a really good LLM. I think you'd have I think depending on what where your allegiance is live that'll probably be I think it's probably more of like an opinion question but yeah I think you're absolutely right. So the way I think so it's the one thing I just would like I might say a different way is that it's still like I said before it still is fundamentally uncertain from the standpoint of the information that's available to the model. Now yes like the physics is truly deterministic underneath but because the model doesn't again see the butterflies or see the little fine scale stuff from its perspective it actually is a random process right because it doesn't if it doesn't know whether the butterfly flapped its wings or not how could it know whether the hurricane is going to form or not right. Now so from the perspective the information available to the model it is also an uncertain random process to some extent but I yeah I think what you're saying is exactly right so I think the underlying structure of text is random in different ways it's it's like again I can I'm gonna pause and then I'm gonna say a word furniture right like weather doesn't work like that right like it doesn't it doesn't just have something pop into its head that's completely different than what's historic like present in the historical record like you pointed out so I think that the structure of the uncertainty is different in weather it's more constrained in a way so in text it's you can imagine you know if you're watching a video you're watching a movie and someone's gonna come through the door you have no idea what they're gonna be wearing as a shirt right it could be wearing blue at anything there's like no way to predict it and whether you can always have some idea you just don't know the fine details on the flip side weather is an extremely complicated process this fluid chaotic fluid system is has you know interactions from small scales to large scales and it's happening all over the earth at once so in some sense instead of just predicting the next word you're predicting like millions of variables at once so I think you can kind of like it probably better like it was you know to have this as like a debate over beers with your friends in your in their LLM lab rather than like something that can be adjudicated just on the basis of these things I guess is one other question on this sort of comparison to LLM world notoriously like the LLMs are trained on the internet right your train your training data set is like all words on the internet and so that's one of the reasons that they've been among the first sort of major AI models in this new wave to commercialize is because there is this gigantic body of training data that you can draw upon now we're hearing lots of folks who are in like robotics world for example like facing the challenge of there just isn't an equivalent data set you can like try to train on YouTube videos or whatever but it's not quite the same thing in the case of weather forecasting it seems to me like in theory you have an incredible historical you can look at every historical weather measurement right if you had access to that data you could if no one know what does right like every input data point they ever took historically and then and then the subsequent next measurement which dictated what happened after that I would think that would be an incredibly rich training data set am I well two questions am I right about that and is that actually available yeah that's that's that's that's a good question so the first thing I would say is actually it's not just language right like the the first being visual neural networks were built were you came out of after ImageNet a big core corpus of image data the first language models like even you know a decade ago they were starting to build large text databases protein folding big databases of protein so actually you see like you know AI and machine learning are still very very sensitive to the availability of high quality data and and large amounts of it and you kind of like you know one of the best ways to advance the field is to go collect high quality data and make it sort of standardized and available so for weather I think that there's there's sort of good things there's good news and bad news so for what one thing is we we were very fortunate when we started I think all the all the folks who were working on AI weather forecasting have benefited tremendously from work that was done by the ECMWF the European Center for Meteor Research Forecasting they built this data set called era five and they've been building these you know era five I think was the fifth generation of the era data set it was a record of the of Earth's weather going back for decades and I think was originally released going back into the 1979 and then they actually opened it to like back into the 60s and they weren't they didn't design this data set to be supporting machine learning I think it was more to just you know have an authoritative record of the climate on Earth over you know year on year and it set like a six hour resolution 25 kilometer spatial resolution so it's very very rich it just happened perfect for machine learning for weather and it was just a really well curated data set the folks ECMWF are just brilliant and sort of organized and systematic and they had made this available and it allowed a lot of people to sort of you know stand on the shoulders and build you know great new AI methods now one thing is though because we had different satellites and different stations over time it's not actually all the data set is standardized but it's not derived from the same underlying observation so the quality going back in time actually gets worse in terms of it's not as accurate of a record of weather just because again as I said before the the input data wasn't as good right now the other thing that's not it's not great about weather is that whether it takes a while to happen so like we have to sort of just wait for more weather data to like for more weather to happen to get more data right like weather data we have so far we're sort of stuck with it now like tomorrow we're going to have one more day of weather data but like when our models are taking six hour steps we have to just kind of wait so you sort of like we got a lot of data but at the same time there's not much room to get more of the same kind of data and I think we and a lot of others are now looking to more unusual or underexplored sources of data to support building richer better models right like is there like a distributed network data like like in theory if you had access to uh I don't know everybody's cell phone everybody's iPhone there's probably a but you know there bunch of sensors in the iPhone like presumably you could pull something from that though it has some signal for you yeah yeah you can geek out on all these things like the way my my favorite one is like you know I have a video doorbell it's just there and watches weather all day right like cars your car right like it's when you're you got the you know it's got a thermometer in it it's got like your windshield wipers so some of them now are you know they they're rain sensing so it's sensing rain or it's like the lights go on automatically when it's dark so like they're sensing whether it's cloudy or these kinds of things so like I get very excited about possibly using all these kinds of things the other one that's even weirder is like you know people will go on and they tweet about the weather or they uh it's like talk about the weather on you know social media and like those types of observation those are still observations we don't know if they're very good but there's a actually a very wide range of pretty unusual underexport data sources but even before we get there like I think you can start to think about there's a lot of companies that are trying to build very cheap weather stations people can put them on their roof like these kinds of things could really help both with the kind of core weather forecast and probably a lot of the applications that people want to use weather for so my takeaway from that is that despite what I said there being this like amazing historical record of every weather measurements it's ever been taken you still feel kind of data poor right or like training data for I guess you're always data poor right like that's sort of the the story of modern ais you're basically always kind of data poor because because it one of the most incredible facts about modern ais is just how well it scales with data more data just means better models and you know I was a person with very skeptical of this I didn't think that it was going to scale like this and I would make my sort of logical arguments but it turns out I was wrong and I think a lot of people were wrong and the the folks who really understood that data could really add value even at extremely large scales were right and pursued that course and brought us to where we are today interesting okay so I guess I want to I want to finish by talking about what might come like if you if you draw a line forward a few years into the future and you pick pick your time three years five years 10 years whatever it is and and you and everybody else is working on AI weather forecasting succeed where might we be like what might be possible in a few years that's not possible today yeah I mean I think that I think there's a lot you know weather affects everything and it's you know it has you know different things energy is obviously a very very sensitive to weather some things are you know only kind of you know loosely affected by weather so one thing I would like to see and I think is very exciting is a wider range of use cases of weather so for example like we know that even people make different choices about you know what to put in their refrigerator or like you know what clothes or whatever these different choices are going to go on a trip what they expect the weather to be I think that you can start to make more subtle and informed kind of guidance and suggestions for people on the basis of what a more accurate weather forecasts and that's kind of like at the consumer level but I also think that at the kind of you know industry level there could be a huge opportunity so for example in energy you know we see there's you know if you have a wind farm or a solar farm you're making forecasts about the weather and then you're kind of using that to figure out like how you know if you're going to have energy to sell and how you price it but I have a feeling that there's a lot of headroom a lot more to be gained in how we you know plan about how our you know how to operate our electrical grids how we predict what the electrical demand is going to be is it going to be hot is it going to be cold is it going to be humid or the air is carrying more you know mass which you know requires more energy to heat cool I think that we just haven't really scratched the surface of the opportunities I think that supply chains and logistics and you know even just like lots of choices that you know driving and these types of things I think really could be better informed by better weather forecasts and I don't think we've even begun to kind of get into this and we need to see the quality of the forecast get better and more customized in these use cases to to start unlocking that the other one I was obviously is just you know better crisis handling crises like you know some of our worries and work on tropical cyclone forecasting you know I'm very proud of and we think hopefully with better forecast we can warn people earlier more accurately we can in some cases where there's a disaster like a wildfire maybe we can even go and intervene and try to stop it before it's completely out of control so again this is not something that we can do today but I'm you know very optimistic that and in general maybe this is coming through but I just we you know my team and I really believe in the power of technology it can have a lot of positive benefits so we often are trying to look for ways that we can you know put technology to best use and sort of you know leave our kids with a world that's better than the one that you know we grew up in our period this was super interesting and useful for me both in the context of thinking about like weather forecasting and just like understanding like the where AI well how AI is getting applied in various industries what the challenges are what the opportunities are so really appreciate your time sure yeah it was great to have to be here and your questions were great too by the way it was really fun Peter Pataglia is the senior director of research at Google Deep Mind's sustainability program this shows a production of latitude media you can head over to latitudemedia.com for links to today's topics latitude is supported by Prayly Adventures this episode was produced by Daniel Waldorf mixing and theme song by Sean Marquand Stephen Lacey is our executive editor I'm Shale Comme and this is Catalyst",
    "release_date": "2026-01-02",
    "duration_ms": 2680000,
    "url": "https://traffic.megaphone.fm/PSMI6195786935.mp3?updated=1767320529",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2026-01-16T02:03:47.724846"
  },
  {
    "title": "The gas turbine crunch",
    "description": "Demand for turbines is growing fast, but so are lead times \u2014 causing serious headaches for developers and even cancellations. In Texas, one of six cancelled projects cited \u201cequipment procurement constraints\u201d as the reasons for its withdrawal.\u00a0\n\nLead times are stretching to four years and sometimes more. Costs are climbing. So what\u2019s behind the bottleneck?\n\nIn this episode, Shayle talks to Anthony Brough, founder and CEO of Dora Partners, a consulting firm focused on the turbine market. Shayle and Anthony cover topics like:\u00a0\n\nWhy previous boom-bust cycles in turbine manufacturing have left the industry skittish \u2014 and why Anthony says leaders are approaching this new peak with \u201cguarded optimism\u201d\n\nThe competing demands on the turbine supply chain, including from power, oil and gas, and aerospace industries\n\nHow lead times have ballooned to four years and, in some cases, even longer\n\nFactors affecting the market beyond load growth, like renewables, storage, affordable gas, and coal retirements\n\nHow investment in tech innovation has raised turbine efficiency\u00a0\n\nHow the industry is preparing for hydrogen \u2014 if hydrogen scales up\n\nResources:\n\nLatitude Media: Engie\u2019s pulled project highlights the worsening economics of gas\n\nLatitude Media: High costs, delays prompt withdrawal of five more Texas gas plants\n\nPower Magazine: Gas Power's Boom Sparks a Turbine Supply Crunch\n\nMarketplace: Will we have enough natural gas turbines to power AI data centers?\n\nCTVC: \ud83c\udf0e Gas turbine gridlock #236\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.\n\nCatalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting BloomEnergy.com.\n\nCatalyst is supported by Third Way. Third Way\u2019s new PACE study surveyed over 200 clean energy professionals to pinpoint the non-cost barriers delaying clean energy deployment today and offers practical solutions to help get projects over the finish line. Read Third Way's full report, and learn more about their PACE initiative, at www.thirdway.org/pace.",
    "summary": "Coming up, it's due time we talk about the gas turbine market. For example, Solar Gas Turbins, a division of Caterpillar is a significant player in the small gas turbine market. They're for the oil and gas market. When you've got the aerospace industry ordering 40,000 aircraft, that's at least 80,000 gas turbines. Grid scale energy storage is sort of a gas peak or replacement product on the grid, right?",
    "transcript": " Hey everybody, this is Shale. Happy holidays. This week we are re-releasing a podcast that we recorded earlier this year on the gas turbine crunch. I will say that since that episode has been released, the crunch has gotten even crunchier and the big turbine OEMs are just as sold out as they were before, perhaps even more so. And you've seen lots and lots and more announcements of purchases and long lead times for all sorts of different kinds of gas turbines. We're getting broader and broader. Aerodirivatives are becoming a thing. Boom, supersonic has pivoted a little bit from building supersonic jet engines to jet engines that can be repurposed for data centers. So it's just as important as it was months ago. And I thought it was a really good conversation because people don't really understand and appreciate how the gas turbine supply chain works. With no further ado, here's our episode familiar with this year. Latitude media covering the new frontiers of the energy transition. I'm Shale Khan and this is Catalyst. What's your outlook on timelines? Do you think that the lead times just get longer and longer and longer? Are we at the peak there? It's going to get worse? Do we know? A good question. I actually don't think they're going to get much worse. I think all of the Williams are working like crazy to try and shorten up their lead times or at least make sure they don't get worse. Coming up, it's due time we talk about the gas turbine market. What if utilities could meet surging electricity demand with energy assets already in homes and businesses? Uplight is making this possible by turning customers and their smart energy devices into predictable grid capacity through an integrated demand stack. Uplight's AI driven platform activates smart thermostats, batteries, EVs and customers to generate shift and save energy when the grid needs it most. Learn how Uplight is helping utilities unlock flexible load at scale, reduce costs and accelerate decarbonization at Uplight.com. Catalyst is brought to you by Antenna Group, the communications and marketing partner for mission driven organizations developing and adopting climate energy and infrastructure solutions. Their team of experts helps businesses like yours identify, refine and amplify your authentic climate story. With over three decades of experience as a growth partner to the most consequential brands in the industry, their team is ready to make an impact on day one. Get started today at Antenna Group.com. I'm Shail Khan. I invest in early stage technologies and energy impact partners. Welcome. So it's a good time to be in the gas turbine business between the relaxation of emissions constraints and the rapid load growth that we've discussed in numerable times on this podcast before. Perhaps the biggest winners are the companies like Mitsubishi, Siemens and GE Vrenova who make turbines. Of course, one result of that is that they're pretty well sold out and they have a lot of pricing power. So it's an interesting moment where momentum is clearly flowing toward natural gas power generation, but it's also actually pretty difficult to build any more of it, especially in the near term. Anyway, it's a really interesting market and one we haven't really talked about here. So let's fix that. Directify the situation. I brought on Tony Bruff. Tony is the president of Dora Partners, which is an energy and gas consultancy specializing in what's going on with the gas turbine industry. Here's Tony. Tony, welcome. Thank you. Glad to be here. I want to start by you giving me a little bit of a recent history lesson on the gas turbine market. How has it been developing over the past? I don't know. You tell me what the relevant timeframe is, but a couple of decades. Oh, that's a good question. I mean, there's been a lot of dynamic change over the last few decades. I mean, it used to be in the 70s and 80s. There were pretty much just two major OEMs, you know, General Electric and even Westinghouse at the time, now owned by Siemens. But really, the number of OEMs have been, have gravitated towards three major OEMs, MHI Mitsubishi Heavy Industries Siemens and General Electric, or now it's GE Vrenova. There are other strong players in the market. For example, Solar Gas Turbins, a division of Caterpillar is a significant player in the small gas turbine market. So how has it changed? So how has it changed? It's really evolved not just in terms of the OEMs, but also there's been several column bubble periods. You know, there was a big bubble period in 1998, 1999 through 2001. And then the market basically fell off a cliff. And it slowly built back up to a really good set of years back in 2012. And then it kind of fell off again. And now we're kind of at another peak, but I would call today's peak more of a real market driven, realistic set of scenarios that's driving the market today. That's interesting that you say that. I mean, because I knew it was characterized historically by these sort of boom and bust cycles. And I think we've seen this in other sectors in the electricity market as well. We've talked before on this podcast about transformers, for example, where you have these very long lead times. And one of the reasons that there are still such long lead times is that transformer manufacturers have gotten burned in the past by building out more capacity and being oversupplied into a market that turned out to bust. And I had a sense that there's kind of a similar dynamic in the turbine world. But it sounds like you're saying this one seems like it's different. What drove those bubbles that then burst in recent history in the market? Was it over exuberance about new gas generation build that just didn't come to fruition or something else? No, actually, there's actually several different dynamics. And that's a really good question. If you go back to that first big bubble back in 98 through 2001, that was really being driven by an artificial demand created by Enron. I mean, they clearly were sending artificial signals to the marketplace that were driving up the cost of electricity significantly in several regions of the country, California, Texas and other areas. And that was also right around the same time that deregulation was coming into play. So those two factors created a lot of panic in the marketplace. And keep in mind, the large utilities in the 60s and 70s, everything was regulated. So they were pretty much just, they only built when they could get the public utility regulators to approve investment. But as deregulation came into play, deregulation came into play, everybody was just basically learning, okay, how do we make money now that there's regulated, deregulated and semi regulated markets to deal with across the country and even to a degree in areas outside the country in Europe and Asia, for example. So and then the Enron thing just created a significant, I would say artificial signal to the marketplace. So those two factors really drove a bubble in the market. And in a little bit of it was unreal. I would say at least half of the volume was artificial. Maybe to put a finer point on that then, it gives this ties to both the deregulation and Enron, which obviously are tied to each other, but is what was happening there, a lot of speculative development of what would be merchant gas projects that never came to fruition. I want to draw that distinction because what's interesting about today's moment is that like, you know, you, I don't know, I don't think there is a lot of new merchant gas being developed. Mostly what's happening is, it's having utilities saying we need it for because we need more capacity or its data centers and they'll be the long term off take on the project. So you're actually not like subject to the merchant risk. You are subject to the, will this data center ever get built risk, which is kind of a different thing. Well, that's true. But, but, but most of that activity was not merchant. Well, there were IPPs. There were a lot of IPPs and independent power producers that were speculating without a doubt. But, you know, there were a lot of orders that were canceled even by large, regulated and semi-regulated utilities like Southern Electric. You know, there were, they had a huge or set of orders and a lot of that stuff had to get either canceled or bought and then resold on the marketplace. It was, it was a real disaster for everybody when the bubble burst. So we'll get into the market today in a little bit more detail, but it, do you think that there is, given that history, given that there is some boom and bust and some cycles that the market has gone through, does that lead to a more conservative approach from, as you said, basically the three big OEMs that control what 70% of the market or something like that to expand capacity? Or do you think that they share the view that you express, which is, actually this one's real. I'm not too worried about being over extended. If I expand capacity now, I'm sold out through whatever it is, 2029, 2030. And so I should just build as much as I possibly can. Like, where do you think they are on the spectrum? Yeah. Well, I think there, there's guarded optimism, very guarded optimism. I mean, certainly all of the OEMs are investing in the future for new production capabilities, particularly Siemens and General Electric, GE, or GE, or you know, I should say. The other thing to keep in mind is about half of the gas turbines is ordered in the marketplace, aren't even for the electric power utility market. They're for the oil and gas market. And so the all of the supply chain that's feeding those three OEMs and others are also competing for supply chain resources going into the oil and gas market. And some of those OEMs are also delivering into the oil and gas market. So there is a lot of interesting dynamics going on. And it's important to look beyond just the power generation or the utility sector when you think about what's happening in the marketplace. Yeah. Can you say more about that? I think that's one thing people don't always appreciate on the outside. What does that supply chain look like and what are the big categories of sort of end markets that these products gets sold into? Right. Well, that's a great question. I have, basically, I described the supply chain for the gas turbine industry in four different levels. I call level zero is raw materials. So, I mean, you talked about transformers, while copper is clearly a big raw material when it comes to transformers, but for gas turbines, it's the super alloys, nickel based alloys, chromium, all those other expensive key ingredients, titanium, all those things that are involved in the raw materials for gas turbines. That's what I call level zero. Level zero, level one is actually manufacturing the raw pieces of product, for example, blades and veins and things of that nature that are being cast or forged. Level two is where they're actually manufacturing the gas turbine from all those components that were developed on level one. So, that's where the OEMs are producing what I call flange to flange gas turbine. And then level three, which is the fourth level, is where it all gets put together into a final package and delivered to an operator site, installed, commissioned aftermarket activities, all that sort of thing. So, all of those. And then when you keep in mind, levels zero and level one are also being impacted by the aerospace industry. You know, there's something like 40,000 aircraft in backlog right now in the world. Well, guess what? All of the same level zero material suppliers and all the level one forgers and casting shops and things of that nature in what I call level one, they're all supporting the aerospace industry at the same time. So, these, you can't look in isolation at the electric power utility market for gas turbines in isolation because you have to consider what's happening in the aerospace industry and what's happening in the power and the oil and gas industry. Because, as I said, 50% of the industrial gas turbines that are delivered in any given year approximately aren't even for the electric power utility sector. They're for the oil and gas sector. And in terms of the market dynamics today, I guess, obviously we have this booming demand for gas turbines in the electric sector, whether on grid or off grid, some people are doing gas turbines for bridge power, for data centers or whatever. But let's call that all in the electric sector, ultimately. Agree. Is the demand, oil and gas prices are low right now? Does that mean that there's low investment on that side? And so, most of the demand is shifting to electric power generation? Or is that not sort of how the cycle works on the oil and gas side? Well, that's a great question, Shale. And the good news is for those that are involved in oil and gas industry is by and large, most of the large oil and gas players have long-term thinking in mind. So they're making five, ten, seven and ten year strategy developments for strategy. Now, well, in any one year, they might reduce their order activity because the oil and gas prices are down. Absolutely, that's correct. But in the long run, oil and gas companies basically stick to a strategy that, an investment strategy that keeps them investing. And typically, what we see are what I call seven year cycles in the oil and gas industry. It'll go up peak at about year seven and then come back down, slowly come back down and then go back up again at another seven year cycle. And it's all driven by upstream activity for development of oil and gas, midstream for transmission and then downstream where you have a lot of LNG, refinery activity, all that sort of stuff. And all those things are somewhat independent of each other. So it does level out the market for the oil and gas industry a little bit, which means that the investment stays. And when you look at the midstream oil and gas market, most of the players midstream, they're making their money not on the price of oil and gas, but on transmission of oil and gas. So they're very much I don't like to use the word immune, but the sensitivity to the price of oil and gas is really low. They're still going to make money because everybody's still using the oil and gas, albeit maybe at a lower price, but they're taxing fee for moving the oil and the gas through the pipelines is still pretty robust and they're making their money. So they're investing. The grid is changing fast. Data centers, electrification and extreme weather are driving a surge in energy demand. Utilities can meet the moment with existing resources by activating energy customers and their distributed energy resources to create predictable and flexible capacity with Uplights integrated demand stack. Uplight coordinates energy efficiency rates demand response and virtual power plant programs into one cohesive strategy to strengthen grid resilience, improve energy affordability and make progress toward clean energy goals. Learn how Uplight is helping leading utilities harness over eight gigawatts of flexible load at Uplight.com. Catalyst is brought to you by antenna group. The OGs of PR and Marketing for Climate Tech is your brand a leader or challenger. Are you looking to win the hearts and minds of customers, partners or investors? Are you ramping up your new Biz pipeline? Are you looking to influence policy conversations? Antenna works with leading brands across the energy climate and infrastructure space to do all of this and more. If you're a startup investor, enterprise or innovation ecosystem that's helping drive climate's age of adoption, antenna group is ready to power your impact. Visit antennagroup.com to learn more. I want to talk about two primary things with gas turbines in the market, particularly for electricity generation, where I spend a lot of time right now. One is timeline and the other is price. We hear a lot right now in the news about both of those things. On the timeline side, we hear about folks like Givernova being sold out through 2029 with an order book behind that. They can sell as much as they can build, at least at the moment it seems. I don't know visibility into actual market pricing, but one interesting data point that you might have seen recently was, I think it was John Ketchum or somebody from Nextera said a decade ago, I could have built a new natural gas project for $750 a kilowatt. I think I'm going to get the numbers close, but not exactly right. Today, it would cost me $2,500 a kilowatt. I don't know how much of that is the turbine itself, but I'm interested in the relationship between how long it takes to get new turbines and how expensive they are becoming. Yeah, good question. That $750 was for a combined cycle plant. I think the $2,500 is a bit aggressive, but it's definitely up around 30 to 35% over the last five years. The price is definitely up. I track all of that very closely. And is it purely a supply-demand thing? Yes and no. Again, raw materials at level zero, raw materials are up everywhere. Even before all of the tariffs come into place, you were seeing demand on aluminum, nickel-based alloys, titanium. All of these things are all interrelated. Again, I'm coming back to the aerospace industry. When you've got the aerospace industry ordering 40,000 aircraft, that's at least 80,000 gas turbines. And they're all drinking from the same supply chain, for the most part. So no, it's not just supply and demand. It's also being driven from, well, it's of course, supply and demand is related to the cost of raw materials. So I don't want to discount that. But certainly raw materials is a big part of it. And if you look at some of the US government's tracking of producer price indices on all of these different elements, you'll see a pretty significant bump in the last three years that is very indicative of what you and I are just talking about. What's your outlook on timelines? Do you think that the lead times just get longer and longer and longer for a while? Where are we in the cycle of the lead times having been getting longer? Are we at the peak there? Is it going to turn back the other direction? Is it going to get worse? Do we know? The good question, I actually don't think they're going to get much worse. I think all of the OEMs are, in fact, I know all the OEMs are working like crazy to try and shorten up their lead times or at least make sure they don't get worse. And part of the reason why is that they're, I mean, customers are eventually just going to get weary, say, okay, we're just going to put things off because they're, as it is, they're putting down 15, 20, 25 percent non-refundable deposits. I mean, all of those things are very painful for customers. And these OEMs have been living through these things, these busts and booms before, and they don't want to upset their customers too much. So they're all working hard to at least flatten out the timeline and if not improve it. And I'm seeing signs of that across the board. Today's timelines are in the, or sorry, lead times are in the like four to five year range. Do I have that about right? I would say between 36 and 48 months, I suppose there are some OEMs that are claiming up to 60 months, but I would say on average, it's around the 48 month period. Got it. The other thing I'm curious about is size, right? There's obviously, you know, it's not a monolithic market. Even within power generation, there's different products that serve different use cases and at different scales. And I think the scale question is sort of an interesting one because the question is sort of, is what's getting built or what is being designed to get built? Large scale generation, gigawatt scale type of stuff is the fact that data centers driving a lot of this, changing the desired scale of the end customer and what does that mean for the products in the supply chain? Good question. Well, I actually look at the market drivers. I think there's at least five major market drivers. And in each one of those market drivers, small, less than 20 megawatt gas turbines, turbines, 20 to 100 megawatts are seeing a different set of dynamics. And then what I call jumbo size units, which are 150, 250 megawatts and above those I call jumbo units. They're all being affected differently, driven by the different market drivers. And I say there's at least five market drivers in the marketplace. One is grid scale battery storage. Number two, cold plant retirements. Number three, grid scale renewable energy expansion. Number four, the development of rapid development of data centers and artificial intelligence exploitation or expansion. And then just the availability of natural gas and its affordability is I'd say the fifth driver. And if you look at each one of those different drivers, those three sized units are all being affected differently. And if you want, I could actually walk through each of the different drivers and then explain how each one of those three different markets are being affected. Yeah, I mean, it's interesting that you described that right? Some of those drivers, I would think would be a suppressant on demand. So I agree. The growth of grid scale energy storage, right? Grid scale energy storage is sort of a gas peak or replacement product on the grid, right? Predominantly. So I would presume that suppresses the market to some degree. But maybe are you saying it results in smaller units being developed on the grid? Yeah. Or what's the dynamic? You're a great lead in the shell. Actually, you would think off just in generically, you think I've read, Oh, well, grid scale battery storage, that's got to drive down the demand for gas turbines. Actually, in some cases, the answer is exactly right, but not in all cases. So I mean, if you if you actually look at the market and what's happened with grid scale, I would say large jumbo sized units. Absolutely. They are being it's a negative, it's a negative dynamic. If you look at gas turbines, say 40 to 100 megawatts, actually, it's a it's an opportunity because there are several of the developers are counting on gas turbines to recharge or develop what I call hybrid systems that use gas as a when it's cost is low to spin up the gas turbine and recharge their grid scale battery storage. So they're not just relying on renewable energy to recharge their batteries. So, so and then when you look at the real small gas turbines, generally, they're not being quite as affected by the grid scale battery storage segment. But clearly, as you correctly pointed out, or you felt intuitively, yeah, large power plants, jumbo units, it's a negative, but for gas turbines, 40 to 100 megawatts, it's actually a little bit of a positive influence. And then I imagine right coal plant retirements, big projects coming offline, presumably get replaced with big assets, at least if you're trying to do one for one. So I assume it's that that is all things equal a positive signal for larger scale turbines. Yeah, for for coal plant retirements, it's really for all three segments, the less than 20 megawatts, the 40 to 100, and the large jumbo, it's a positive influence, but mostly for the large jumbo units. But interestingly enough, you see a lot of mobile power and peaking units being installed as support for the grid where coal plant retirements are occurring. Well, you see that in the context of some of your other drivers, right? Like, I know of some projects that are coal plant is retiring, we're going to replace it with like a big solar plus battery installation. And then we probably need some smaller scale peaking gas to supplement that. Yeah, right. It's like that kind of thing. Well, yeah, if you look at if you look at grid scale renewable energy, I mean, this the amount of grid scale activity is going up just explosively, it's expected to double in the next five years and the cost of a levelized cost of electricity for for solar power is way, way down. But so that has a negative impact on the large utility or jumbo size gas turbines. But but definitely it has a positive influence on mobile units, peaking gas turbines, just because when the when the sun goes down and the wind stops blowing, you know, you've got to have backup power. And those units, I would say from about 15 megawatts up to 100 megawatts are actually very good investments for, I call it, renewable offset. And when you mentioned the mobile thing, I mean, those types of installations, you don't necessarily, you're not looking for mobile generators. I think of the mobile generators as being a good fit for either like an off grid type application, you see a lot of this in the oil and gas world or for bridge power type situations where you're looking to this is what you see now where look, we need we need power now, because we're building a data center and the grid connection is going to take three to five years. So we need a bridge, but we don't need it forever. Am I wrong to think that that's where the mobile power segment ends up? Well, you're not wrong, but you're not 100% right either, because clearly, when it comes to data centers and artificial intelligence, mobile power and even permanent onsite power is as a backup to the and supporting the demand for data centers is a very strong influence on both mobile power and permanent onsite units. But believe it or not, there's a lot of utilities who will buy mobile units. They'll put and they'll locate them in a what they call a grid sensitive area. And over the course of five to 10 years, they'll improve their infrastructure and then they'll move those mobile units to another sensitive grid sensitive area. And so the mobile power has just been a fantastic opportunity for basically three companies, solar gas turbines, the Division of Caterpillar, the GE Vernova for their mobile units, and for MHI aero power for their mobile units. Those three players have done extremely well with mobile powered units for a variety of reasons, even in oil and gas. But for the reasons that you and I have just discussed in the last 10 minutes, absolutely. And I don't see that market going away at all. Yeah, if anything, it's getting super charged by additional use cases as we've just absolutely, which gets to that sort of 100% on. Yeah, which gets to that sort of the one that seems to be the biggest net new thing that's happening right now, but like is is a huge deal is all the gas turbines being developed for data centers, whether mobile or stationary, right? But you see like, you know, there's that partnership between Chevron and engine number one where they've they've secured gigawatts worth of GE Vernova turbines. They're going to go use those to develop a bunch of data centers. And then I'm not sure whether those are actually intended to be permanent or just bridge power. But like, that's one example amongst many. And it seems to me as the is the factor that's kind of tipping this market over the edge from just being a generally tight market to like a historically tight market. Yeah, well, you're making a good point. I mean, if you look at data centers, there's like 11,000 data centers serving the digital commerce and artificial intelligence community already around the world. And because they many of them have been around the average electrical loads around four megawatts. But there's like 1400 new data centers planned in the United States alone. And over 1000 of those are all large scale. They're going to need a lot more than four megawatts. I mean, some of those data centers, their electrical load is more than the community around them. Yeah, there's a there. I mean, it's just like a electric co-op. I think it's Susquehana co-op or something like that in Virginia that like, I remember seeing some some filing some regulatory filing, where they were projecting their load growth to like more than double based on purely a couple of data centers that are coming into the territory. Yeah. In fact, you you just touched on a good point that whole region around Virginia, Washington DC, that whole area, there's more data centers in that area than anywhere else in the world. Right. It's just a mecca of data centers. But but these dynamics are really interesting around data centers. And I don't think it's going away. I think it's you see people using artificial intelligence more digital commerce is just booming and it's not going away to me. Two things can be true at the same time. I think it can be true that this is the demand from the gas turbine OEM perspective, the demand is real. The market will buy an enormous volume of new gas turbines to serve these markets. And that's also true, by the way, of like utilities who are trying to manage interconnection requests and so on. Like it can be true that that is real. And also that we are in a speculative bubble on the development side because there will not be, I mean, as you correctly said, there are a thousand large scale data centers in development in the United States. And I will state categorically, I don't think there will be a thousand new hyperscale data centers in the United States anytime soon. I don't think there's actually that much demand for it. So like both things are true. There's all these there are cowboys out there trying to take advantage of the moment. So the challenge, of course, then if you are on the supply side, whether it's a utility or you're a gas turbine OEM, is how do I make sure that the buyers I'm signing up with are real? And that gets to your point of like these big non-refundable deposits. If you have all the market power, that's sort of how you take advantage of it. So it seems like they're doing the right thing in that regard, at least. Yeah. Well, and the other dynamic to keep in mind, Shell, not only that is, it's not just one of these market drivers that's making things happen. It's all five of these market drivers that I've mentioned, including the price of natural gas, which is very affordable in the United States. So when you combine all of these what I call market drivers, it creates a situation where these OEMs are relatively comfortable building out a supply chain strategy to support the market because they're not just relying on one dynamic. Back when I was executive at one of these large OEMs 20 years ago, we were basically counting on only one of those market drivers to happen and one of them didn't happen. And so it hurt our strategy. But now you have a situation where you have three to five key market drivers that are all paying the market. And so it creates a little bit of, I would call, risk comfort for the OEMs because they know it's not just one thing they're counting on to make the market move. Okay. Final question for you, I guess, is on the technology side, is there any I mean, these are pretty mature technologies. Is there any significant innovation that either we have seen recently or that you expect to see in the next few years? Like, will the market change as a result of technological innovation? Or is this just a, you know, rinse and repeat and stamp them out as much as we can kind of a situation? Yeah, good question. I would touch on two areas. First off, all of the OEMs have spent an enormous amount of money trying to get and they've been very successful in slowly increasing the efficiency of their combined cycle plants. I mean, it used to be combined cycle plants. Average efficiency was about 55%. And they slowly crept it up to 60. And then they kind of hit a dead spot and they couldn't figure out how to get above 60. And then they started to evolve developing their, I'll call it a very holistic strategy to the power plant. So it wasn't just the gas turbine. It was the HRSG. It was a whole, all sorts of different technical factors that they were lovers that they were pulling to try and squeeze more efficiency out of their power plants. And they crept it up to 60. And they got to 60 and a half, 61, 60.3. I mean, they're starting to push 62% efficiency and more. And I don't think they're going to quit because if you look at the, if you look at the levelized cost of electricity for, and that's a big factor that these utilities are using and assessing which OEM they're going to use, fuel is a big, big element in the levelized cost of electricity. So the more efficient and the more efficient, the more effective that the OEM is in convincing that customer that they have a more efficient unit and they even guarantee in it the better for them. They'll be more competitive. So efficiency is, it's not coming up by leaps and bounds, but it's a gradual increase over time. It's been quite remarkable, you guys, whether you got to hand it to all three of the major OEMs that they've been able to make some very significant improvements in efficiency, albeit very difficult. The second big area of technology development is, is converting their combustion systems over to using hydrogen. Now, I put all of that into a big, big set of quotes because while they're all working on hydrogen and they can, and they've all demonstrated to a degree some capability of operating in hydrogen, the biggest problem is where are they going to get it? The amount of hydrogen that you need to run one of these large jumble sized units, it's just an enormous amount of gas and where are you going to get it from? So while they're all spending a lot of money and their engineers are working very diligently and doing some fantastic development, I have some doubts as to whether the market will actually see a significant increase in purchases of gas turbines that actually are using hydrogen. But clearly they're all working on it. Yeah, it's a, you know, the bet is if we build it, they will come. If we build hydrogen ready gas turbines, then the market will show up for them and the hydrogen will be there. And of course, it's a dynamic market for hydrogen at the moment. So we'll find out whether that plays out well for them. But good point on efficiency, it's like a steady grind, but it adds up a lot over time. Yeah. Tony, this was awesome. Really appreciate the time. Thanks so much for joining. Thank you. Tony Bruff is the president of Dora Partners in Energy and Gas Consultancy. This show is a production of Latitude Media. You can head over to LatitudeMedia.com for links to today's topics. Latitude is supported by Prelude Ventures. Prelude Backs Visionaries accelerating climate innovation that will reshape the global economy for the betterment of people and planet. Learn more at PreludeVentures.com. This episode is produced by Daniel Waldorf, mixing and theme song by Sean Marquand. Stephen Lacey is our executive editor. I'm Shane Kahn and this is Catalyst.",
    "release_date": "2025-12-26",
    "duration_ms": 2338000,
    "url": "https://traffic.megaphone.fm/PSMI8487504796.mp3?updated=1766714087",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2026-01-16T02:06:59.919834"
  },
  {
    "title": "Will inference move to the edge?",
    "description": "Today virtually all AI compute takes place in centralized data centers, driving the demand for massive power infrastructure.\n\nBut as workloads shift from training to inference, and AI applications become more latency-sensitive (autonomous vehicles, anyone?), there\u2018s another pathway: migrating a portion of inference from centralized computing to the edge. Instead of a gigawatt-scale data center in a remote location, we might see a fleet of smaller data centers clustered around an urban core. Some inference might even shift to our devices.\u00a0\n\nSo how likely is a shift like this, and what would need to happen for it to substantially reshape AI power?\n\nIn this episode, Shayle talks to Dr. Ben Lee, a professor of electrical engineering and computer science at the University of Pennsylvania, as well as a visiting researcher at Google. Shayle and Ben cover topics like:\n\n\n  \nThe three main categories of compute: hyperscale, edge, and on-device\n\n\n\n  \nWhy training is unlikely to move from hyperscale\n\n\n\n  \nThe low latency demands of new applications like autonomous vehicles\n\n\n\n  \nHow generative AI is training us to tolerate longer latencies\u00a0\n\n\n\n  \nWhy distributed inference doesn\u2018t face the same technical challenges as distributed training\n\n\n\n  \nWhy consumer devices may limit model capability\u00a0\n\n\n\n\nResources:\n\n\n  \nACM SIGMETRICS Performance Evaluation Review: A Case Study of Environmental Footprints for Generative AI Inference: Cloud versus Edge\n\n\n\n  \nInternet of Things and Cyber-Physical Systems: Edge AI: A survey\n\n\n\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.\n\n Catalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting\u2060 \u2060\u2060BloomEnergy.com\u2060.\n\nCatalyst is supported by Third Way. Third Way\u2019s new PACE study surveyed over 200 clean energy professionals to pinpoint the non-cost barriers delaying clean energy deployment today and offers practical solutions to help get projects over the finish line. Read Third Way's full report, and learn more about their PACE initiative, at www.thirdway.org/pace.",
    "summary": "in large centralized data centers, versus edge versus edge edge, i.e. You need large data sets. Let's talk about the edge version first, which is essentially smaller data centers, still data centers, but smaller and more local. So as a result, that's why we're talking about these massive data centers for training. I'd say that we've been talking about edge data centers a lot.",
    "transcript": " A very brief word before we start the show. We've got a survey for listeners of Catalyst and Open Circuit, and we would be so grateful if you could take a few moments to fill it out. As our audience continues to expand, it's an opportunity to understand how and why you listen to our shows, and it helps us continue bringing relevant content on the tech and markets you care about in clean energy. If you fill it out, you'll get a chance to win a $100 gift card from Amazon, and you can find it at latitudemedia.com. If you're not a survey, or just click the survey link in the show notes. Thank you so much. Latitude Media. Covering the new frontiers of the energy transition. I'm Shail Khan, and this is Catalyst. We could be getting 80% of our compute done locally, and leaving 20% of the heavy lifting for the data center cloud. Of the 80%, I would say most of that will be on the edge. I think maybe on the word of 1% ends up being put on your consumer electronics. Coming up, could the age of edge inference blunt the big data center boom? What if utilities could meet surging electricity demand with energy assets already in homes and businesses? Uplight is making this possible by turning customers and their smart energy devices into predictable grid capacity through an integrated demand stack. Uplight's AI driven platform activates smart thermostats, batteries, EVs, and customers to generate, shift, and save energy when the grid needs it most. Learn how Uplight is helping utilities unlock flexible load at scale, reduce costs, and accelerate decarbonization at Uplight.com. Catalyst is brought to you by Antenna Group, the communications and marketing partner for mission driven organizations developing and adopting climate, energy, and infrastructure solutions. Their team of experts helps businesses like yours identify, refine, and amplify your authentic climate story. With over three decades of experience as a growth partner to the most consequential brands in the industry, their team is ready to make an impact on day one. Get started today at AntennaGroup.com. I'm Shail Khan. I lead the early stage venture strategy and energy impact partners. Welcome. Okay, so here's an energy question disguised as an AI infrastructure question. What proportion of the world's AI compute in 2035 will be cloud, i.e. in large centralized data centers, versus edge versus edge edge, i.e. on device? It's an energy question because the answer today is effectively 100% in that first category, cloud. And that's why we have this crazy dynamic in the electricity sector, and actually in the natural gas sector too, where hyperscalers and neo clouds and developers and real estate speculators and crypto miners turned AI companies and more are hunting for sites that can accommodate hundreds of megawatts or gigawatts of power. And the whole thing, as we know, is crashing through the electricity sector, affecting generation and transmission distribution, prices, now politics and so on. But there's a narrative that I've heard a number of times that if borne out would potentially present a very different future from the present. This is one where AI workloads, first of all, shift significantly from training to inference. And then where those inference workloads become highly latency sensitive and are also able to be executed in a more distributed fashion. And as a result, much of that compute and thus the power demand shifts from these big centralized data centers to the edge. That could mean it shifts to 10 megawatt data centers clustered around an urban core or an autonomous vehicle corridor, or at the limit, it could mean inference compute happens on device and centralized data centers fall back into a pure training position. Any version of this that takes significant share of the market would have profound implications for the energy question and for the grid. So it's worth exploring, which is what I'm doing today with my guest, Dr. Ben Lee. Ben is a professor of electrical engineering and computer science at the University of Pennsylvania. He's also a visiting researcher at Google. By the way, this edge AI infrastructure world and the energy implications thereof is super interesting to me as you will hear. So if you are building something in the space, please come get in touch. In the meantime, here's Ben. Ben, welcome. Great to be here. Thanks so much. I'm very excited for this conversation because this is the topic that I, in my energy circles that I travel in, I've heard Scuttlebutt about a bunch of times, but I've never actually spent the time to like really try to understand the topic basically being how much of inference compute might move from central cloud infrastructure to the edge and then how far to the edge, of course, being another question. I think we should start by actually defining those categories a little bit. How do you think about the categorization of like where compute can occur? Then we'll talk about each of those categories individually. Right. So even before we talk about generative AI, there, for classical compute, cloud computing in general, all of the services we love to and changed the way we live and work today, there are three levels generally I think about for compute. The first is massive hyperscale data centers, the ones run by Microsoft and Google and Amazon, hundreds of thousands of machines, massive facilities. That's what most people think about when they think about cloud computing. At the other end of the extreme would be personal devices, consumer electronics. So you think about your phone, you think about your tablet, your laptop, plenty of compute can happen there as well. There is a perhaps less understood middle layer or intermediate layer called edge computing. And edge computing really means that there are times where you don't want to go all the way to this remote massive facility and wait for the data to go out to that data center and then come back, you might want to access some compute that's a little bit closer to you, maybe in the same city, maybe in the same geographic region, that's edge computing. So they're still going to supply really capable of high performance machines, these servers. But you don't suffer those longer communication times or latencies that you might if you would go to that remote massive data center. My recollection is that there was, I think, okay, so the advent of cloud computing meant the build out of lots of big centralized data centers. There was a fair amount of conversation some number of years ago in the first wave of excitement around autonomous vehicles in particular that you might see a fair amount of edge infrastructure get built because of the latency tolerance requirement for AVs. I mean, I'm on the outside, so tell me if I've got the kind of narrative wrong here. Then it seems to me that because AVs were generally delayed or maybe the need wasn't as high like what we've got today, if you just look at the infrastructure today, it seems like the vast, vast majority of classical compute even, except for stuff that's sitting in like mainframes that companies is in the cloud and the big centralized data centers. Do I have that right? That's right. This is a decades-long trend. I mean, we've seen this progression, this adoption of cloud computing over the last 15 to 20 years, and there are a couple of reasons we are seeing that shift or we have seen that shift. The first is that computing in a massive data center run by the hyperscaler companies, they be these big tech companies, it's much more energy efficient. They know how to deploy these facilities. They know how to cool them and build HVAC systems efficiently. They're in current very small overheads per watt of compute. There's this industry standard metric called power usage effectiveness, or PUE, and that's the ratio of the power you're using compared to the power that's going to compute. Google's PUE is close to 1.1, which is to say for every watt going to compute, there's an additional 0.1 watts going to the old where it's a power delivery or cooling or whatever. That's really incredibly efficient. Most mom and pop data center operators, most enterprise data center operators don't get the scale and efficiency that these hyperscalers do. The scale also gives a second key advantage, which is the ability to share hardware. You buy the hardware once and you have lots of users sharing the same physical hardware. That allows us to drive the cost down, allows the hyperscaler operators to drive the costs down. That essentially gets a massive increase in efficiency. Most compute now is being done in these large data centers and in the cloud. Let's talk about the world of AI now, which is where all this growth in compute is happening. AI workloads, of course, divided into two major categories, one being training of models and the other being inference. I think we'll spend most of our time today talking about inference probably, but let's spend one minute on training. Is there any movement or argument that training should take place anywhere other than large centralized data centers? It seems very clear to me that the trend right now is just build the largest possible data center to train the largest possible model. Is there anyone who thinks that that might turn in the other direction? Some, but that really hasn't gotten much traction. The reason why we see most training happening in massive data centers is because of the scale. You need large data sets. You need lots of GPUs all closely coordinated learning the model parameters. The only scenario that some people have explored for training away from the data center is if you've got private data and somehow you want to refine your model or somehow fine tune your model with that private data. You don't want to share it with the hyperscalers. That has been primarily a research question rather than a production system that people have deployed. Okay. Let's assume then that the vast majority of training compute is still going to happen in centralized data centers. As it stands today, I don't know if you know the numbers, but just high level of all AI workloads, how much is training versus inference? I think the other big point people have made is over time, the proportion of workloads going toward inference is going to increase. The proportion of workloads going toward training may decrease as we sort of asymptote the next model or something like that. Today, it's mostly training still. I would agree with that. I think to first order, the training costs are historically what people have cared about the most because the data sets are massive and they're talking about these massive 1000 megawatt data centers for the training workloads. There was a study we did when I was visiting research scientists at Meta where we found that energy costs for AI were roughly broken into three categories. There's a data pre-processing aspect as well. That's about a third. The training is another third and then the inference or the use of the model is the last third. Clearly, those fractions are evolving rapidly. I would agree with you when you're saying that the training costs are probably flat lining. They were reading a plateau and how quickly they are growing, perhaps. If the optimism of our AI is to be justified, you're going to have to see inference costs go way up because that will be an indicator that adoption has gone up in a fairly significant way, both among individual users but also among companies and enterprise users. I think it's true to say that inference costs are large and potentially will grow very rapidly. Then we're getting to the crux of our question today, which is inference workloads, inference costs increase over time. Usage of the models increases over time. That's the presumption of everything going on in AI world. The question is, will that inference compute predominantly still take place in these big centralized cloud data centers or will some or much of it potentially shift either to one of the other two categories you described, edge localized or fully localized on device? Let's talk about the edge version first, which is essentially smaller data centers, still data centers, but smaller and more local. What's the argument for why that might happen and what are the limitations? The argument in favor of edge computing is mainly the proximity to the end user. We have been conditioned in an era before generative AI that when we access internet-based services like a search engine, we expect the answer to come back in the order of 100 milliseconds. That is the order of magnitude that we're talking about. As a result, to get those 100 millisecond latencies, oftentimes you require computation closer to the user. You don't have to travel across internet. You do not have to travel from the west coast out to the east coast and back again, the data. Get that answer back in a timely way. What is interesting with generative AI is that we are being reconditioned to tolerate much longer delays. If you use something like GPT or you use something like Claude or your favorite chat bot, oftentimes it's just thinking for seconds and seconds, maybe tens of seconds before it gets you the first token. The question there is to what extent we care about that latency and need that really fast responsive access to the answer? Yeah. I think we've been especially trained even further in that direction with the introduction of things like deep research, where even in the name, you think, well, of course that has to take time. It is deep research that they are doing. It's an interesting point that maybe we are becoming reconditioned to allowing more latency. The argument that I've heard for a while latency is really in a matter apart from just wanting search queries or chat queries to come back quicker is the next wave of applications for AI. Maybe we go back to autonomous vehicle world and things like that, where latency, making decisions in near real time does become really important. Robotics being another category that could be a major user of AI compute, but needs really, really low latency. Is that part of the argument for shifting some compute to the edge? Yes, absolutely. So the classic computer you mentioned, not honest vehicles, robotics, fit into what we call a cyber physical AI. So cyber physical systems are those that have a cyber component, a computational component, but also interact with the physical world. And once those interactions with the physical world arise, then we care about responsiveness because with that, it underpins safety guarantees and the ability to make sure that your robotic arm is able to respond quickly enough to hazards, your autonomous vehicles are able to do so. So I agree that there will be cases where we will need those really low latencies and that is going to require edge computing much closer to the user. So we have much shorter internet delays, network delays. I'm curious to understand the trade-offs here, right? Like I know with model training, there are technical reasons why you want all your compute as clustered together as closely as possible. You want every GPU as close to every other GPU as you can make them minimizing the copper between them or the optics or whatever it is that's communicating between them. And that for some reason that you can explain to me makes model training more effective. Is there a similar dynamic in inference? Is there a technical reason why that you're paying a penalty if you shift to smaller data centers at the edge? Or is there no technical reason why it's suboptimal? Right. Yeah. Let's talk about the training piece first. The reason why we need 1000 megawatt data centers where we have hundreds of thousands of GPUs connected so closely together is because the data sets are massive and the models are massive. We're trying to learn on the order of a trillion parameters for these machine learning models, these AI models. And we're trying to do it on the wealth of data we find in the internet. There's no way that any single GPU can handle that much data. So what we end up doing is partitioning the data into smaller pieces and then handing each GPU a slice or a partition of this data. And each GPU will turn on its own share on its own partition of the data and learn the models that work best for its piece of the data. And all the other GPUs in the data center are doing the same thing on their partitions of the data periodically. What they will do is they will compare notes. They will share the weights that they've learned. And this sharing is really, really expensive. And some of the people in the energy space may know that there are massive energy fluctuations or power fluctuations we will see in data center usage when the GPUs go from this computational intensive phase where you're learning the model weights to this communication intensive phase where they're comparing notes and sharing their intermediate results with each other. So as a result, that's why we're talking about these massive data centers for training. They all need to communicate frequently to share what they've learned from their own data sets. For inference, we don't see that effect. Just to add the craziest thing to me about how model training data centers operate right now, the absolute craziest thing is as you said, there are surprisingly large spikes in power demand as a result of how the models are trained. What they do in large part, because those spikes are actually problematic, not just to the grid, but to the equipment inside the data center as well. So what they do at least sometimes to manage that is they create dummy workloads. So they keep the power profile basically flat, but you are literally just wasting energy on absolutely nothing. Nothing is happening during those times. They're dummy workloads. At that scale, the fact that that is happening is wild to me. Absolutely. I think we've seen this in other contexts as well, but not perhaps at this scale, this notion of an electrical engineering, we'll call it the DIDT problem, the change in current divided by a change in time. If large current swings over a short period of time, you could imagine building batteries to sort of tap things out or decouple, and certainly a lot of people are thinking about that. But the easiest thing to do might be to just modulate the software, as you say, because we have very precise control over what the software does. So that is an active and ongoing area of research, and then you say it further develop. The grid is changing fast. Data centers, electrification, and extreme weather are driving a surge in energy demand. Utilities can meet the moment with existing resources by activating energy customers and their distributed energy resources to create predictable and flexible capacity with Uplights Integrated Demand Stack. Uplight coordinates energy efficiency, rates, demand response, and virtual power plant programs into one cohesive strategy to strengthen grid resilience, improve energy affordability, and make progress toward clean energy goals. Learn how Uplight is helping leading utilities harness over 8 gigawatts of flexible load at Uplight.com. Catalyst is brought to you by Antenna Group, the OGs of PR and Marketing for Climate Tech. Is your brand a leader or challenger? Are you looking to win the hearts and minds of customers, partners, or investors? Are you ramping up your new Biz pipeline? Are you looking to influence policy conversations? Antenna works with leading brands across the energy, climate, and infrastructure space to do all of this and more. If you're a startup, investor, enterprise, or innovation ecosystem that's helping drive climate's age of adoption, Antenna Group is ready to power your impact. Visit AntennaGroup.com to learn more. Okay, so then on to inference. So you're saying inference does not contain that same challenge. So is there any, what is the downside to shifting inference workloads to the edge? To my knowledge, there isn't much of a downside because the reason why inference is amenable to edge computing is because when you send a prompt to, for processing it by a large language model, that prompt is probably handled by one GPU or maybe eight GPUs inside a single machine. So, and the reason that it is is because the model sits in that machine, the data sits in that machine, and all of your prior conversations with that bot are sitting in that machine. And it's a very localized piece of compute that needs to be done. And if you don't need tens or hundreds of GPUs to be coordinating to give you an answer back, you've got that one GPU or a tightly coupled GPU is giving you that answer back. And that is amenable. That is great for edge computing and we can certainly supply that. So a thought experiment that I've given people recently in thinking about this is let's just say that you need a gigawatt of inference compute in five years from now or seven years from now, something like that. You think you need a gigawatt? Wherein the demand for that gigawatt is geographically centralized somewhere. Let's just say you need a gigawatt of inference compute to serve the Dallas metropolitan area, whatever it might be. At that point, a few years from now, this is back to the power perspective. Is it going to be easier for you to find and cite a one gigawatt site or 110 megawatt sites within that geographic region? Today, I think it is still probably easier to find the gigawatt site or at least the past couple of years it has been. There are that many gigawatt sites out there from a power availability perspective. So at some point, is that going to flip? And is it going to be easier to build 110 megawatt sites, which sounds really hard to do and indeed is, but these are all hard problems. So if that happens, do you think that we are going to see a significant portion of that inference workload move to that type of scale? Is that the right scale? Like should we be looking at 10 megawatt sites, 100 megawatt sites, one megawatt sites? How far to the edge do we want to go? Yeah, absolutely. And I agree with the premise of that question 100%. I think that there are two reasons to go to smaller and many smaller data centers. The first is the one you mentioned, power, provisioning and connection to the grid. The second is the fact that you don't need a massive GPU coordination for an inference workload. I guess the catch might be that if you are thinking about your existing edge data centers, maybe you've got data centers in downtown Los Angeles or something like that already serving workloads, those workloads may not be configured to handle GPU and AI compute. They may have power delivery infrastructure that was optimized for CPUs. They might have HVAC systems optimized for the much lower power density of CPUs. So it's not simply a matter of pulling out your CPU's and replacing them with GPU's. You may have to retrofit the facility itself to support that. But I agree, I think finding capacity there may eventually become easier than finding the next thousand megawatts. Is there any limitation? I can imagine. I'm trying to think of why you wouldn't do that. You need to have a fair amount of memory and you need to house all the model weights and so on in every individual data center if you're going to do that at the edge. There's got to be some minimum viable scale, I assume. Maybe to give you a sense of the type of data centers we were talking about in the past. Again, in a study that we had done with Meta, we looked at 15 of their data centers before generative AI and the scale of those facilities were somewhere between 15 to 50 megawatts. So less than 100 megawatts. And certainly that was fairly conventional, uncontroversial to build those sites of data centers in the past. So that's the starting point, I think, in terms of the scale. Now, as you scaled down towards, for example, one megawatt, not clear at what point things are making less sense. I guess the other point here, the way that the data center build out has gone historically, just like the cloud data center build out, it's been fairly clustered in these regions. And there's a reason why Northern Virginia is the data center hub of the world and there are others as well, Chicago, Dallas, et cetera. And that, as I understand it, is largely because the cloud providers needed to offer a certain level of reliability to their customers. And so they could have redundancy within a given region. And that was helpful to them in terms of what they were offering. Do you think that this future world, wherein a bunch of inference compute moves to the edge, let's call it 15 to 50 megawatt data centers then, instead of hundreds or thousands of megawatt data centers, does it look similar? Is that you have a bunch of a small number of regions that have a really high concentration of those 15 to 15 megawatt data centers? Or could it be much more dispersed because the whole point of this is really low latency and local and you don't need them to be as clustered? I think there are lots of different aspects at play in terms of data center siding. I think the redundancy is definitely one of them. And I have trouble disinterangling the role that some of these other factors play as well. Some people talk about tax breaks and incentives from local companies and local states. Some people talk about proximity to internet exchange points. So not only are you talking about congestion-free power movement, but you also talk about congestion-free data movement into and out of the data centers, Northern Virginia has that. And then of course the availability of the power itself. I guess I would say that when you start talking about many of these smaller data centers, from a redundancy perspective, it might be okay that they're not all geographically clustered as long as you have a strategy for rolling over the compute or rolling over the workload to spare capacity somewhere within that region that has a similar performance profile or some sort of similar latency or delay characteristic. So that's really the concern whether you have robustness, geographical redundancy and resilience there. Is this happening? It's interesting. I was thinking, okay, so it sounds like you're saying there's not a big downside. We already have significant inference workloads. It's not like we're waiting on workloads to show up that could accommodate this. And yet, if you look at most everybody, building data centers, certainly the hyperscalers and I think the colos and folks as well, the focus continues to be on we got to find big sites for big data centers. Why don't we see more development of this smaller scale edge AI inference world? I think it really depends on the workload and the application and we don't know. I view AI as a more fundamental basic technology and we don't necessarily know what application or capability will be layered on top of it. I'd say that we've been talking about edge data centers a lot. There are other words for this type of data center. Content distribution network is one of those examples of CDN or a point of presence, a BOP that the facilities are sometimes called and they exist in fairly significant numbers. Content distribution networks ensure that when you want to access, for example, newtimes.com or WSH.com, your web page is not being served from the other end of the country. Those web pages are saying close to you because the content distribution network took those updated web pages and moved them to facilities near you, data centers near you. Likewise, companies like Meta, when they have Instagram or when they have these social media applications, they also have these points of presence that supply data from local points of presence rather than retrieving content for your feed from across the country. We already see that but these are application level performance requirements whether they be for social media or for other news content. Once it becomes clear what applications of AI really drive further inference deployments, then we'll know what sort of performance requirements are needed, what we call caching techniques or strategies might be useful so that we can keep fresher data or more frequently used models closer to these users and then serve them more quickly. I think we'll become clearer as we see which models really get traction, which applications really get traction. Right. So maybe the state of affairs today is, look, anybody who's developing data centers, we know we need the big centralized data centers because there is currently essentially endless demand to train models at least relative to the availability of compute today. So we know we need to build the big centralized ones. We might as well use those big centralized ones that we know we need right now for inference workloads such as they are today, but we don't have enough certainty yet about what the inference workloads are going to be long-term to invest that kind of capital and time expenditure that it would take to build out the network of 110 megawatt data centers in a particular geographic region, something like that. But that's right. And I would say maybe that my crystal ball is as clear as anyone else's crystal ball, but I feel like there's a huge amount of GPU capacity being discussed in the pipeline in these large data centers. And if it turns out that maybe there are diminishing returns from training larger and larger models or maybe we run out of data because we've exhausted all the data that's available on the internet, when those things happen, it may be that demand for these GPUs in these largest data centers will flatten out and will have spare capacity, at which point, as you say, they will be used or repurposed to serve and inference. And then it will be hard to make the case for building yet more data centers, smaller ones which you can use closer to the users. I think the catch there will be if one of these model providers or one of these application developers makes performance a distinguishing feature of their offering. If they start competing on performance rather than on capability, then we're going to see, well, I may have 1,000 GPUs in the middle of Nebraska that are already deployed, but I really want to break into the San Francisco market. I've got to build my GPUs right there and have them available. All right. So speaking of performance, let's transition to the full extreme version of this, which is also, I think, theoretically, the most disruptive from an energy perspective, which is shifting any significant portion of these inference workloads all the way onto the device. Skip the either skip the middle ground of edge, five megawatt data centers or 15 or 50 or include them, but but shift workloads that would have gone to a big data center. There requires a lot of power straight onto your iPhone or your iPad or whatever it is. And we've heard some climmers of this as well. Give me the similar pros and cons of shifting that workload straight onto the device. Pros, primarily two things. One is performance. You don't have to go across internet. The model is right there and the compute is right there. Assuming that you get really capable of hardware and your device as well, you get really quickly responsive answers from your AI. The second is also something we've mentioned earlier, which is the notion of privacy. You don't necessarily need to send your data out into this hyperscale data center where it gets blended with lots of other user data. And you have made fewer guarantees about what happens to it. Localized compute is certainly more private than compute and shared systems. So those are the two key advantages. And then I guess third would be that it gets more tightly integrated with the capabilities on a particular platform. So for example, Apple's ecosystem. Right. And Apple seems like the obvious candidate to do this clearly. You mentioned privacy. Apple is particularly focused on privacy. They have the hardware, the device. Like Apple is notoriously or at least reputationally behind in the AI race. And so it's not hard to picture that if somebody is going to move a lot of this inference on device, it's going to be Apple. But there is a real trade-off here, I assume. Yes. And the trade-off is primarily with respect to the capabilities of the device. So if we have a very large model, we're going to have to deploy that model on a much more capable hardware platform than we've got today. This means having some number of gigabytes of memory to hold the model weights and then also some additional gigabytes of memory to hold the context as you develop this conversation with the model. In addition to the memory, you're also going to need the compute. You're not going to have this high performance GPU sitting inside your phone. So you're going to have to have specialized chips. Those specialized chips on your hand are going to be less powerful, less capable than the ones in the data center. So all of this speaks to not getting exactly the same model that you would get into the data center. You would get a shrunk-down model. Maybe in the data center, you would have a trillion parameters, this massive GPT-5 model, for example. But on a personal consumer electronics device, you might only have 7 billion parameters. So orders of magnitude smaller. And that smaller model will be less capable. It will give you less capable answers. It will be capable of doing fewer tasks. But maybe that's OK because you've identified only a handful of tasks that you really care about on your personal device. So that is really the trade-off. As you go towards the device, you're going to have to shrink the size of the model down. You're also going to get less and less capability out of your AI. The final thing, of course, is the power and energy profile. At the data center scale, we care primarily about power because power influences infrastructure and power delivery and influences thermal and so on. Thermal management. For device level compute, there are two considerations. We care about energy rather than power because that affects battery life. So even if you could deliver a really capable GPU chip onto your phone, the question is how long would your phone last if you were using that chip on a fairly consistent basis? So the energy aspect will continue to be challenging. And then the thermal aspect will also be challenging. If you have a really powerful device, that's going to be a hot brick inside your pocket. And that's going to be a deal breaker as well. So when you say deal breaker, is there progress toward on-device inference? I mean, to your point on performance, that strikes me as like, OK, this is now, we're now again in the context of specific workloads. Certain types of workloads, like a 7 billion parameter model, might be fine. And others, it wouldn't be. And so maybe there will be some on-device chip and some inference that you can do on-device, but you pull up your chat GPT app or whatever. And of course, it's going to send you back out to the cloud or maybe to the edge. These other challenges of thermal management, things like that are hardware challenges. Where are we in the progression of on-device inference? Is it coming? Is it not coming? Do we not know? I think the assumption with on-device inference is that you'll be able to shrink the model without loss in performance for the tasks you care about. That is the primary strategy. The computer scientists have been taking. On the hardware side, we have made strides in developing custom chips, custom silicon, for the specific types of Tensor algebra that are required for machine learning models. So we know how to build those chips. And that gives us energy efficient compute, higher performance. We know how to build really capable memory systems or solid state disks. So when your phone now has hundreds of gigabytes of memory on it or hundreds of gigabytes of storage on it, so there's a question of, well, maybe you'll end up using less of it for your photos and more of it for your AI model, something like that. So I think there are fairly significant resource constraints, but I don't think that they are insurmountable in the sense that more intelligent hardware design and more intelligent hardware management could go some ways in terms of making these AI models feasible on the device. Okay. So I'm going to put you on the spot and we promise not to hold you to these numbers, but just to give a sense of where we think things are heading. If we're fast forwarding 10 years, let's just say we're in 2035. And imagine there's a total volume of inference compute in the world or whatever that's, let's just say it's 100 megawatts total. What would be your guess of the ranges of how much of that compute is going to take place in large centralized data centers or versus at the edge? Let's, we'll draw a line. Let's say, you know, 100 megawatts and above is large centralized, some hundred megawatts, but not on device is edge. And then the third category, of course, being on device, like how much of it can go anywhere but the centralized data centers? So I would go straight to this idea of having a 2080 rule because we see this all the time in computer systems where you have 20% of your tasks being extremely popular. Maybe there are 20 things that you always want to do and that you spend 80% of your AI compute doing those things. That could be email processing. That could be photo analysis. That could be, so we can identify what those really compelling applications and tasks are and we're going to be spending most of our time doing that. And then for the remainder of the long, heavy tail of other tasks that people might want to do, that will always be backup capabilities residing in the cloud data center. So I would say that we could be getting 80% of our compute done locally and leaving 20% of the heavy lifting or the more esoteric, the more corner case compute for the data center cloud. That is of course excluding the training. The training will continue to all reside in the massive facilities. But in terms of the inference, I think there's huge potential. Right. But yeah, that's actually a very significant shift of 80% of the inference workload. Appreciate that that does include training. But still, 80% of the inference workload could end up local. That's a significant shift and has pretty profound implications for the energy picture as well. So you saying that 80% just to pin you down even a little bit more, is that local in the sense of being at the edge or is that local in the sense of being on device? Or what do you think the split ends up being there? Yeah. So I think of the 80%, I would say most of that will be on the edge. Like I suspected is today, I think that if you look at what we talked about earlier, content delivery networks, points of presence, they've probably identified 20% of the content that 80% of the people will be looking at most of the time and they're putting it at the edge. I think maybe on the word of 1% ends up being put on your consumer electronics. Actually, even for today's compute, when we set aside AI, there is a trend towards consumer electronics hiding that flow of data back and forth between the device and the edge for you. So sometimes they'll like if you use a cloud storage service like Dropbox or if you're using a photo storage service, they will let you pretend that you have access to all of your videos or all of your photos and all of your documents and they will transparently behind the scenes, move things back and forth between the data center and your local device. So you may think you have all of it, but maybe you've only got a tiny sliver, less than 1% on your local device. Right. Certain things open up in my box instance, certain things open up much faster than others when I try to open them. And it's occurred to me that that is why if I step back then, okay, so it sounds like what you're saying in this scenario, you're painting of the future. 80% of the inference workloads are edge, very little of it actually on device and then the other 20% or so sitting in cloud, big cloud data centers. So when I think about the energy implications of that, there's I think a couple of ways to think about it that are pretty interesting. One is this, okay, so maybe a fair amount of the energy consumption of at least inference compute is going to shift to these 5 megawatt, 15 megawatt, 50 megawatt type local sites. That has big implications for the grid in ways that are, I don't know, both good and bad, probably harder to manage than some ways easier to manage in other ways. But the overall energy consumption of inference compute, I would expect, and you can tell me if I'm wrong, would actually be higher in this scenario than it would be if it was all centralized, because I assume the PUE that you get for these edge data centers isn't quite as good as it is for the large centralized data centers. So like on balance, this probably means more overall AI energy consumption. Do you think that's right? Yes, yes, I think you get economies of scale. When you go to a gigawatt or two gigawatts, you have a single facility, you're managing it in a highly optimized coordinated way, and you've got hundreds of thousands of these machines all managed very precisely. I think as you shrink the system down, you will lose an efficiency. You will be trying to build these 20 megawatt data centers and maybe footprints or facilities that weren't designed initially for those workloads. So yes, I think total energy costs may go up as a result. We're talking about inference workloads, to some extent, as a monolith. I'm sure they are not. So are there big distinctions in your mind in terms of the different types of inference workloads and how that influences where they should be housed? Right, yes. So that's a really great question, actually. I would say that there are fundamental limits that a number of inference queries a human user can actually produce because we're ultimately limited by the speed of our typing. The number of tokens we can actually produce to query the models. So there is some of that where humans will continue to send requests to agents. But I think increasingly, most of the inference workload will come from other software agents. This could be a search engine retrieving web pages and then asking the large language model to summarize it for individual coherence discussion for you. This could be your photo app learning something about your images or this could be your mail app doing something with the mails and helping you compose messages. So all of that is done behind the scenes. And those inference workloads are potentially much larger because of course, software can generate those requests at much, much higher rates. From the perspective of where that computation happens, to the extent that the data center already has servers running your mail workloads or to the extent that your search engines are already running in the same data center, the communications that the model will be a bottleneck. So if you have a data center in Nebraska, running your search engine for you or doing some of these other big heavy lifting, heavy software jobs, then potentially they could query and execute interference in these largest hyperscale data centers. All right, Ben, this was super interesting. Really appreciate your time. It was my pleasure. I really enjoyed the conversation. Thanks so much. Dr. Ben Lee is a professor of electrical engineering and computer science at the University of Pennsylvania. He's also a visiting researcher at Google. This shows production of latitude media can head over latitude media.com for links to today's topics. Latitude is supported by Prayly Adventures. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Marklon. Stephen Lacey is our executive editor. I'm Shail Khan and this is Callist.",
    "release_date": "2025-12-18",
    "duration_ms": 2867000,
    "url": "https://traffic.megaphone.fm/PSMI8071520231.mp3?updated=1766013978",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2026-01-16T02:11:05.754943"
  }
]