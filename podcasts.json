[
  {
    "title": "The story of steam",
    "description": "Addison Stark thinks waste heat is a waste of time. The real opportunity, he argues, is decarbonizing industrial steam, which accounts for roughly 30% of industrial heat in the U.S. But doing that means deploying alternatives to the fossil fuel boilers industry currently relies on.\n\nSo how do you clean up steam? And why does Addison think waste heat is overhyped?\n\nIn this episode, Shayle talks with Addison Stark, the CEO \u2014 or as he likes to call himself, chief boiler maker \u2014 of industrial heat pump startup AtmosZero. They dive into topics like:\n\nThe difference between saturated and superheated steam \u2014 and why it matters\n\nWhy fuel dominates OpEx in steam generation, and how fuel types vary across regions\n\nHow the cost of steam affects overall cost of delivered products\n\nWhy resistive boilers reached maturity ahead of heat pumps\n\nWhy standardized, air-source heat pumps are emerging as an attractive alternative to resistive boilers\n\nThe role of thermal storage combined with renewable PPAs\n\nWhy Addison thinks waste heat is a distraction for decarbonization\n\nResources:\n\nJoule: To decarbonize industry, we must decarbonize heat\n\nThe Green Blueprint: Rondo Energy\u2019s complicated path to building heat batteries\n\nCatalyst: Solving the conundrum of industrial heat\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is executive editor.\n\nCatalyst is brought to you by Anza, a platform enabling solar and storage developers and buyers to save time, reduce risk, and increase profits in their equipment selection process. Anza gives clients access to pricing, technical, and risk data plus tools that they\u2019ve never had access to before. Learn more at go.anzarenewables.com/latitude.\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.",
    "summary": "The podcast discusses the challenges and opportunities in decarbonizing industrial steam generation. It highlights the importance of steam in various industries and the dominance of natural gas as a fuel source. The conversation delves into different decarbonization pathways, such as electrifying boilers with resistance or heat pumps. The limitations of utilizing waste heat and the potential of standardized, air-source heat pump solutions are also explored. The economic factors, including fuel costs and op-ex, are significant drivers in the industrial steam market. The podcast is sponsored by ANZA and EnergyHub, promoting data analytics platforms and virtual power plant solutions for energy storage and grid flexibility.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shail Khan and this is Catalyst. Waste heat is a waste of time because people are chasing after a small increase in COP to justify and minimize op-ex, but what they've inadvertently done is essentially driven a massive increase in cap-ex by trying to capture waste heat. Coming up, the story of steam. Do you want instant access to energy storage supplier pricing that's project-specific, or the ability to compare domestically-made battery and PCS options across the market? Anza now offers the industry's first battery energy storage data and analytics platform to make better development and procurement decisions. Anza provides in-depth commercial, technical, and risk data and analytics to help developers choose the best equipment for any project. Improve your returns and save months of evaluation time with Anza. Learn more about Anza's energy storage subscriptions at go.anzarenewables.com slash latitude, or click the link in the show notes. Imagine a world where connected devices like EVs, home batteries, and smart thermostats work together to support a more efficient and reliable power grid. Well, you don't have to imagine it anymore. This vision is a reality today, thanks to Energy Hub. With Energy Hub's Edge Derm's platform, utilities can create virtual power plants through customer-centric flexibility programs, making it easy to manage distributed energy resources and balance the grid. Unlock grid flexibility and reliability through cross-DER management with Energy Hub, the trusted Edge Derm's leader. Visit energyhub.com to learn more. I'm Shail Khan. I invest in early-stage technologies and energy impact partners. Welcome. Let's start with a number, 50%. That's roughly how much of all industrial energy use globally goes to generating steam. How do we make steam today? Well, we boil water. It's basically that simple. It's what drives paper mills, food processing, chemical production, textile manufacturing, you name it. Steam is the silent workhorse of industry. Right now, it's mostly powered by hydrocarbons, depending on where we're talking about its natural gas or maybe even coal could be oil in some places. We heat water into steam, and that keeps our industrial processes humming, which makes decarbonizing steam not just a niche technical challenge, but a big emissions opportunity basically hiding in plain sight. So what are the options? And also, what about all that waste heat that often tantalizes entrepreneurs looking to turn waste into value? Well, let's explore. To dig into that, I brought on someone who basically spends all day thinking about this. Addison Stark is the co-founder and chief boiler maker, his term, of Atmos Zero, which is an EIP portfolio company, I should note. They're developing what they call Boiler 2.0, which is a heat pump-driven electrification solution for industrial steam. Here's Addison. Addison, welcome. Shale, long-time listener, first-time caller, I suppose. I suppose. Excited to have you school me publicly, which you've done privately many times, about industrial steam. Talk to me about the market for industrial steam. What is it? Where do we use it? How big is it? You know, as the true thermodynamicist mechanical engineer that I am, I actually want to take a step back first and say, well, what is steam, right? I mean, and why do we care about steam and why am I excited to tell you and talk about it today? Is steam is gaseous water, but it's been the most important working fluid that we've had in industry in the built environment since 1867, when Babcock and Wilcox patented the combustion boiler. They moved from a brick-by-brick built combustion systems on site to a factory-built boiler that really was the catalyst to drive the industrial revolution. It's really meant that all of industry has been built around this super-valuable working fluid. The amount of heat that can be delivered through the phase change of water, the latent heat of vaporization or condensation is tremendous. It allows us to actually have very compact chemical processes, phase change separation, being used in chemical facilities, but also it is what has driven heating in the built environment for just as long. Some of the oldest boilers that I've seen are generally things that have been delivering both heat to industry in London, but also to buildings to keep them warm. And we use the same form factor today. You know, I mean, today steam is accounts for about half of all industrial heat that's being delivered. It's the most important working fluid in industry, and it is an outsized impact in the food and beverage industry, the chemicals industry, pulp and paper, pharma, personal care products, cosmetics, wherever you think of a biological process or cooking, steam is being used. And how much is steam steam? I guess what I mean to ask is like, I know that one way to divide up the market for industrial steam is by temperature requirements. So obviously there are different temperature gradients of steam that are required. But beyond that, are there any other ways that you distinguish between different types of steam that are required for different applications? Well, that's a great distinction, right? When I first got into industrial heat, it was back during COVID. I was doing two things. I was baking sourdough and then grinding my axe against this idea that industry was hard to carbonize. And I really got into this question of what's most important, and you start to look at industrial heat, and as exactly as you put it, people look at temperature ranges, but then working fluids. And then each working fluid, like steam in particular, can be subdivided. There's kind of two different ways we think about steam. In the chemicals processing where steam is used as a reactant, it's known as what we call superheated steam. It's essentially purely gaseous. It's like not dissimilar to nitrogen or oxygen or any sort of a pure ideal gas. However, what is used most commonly to deliver heat is known as saturated steam. Essentially, steam sitting right in equilibrium with liquid. It's going back and forth between the phases of liquid and gaseous, but that's where all of that potent thermal transfer is, where you can really get a ton of heat transfer. So the majority of heat delivery that's done by steam is all through saturated, and that's what boilers deliver today. Generally, almost all heat delivery through steam is done around 225 Celsius and below. Generally that above there, you run into some heat applications, but a lot of reaction applications as well. Okay. What we're doing in terms of heat applications, you said 225 C and below, and that's where we're using boilers. What has changed? You mentioned the original Babcock and Wilcox patent in the 1800s. How similar or different is today's industrial boiler versus what came in the 1800s? Some may first pass an engineer who worked at Babcock and Wilcox in the 1867 as part of that would recognize what we use today. The same form factor where essentially burning fossil fuels to boil water to be able to deliver saturated or super heated steam to processes. But there have been improvements on the fireside ways to continue to improve the efficiency of how much of the chemical energy we're able to convert to steam heat has continued to improve. Also focuses on minimizing not just CO2 emissions, so that comes from efficiency, but then also on socks, knocks, particulates, other sort of criteria pollutants. There's been continued improvement on that, mostly driven through regulation, but it's the same product. That's the reality and the whole market for boilers has largely built around that fact, which is a factory built combustion device that's able to deliver steam in a very highly efficient way and integrated in a very smooth way. Let's talk a little about the economics of steam delivery. You mentioned that what we're doing is burning fossil fuels. The first question is, which fossil fuels are we burning where for industrial steam? Yeah, that was a bit of an oversimplification on my part. Steam is generated not just with fossil fuels, but some places you're using electricity, some places you're using biofuels. But today in North America, predominantly we're burning natural gas in Europe that's driven by LNG. But in China, in other developing markets, you still see utilization of coal. And even some places where you don't have access to imported natural gas, you're often using even oil or bunker fuel. Some places where you see some effort towards decarbonization has been done. People will be using biomass boilers or if you just have enough forestry resources. This is very common in pulp and paper just to use that directly. Or you see the utilization of RNG in Eastern Europe, in North America, where that market has been matured. Okay, so talking about the economics then, to a first order is the cost of industrial steam basically a function of the cost of that underlying fossil fuel commodity? Like does the cost of industrial steam in North America vary directly with the cost of natural gas, essentially? Yeah. So the cost of steam is really dominated by the fuel cost. Like any sort of energy conversion process, the capital is important as an upfront cost. But once you look at the 20-year life cycle of a boiler, or sometimes we're out in the field and we see 30, 40, 50-year-old boilers, it's really about the apex, the fuel and the maintenance, but fuel itself can be 70 to 90% of that op-ex itself. So it's the dominant factor in the cost of steam delivery. In North America, natural gas is cheap. And it really is the dominant fuel in steam generation for industrial facilities. And what kind of cost are we talking about? And I guess the other question is, and this will vary by application, but how important is the cost of that steam to the ultimate cost of whatever product is being produced? Is it a major cost driver for the end product? Or is it pretty de minimis? Like do they care? How much do they care? So different industries have different exposure to the cost of steam in the ultimate delivered product, right? If you look at food and beverage industry, generally the cost of steam is a small fraction of the delivered product because at the end of the day, let's say you're brewing beer. You're dominated by the cost of hops and barley and other sorts of ingredients. And while your most important scope one emissions are from the boiler on site, it's a rather small impact on the embedded cost. So there is room for innovation there. But if you look at the cost of steam today in facilities, it's really a function of what are you getting your natural gas at at the facility cost itself. And that varies widely. So it's really you look at the natural gas costs that you're paying at on a small, like we were estimating before 10% from the capital. And that really becomes kind of your levelized cost of steam that you're utilizing in the facility. Catalysts is brought to you by ANSA. ANSA offers a one of a kind data analytics platform and advisory services to support better project development and procurement decisions. For energy storage developers, ANSA's platform provides crucial information that you never had easy access to before. Now at your fingertips is real time pricing for a long list of system configurations to suit any project. ANSA provides a 360 degree view of the market with a lifecycle cost analytics and commercial technical and risk data. With ANSA, developers can easily determine which products to use in their designs, finance models and RFPs. Learn more about how ANSA helps save time and maximize profit at go.ansarunuables.com slash latitude. Catalysts is brought to you by EnergyHub. EnergyHub helps utilities build next generation virtual power plants that unlock reliable flexibility at every level of the grid. The EnergyHub platform takes the guesswork at a balancing energy supply and demand. It uses machine learning to control customer owned distributed energy resources, like EVs, home batteries and smart thermostats, to precisely shape load profiles for grid flexibility and reliability. As the industry leader, EnergyHub helps more than 80 utilities manage 1.6 million devices. That's more than any other edge derms on the market. Click the link in the show notes to learn more or go to energyhub.com. Okay, so let's assume one cares about decarbonization and one comes to the realization that half of the industrial energy in the world is delivered as steam and that we want to do something about the emissions associated with that, which is a huge bucket of emissions. Let's talk about the different pathways for decarbonization. The first one I think that maybe is you tell me if you feel differently, but maybe is I guess the most mature or at least most widely adopted today is just like electrify the boiler, make a resistance boiler, right? And instead of burning a fossil fuel, you use electricity to heat the water. How much of that is out there today and what are the limitations of it? You're right that probably the most off the shelf solution for electrification of the boiler room is resistive or electrode boilers. Sometimes they're known as a trade. It really depends on how high of voltage and how high of a throughput you're putting through. And while the total penetration in the market is relatively small, maybe about 1 to 2 percent of the boiler market today, it's the fastest growing subsector in the boiler market. So if you look at the growth of the boiler market, it's about a $17 billion a year market with 6 percent growth per year, but electric-resistant boilers are growing at about 26 percent per year. When people are looking to electrify, when people are looking to move away from combustion, what's available off the shelf today is a resistive electric boiler. Of course, you're signing up for higher cost, right? We were just talking about how expensive natural gas is. Generally, if you're moving from a natural gas steam to electric steam, you're looking to a 2 to 3x increase. Really you're just increasing relative to what your facility's spark spread is. Now the other off the shelf solution that manufacturers have is really geographically dependent. Do you have access to either biomass or RNG? These are similarly large increases in op-ex as well, just because the fuel costs is much more expensive than natural gas here in the US, where natural gas is so cheap. Okay, so then the alternative if you want to electrify is what you guys are focused on at M0, which is using heat pumps. We've talked a bunch about heat pumps on this podcast before in the context of residential for the most part. I think people appreciate Andy Luberschade, who I know you know well and our listeners have heard many times, talks about the magic of heat pumps, the concept of basically getting more energy out than you put into it. In some ways, heat pumps seem like sort of an obvious solution here, if you could make them big enough and powerful enough. Why in your mind have heat pumps not taken off more? Why is it that the most mature thing is the resistive boiler and not the heat pump today? Well, as the thermodynamicist at heart, I need to take issue with the magic statement. Obviously, it's only magic insofar as it still satisfies the first and second law of thermodynamics. And we are of course getting more, let's call it usable energy out. We're getting in a heat pump, you can get anywhere from 2 to 3x of the heat out of the electricity put in, but where is that heat coming from? We're sourcing it from somewhere. Industrial heat pumps have been, let's call it a nascent market for 30 years. Essentially, heat pumps that go much higher in temperature than residential heat pumps, because ultimately you got to get up to above 100 Celsius to be able to deliver steam. So how people have traditionally tried to do that is they've captured waste heat in the facility. They'll go after and find some sort of a source from a unit operation on the manufacturing floor, capture that and then upgrade it. Now that has kept it to the point where essentially every facility has been bespoke. So waste heat is often mismatched in time, temperature or location relative to steam demand. And it's led to bespoke expensive and slow deploy projects. You know, the tangent or the little pithy thing that I like to say is waste heat is a waste of time. It's actually limited this industry for some time because people are chasing after a small increase in COP to be able to justify and minimize op-ex, but what they've inadvertently done is essentially driven a massive increase in cap-ex by trying to capture waste heat. And at most zero, what I thought about and really what led to why I really got interested in can we do heat pumps better is how do we standardize them, productize them, and what we saw was an opportunity to go air source to avoid waste heat. So you know, that's one view that I have of a drop-in mass manufactured approach. There are a couple other ones as well, but I think that this is a scalable way to go after it. Let's stay on that tangent for a minute because I do think it's an interesting one. I like your phrasing, waste heat is a waste of time. So waste heat is this tantalizing mirage that I feel like I see entrepreneurs and academics and all sorts of people going after with a regular cadence because not just for the purpose of running heat pump, but in general, there is so, so, so much industrial waste heat. And so you look at those numbers, you look at one of the Sankey diagrams and you see how much energy we waste from industrial processes. And you think, gee, it sure would be nice if we could use that waste heat. And oftentimes the waste heat is, you know, sometimes it is used in some processes. Well, and it's not. It's often because it's too low temperature to actually do anything with useful on the site. So then you think, okay, great. Well, I've got this waste heat that is hotter than ambient. And so it should be cheaper for me to upgrade it to whatever temperature I need. And if only I could do that, like this is just an opportunity hidden in plain sight. And so I see it very commonly that people, whether it's running a heat pump or something else, want and want to do something with waste heat. And you along with with Greg Teal on our team have been on a, I think, a long term tirade to say it is a mirage, basically. It's not that it doesn't exist. It's that accessing and utilizing waste heat industrial facilities is way harder than you think it's going to be. So can you describe it a little bit more detail why that's your view? It's in the words, right? I mean, waste heat is waste. And at the end of the day, we've got to get it out of the facility. And that's just obeying the second law of thermodynamics. Now I'm not going to go down a deep thermodynamic tangent here, but there are a couple of scaling things to think about. So there's two things that people try to do often. Well, three things probably with waste heat. Number one, capture it and upgrade it in a heat pump to be able to deliver heat. Number two is capture it and try and convert it into electricity. Or number three, capture it and utilize it to drive processes for chemical processes or separations or something else. For all of those things, you essentially need to find a way to capture that waste heat. And that's where the first most expensive step comes in. The lower the temperature it is, you need to have larger heat exchangers to be able to capture that and put it into the other working fluid. That increases CAPX. The other thing is this waste heat is not always located in the exact same place at the exact same temperature in every given facility. So you're building bespoke one-off heat exchangers with very expensive engineering hours to go and build and capture that in that facility. And so if you're a manufacturer, say you're a global cosmetics manufacturer and you have 20 manufacturing facilities around the world, your facility in Europe might not look like the one in South America actually has slightly different temperatures of waste heat, different locations. You cannot take what you did to capture that waste heat in one facility and apply it in the other. So there's no real scales of mass manufacturing or volume to be able to gain there. It's all about are you going to be able to get the economic value of that waste heat on a project by project basis. So there's two challenges that I see. Number one is waste heat destroys repeatability of any given solution no matter what you're trying to do. And of course right there is what you see is when we look at what has successfully scaled in climate and energy technology as things that are manufacturable, modular, repeatable. That's why the boiler was successful. There was nothing, it was the transition from bespoke boilers to mass manufactured boilers that allowed the industrial revolution to go. We shouldn't assume that we can continue to do bespoke approaches. The second challenge with waste heat is this fact that it is waste, it is low value. Heat carries this other thing. It's not just the energy but it's also the entropy associated with that heat. At the lower the temperature, the relative fraction of energy to entropy is decreasing. Essentially the total usable energy in there is much lower. So you have to do more work just to actually get something out of it and there's less to get out of it. So it has been tantalizing for 30 years. It's kept industrial heat pumps to be a very limited and one off bespoke industry and no one has really been able to scale. And that's what that nut is that we're trying to crack both at Atmos Zero and I hope through Greg's input that you guys are continuing to fight the good fight with us. So obviously the challenge with doing what you're doing though, I mean the reason that waste heat seems nice in the context of delivering industrial heat with the heat pump is that you're starting at a higher temperature than ambient. So if what you're doing is air sourced, which you are, then the challenge is you need a big temperature lift, at least relatively speaking. You need to get from ambient up to 100 degrees C or more. Talk to me about what that actual technical challenge is. What are the mechanics of a heat pump that make the higher the temperature lift the harder it is to do? Well first off there is a little bit of a trade off that you, an economic trade off that you hit immediately. Theoretically the higher the lift that you're trying to go in a heat pump, the lower your overall COP coefficient of performance, the overall efficiency you can achieve. So by ignoring waste heat you're actually decreasing the total efficiency you can achieve, which is a trade off right? We're essentially looking at decreasing our overall efficiency, but ideally to be able to massively decrease capex. Now the challenge there is well we have more capex in this kind of a solution because you need to have a higher lift heat pump. So in order to overcome that you really just need to focus on having a multi-stage approach. Essentially have, think about taking two heat pumps and stacking them on each other to be able to get up to the temperatures you need to do. Now it's managing complexity at that point. But ultimately when you think about building heat pumps to be able to deliver steam, you want to focus on having something that is highly efficient but repeatable just like the boiler and that's what you focused on. I guess let's talk finally about the economics again, rounding back to that. You mentioned this before, but this is true of all electrification things. You have this challenge of the spark spread, which is the difference between the price of electricity and the price of natural gas basically. And when you're electrifying electricity is in North America, let's talk geographically too. North America, electricity is way more expensive than natural gas basically. And so like therein lies your unit economics challenge if you want 2D carbonizer, if you want to electrify. Of course with heat pumps, you make some of that up with your COP. So the fact that you have this efficiency can help a little bit. What do you think it takes to get truly economic industrial heat pumps in North America versus in Europe where I know the equation is very different? You're getting to a very important point in let's call it industrial heat decarbonization no matter what working fluid. The challenge and particularly it's the US not just all of North America, but in the US is the fact that we have natural gas resources that are incredibly plentiful and incredibly cheap and well integrated in infrastructure. We have massive natural gas pipelines that go to every industrial facility and we have therefore very low cost access to steam, process heat anywhere. That is a challenge for any sort of an approach here. We know that it has limited the deployment of resistive boilers here because you're just signing up for a direct one to one switch to electricity prices instead of gas prices. But then the two approaches that allow cost effective ways as we've been talking about and what we do is heat pumps through increasing the efficiency through a high enough COP. You can bridge that spark spread gap and the other approach is thermal storage. And so I know and we're excited about thermal storage is kind of that complimentary approach where when you have access to time of day pricing with renewables, you can hopefully drive down that cost low enough through charging those and deploying those. That's the two approaches. Now it's different suited for the different kinds of facilities that use steam. Very large facilities with access to time of day PPAs or behind the meter renewables is a really great place for thermal storage. We see that as an excellent opportunity and also for higher temperatures. However, for lower temperature steam, think below 200 Celsius where you might be an end of the wire price taker for electricity. You need to have a high enough COP to be able to bridge that spark spread. That's where heat pumps can win because not only are heat pumps an ideal solution there because of the low temperature, but you can get a high enough COP to actually use just direct industrial tariff electricity off of the grid and not worry so much about having to also engage in the electricity market as an end user. The reality is when you look at manufacturing in the US, 65% of manufacturing facilities have a thermal load below 10 megawatts. So really you need small enough scalable solutions that look like boilers to be able to be a solution for the call it the light duty manufacturer. Okay. So that's the US with our plentiful cheap, beautiful natural gas. What about Europe? Everything changed in Europe after the invasion of Ukraine, the sabotage of Nord Stream, no longer do you have a ready and plentiful access to pipeline gas coming in from Russia. So now in Europe, natural gas prices are much more closely pegged to global LNG imports into Europe and that has changed the spark spread there and the equation there. But it's not just about raw economics from a spark spread standpoint. The other economic impact in Europe is access to steam to keep manufacturing up and running is a critical utility in manufacturing. So thinking about supply chain and energy security is just as an important impetus to transitioning to an electrified solution for Europe beyond just the raw spark spread. Addison, this was fun as always. Thank you so much for joining. Shell, this was great. You know, as we like to say around here, full steam ahead. Addison Stark is the co-founder and chief boiler maker of Atmosera. This shows a production of latitude media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. Prelude backs visionaries accelerating climate innovation that will reshape the global economy for the betterment of people and planet. Learn more at PreludeVentures.com. This episode was produced by Daniel Waldorf. Mixing in theme song by Sean Markwond. Steven Lacey is our executive editor. I'm Shell Kahn and this is Catalyst.",
    "release_date": "2025-06-19",
    "duration_ms": 1986000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI6474677925.mp3?updated=1750282036",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-06-22T01:55:11.508546"
  },
  {
    "title": "The state of play of data center development",
    "description": "The future of the grid increasingly hinges on where and how data centers get built. To forecast the kind of power infrastructure we need to meet AI\u2019s growing appetite, we first need to understand a laundry list of variables: data center size, workload type, latency, reliability \u2014 even the variety of a data center\u2019s coolant system.\u00a0\n\nSo what\u2019s the state of play in data center development today \u2014 and how are the trends shaping grid needs?\n\nIn this episode, Shayle talks to Chris Sharp, chief technology officer of Digital Realty, a developer, owner and operator of data centers. They cover topics like:\n\nHow AI inference workloads are clustering in existing regions, driven by latency and throughput requirements\n\n\u201cData gravity\u201d and \u201cdata oceans\u201d: how large concentrations of data attract more compute infrastructure\n\nWhat\u2019s driving longer lead times: interconnection delays, equipment bottlenecks, or both?\n\nLarge-scale builds vs. incremental additions and densification of existing infrastructure\n\n\u201cBraggawatts\u201d vs. real demand: separating hype from reality\n\nThe diverging power needs of training vs. inference, and whether any workloads work with intermittent power\n\nThe evolving role of \u201cbridge power\u201d and why diesel and gas are still in the mix\n\nResources:\n\nLatitude Media: Google\u2019s new data center model signals a massive market shift\n\nLatitude Media: The future of energy-first data centers takes shape\n\nLatitude Media: Can a new coalition turn data centers into grid assets?\n\nLatitude Media: Do microgrids make sense for data centers?\u00a0\n\nThe New York Times: Wall St. Is All In on A.I. Data Centers. But Are They the Next Bubble?\n\nCatalyst: The case for colocating data centers and generation\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is executive editor.\n\nCatalyst is brought to you by Anza, a platform enabling solar and storage developers and buyers to save time, reduce risk, and increase profits in their equipment selection process. Anza gives clients access to pricing, technical, and risk data plus tools that they\u2019ve never had access to before. Learn more at go.anzarenewables.com/latitude.\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.",
    "summary": "Shell Kahn, host of the Catalyst podcast, talks with Chris Sharp, CTO of Digital Realty, about the evolving landscape of data centers in the energy transition era. They discuss the geographical dispersion of data centers, focusing on regions like Northern Virginia and the importance of availability zones for low latency and high availability. The conversation delves into the challenges of power constraints and infrastructure development timelines, highlighting the need for strategic planning and collaboration with utility operators. They explore the demand for varying data center sizes and the complexity of accommodating diverse workloads in the AI space. The discussion also touches on the potential strategies for building data centers to meet evolving power demands efficiently.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shell Kahn, and this is Catalyst. There's a lot of noise, like one of the things we've been joking about is a lot of braggle-wats. Oh, I have a gigawatt. I have a gigawatt. Coming up, what's more insatiable, power demand from data centers or my appetite to talk about it? I'm Shell Kahn. I invest in early-stage companies and energy impact partners. Welcome. So we've, of course, spent a lot of time on this podcast talking about the Energy Data Center Nexus. Too much time? Who's to say? Objectively, it's the biggest thing happening right now, so buzz off haters. Anyway, one thing we haven't done amidst all that discussion is talking to somebody who's actually building data centers, and has been for a long time, for that matter. So that seems dumb. Fortunately, there's Chris Sharp. Chris is the CTO of Digital Realty, which has been around for 20 years, developing, owning, and operating co-located data centers all over the world. Chris is just very insightful about what's going on in the space and what's coming next. So I brought him on to talk about what the hell's happening in data center world, and a fair bit about what's happening at the data center power nexus. Here's Chris. Chris, welcome. Thank you. Thanks for having me. Looking forward to it. Excited to talk about the state of data centers in the world. I think particularly in the United States, let's restrain ourselves to something reasonable to talk about, because there's a lot to talk about even here. You've been in data center world. How long now? Like 15 years or more? Yeah, 15 plus years. Longer than I could admit, believe me, when I started, it wasn't cool, and it wasn't at the forefront of every headline. So that's changing. Congrats on finally being cool. Thank you. I wouldn't go that far, but okay. Yeah, sure. I want to start by talking about geography a little bit. Again, maybe we'll focus primarily on the US. I mean, my perception of how this world has evolved is that historically, the data center development activity and operation activity was very concentrated in a pretty small number of regions, Northern Virginia being the one everybody probably knows the most about, but then there were like some other tier two regions behind that. And that part of what has happened in this new wave of excitement and AI and hyperscale data center is getting planned everywhere, is that there's been a big geographic dispersion. And so I guess one question for you is, is that true or is it still really regions that drive the majority of the growth? Yeah, so I think you have to take a step back and look at the problem from two lenses, right? The first lenses is, what are the workloads coming to market? And I think that lens is interesting, right, where the most simplistic terminology people hear about training and inference, I think training has forced a broader regional deployment, but that's for training these kind of frontier models. I would say that there's been a lot of growth in that, but we see that kind of leveling out, where we really see the consumption of AI are inference, that's driving that regional specific growth going forward. And I think that's where it's more embedded in a lot of the existing, if you will, follow the clouds with availability zones. That's where it's really starting to be that investment growing and evolving quite quickly. And I think you brought it up with Northern Virginia that NOVA market has been one of the critical availability zones, which is now represented as a critical kind of AI growth sector going forward as well. When you say availability zone, what's the promise? What's the availability promise that's being made? Because this is what's driving its regions for this reason, right? 100%. It is a promise of a certain level of availability. Yeah. And so I think I always do a one step further on that training inference, but it's monetization. Those availability zones were foundational and set up gravity, if you will, of what was driving that as SLAs and consumption to the enterprise, right? And I think that's where you see a lot of these capability and infrastructure being invested in these zones all around the globe, where there's a major city center. It's usually closer to the CBD because there's proximate requirements with throughput and latency associated with it. But those availability zones are what has built that kind of, if you will, first wave of cloud infrastructure coming to market. And availability zones are slowly evolved. They're not everywhere. They're not in these tier two, tier three markets, but we're seeing a lot of AI applications being embedded inside of those availability zones. And the last piece I believe you with is that AI is an and not an or to cloud, right? I want people to really comfortable with that is that a lot of these AI capabilities are being embedded in the cloud services you're consuming today, like co-pilot and some of the early capabilities come into market. But that's how we really see a lot of these markets maturing over time. You mentioned latency there. So my laypersons understanding here of what you were describing is, okay, training a model, you can kind of do anywhere, but the models are getting bigger and bigger and bigger. And so we need bigger and bigger data centers, but they're not as geographically constrained. And thus, you can put a training focused data center, maybe in the middle of nowhere, assuming you have all the other things that you need, you have power, you have labor, you have water, et cetera, et cetera. But then inference latency matters more. And thus, you want high and not only latency, but I guess availability as well, because these are time sensitive requests. And so that's why you want to be clustered in a region and so on. I've heard some people, this will get to the energy data center nexus a little bit, speculating that you could bifurcate even the inference workloads into things that are latency sensitive and things that are not. And the ones that are not, maybe you go take advantage of cheap, clean power, maybe even intermittent power out in the middle of nowhere, which definitely exists, but is not where the rest of the data centers are getting cited. Do you view that as a viable approach, given the actual workloads? It is, it is. And it's great that we're going through the workloads, right? Like that workload is what depicts the infrastructure required to making it successful. And I think latency and throughput remain many different things. And so I always try to double click on a little bit. The amount of throughput required is what's challenging, right? And latency, as long as it's consistent for a lot of the workloads we see, they can operate fine. But it's that throughput, the amount of data that's required for delivering kind of an inference type of solution is something that is again proximate, not only to the consumer, but proximate to an ecosystem. So I'll, I'll, I'll hit on your second point where, yeah, we see a lot of text to text scenarios where that workload can be deployed, you know, in two or three markets throughout North America and service the entire market. And so that's a very simplistic kind of scenario where we're in the early innings of AI and the complexities hasn't, hasn't really come to fruition for the broader market. But as you see, bimodal, some of these more advanced reasoning models where a token isn't just generated against a prompt and then you consume it and it's done, a token may be generated inside of an AI world and go through multiple models to ensure that, you know, it's not hallucinating or that it has a mixture of experts or a depth expertise in that outcome of that token. So that's where these ecosystems of other AI infrastructure being proximate to itself, not only just the data, but I would be remiss not to hit that AI is only as smart as the data sets you feed it. And so those training, you were able to feed that monolithic set of data in the middle of nowhere, but now we're more real time micro learning inference. It's starting to become more proximate to where the data oceans and that data gravity, which we've produced a report a long time ago is happening around these availability zones and these epicenters of these tier one markets. So it sounds like what you're saying, if I'm interpreting it right, is that this notion of let's go where this just all else equals go where the stranded power is. It probably has some validity because there are some workloads, text to text, for example, as you said, that can handle that where the throughput requirements are not so high and the latency requirements are not so high. But it also sounds like you're saying the direction of travel is in the opposite direction because actually the workloads are becoming more sophisticated, leveraging multiple models and the throughput requirements are getting higher. And so that set of opportunities, just go wherever the cheap available power is probably dries up, or at least the relative share of like how much you can build in that use case versus how much you could build if you actually have a cluster. And it's all regional and it's near all the other models. Like that's going to be a much bigger opportunity. Do I have that about right? Yeah, I know you're spot on. And there's a confluence of events that are happening there. It's not just power. The expense to stand this infrastructure up is these chips are not cheap, right? And so being able to utilize that over a longer horizon of workload and driving that utilization is also a form factor of if I haven't installed for training and training can be very a spiky workload that I can embed. And that capability get higher utilization out of that investment because the Royce is real on this. And so you see a lot driving that direction as well. But no, as we see these higher value kind of aggregators, if you will, of multiple models, right? Multiple capabilities. That's really starting to become more proximate to one another. And again, data is everything to a lot of these environments, be it hyperscalers or be it enterprise, which we focus on both having the ability to embed algorithms or this accelerated compute infrastructure in close proximity to their existing data oceans or constant data creation models is everything to our customers. Okay, so assuming that the majority of the growth will continue to occur in regions, maybe not all in today's tier one region, but that it's still going to be sort of a regionally driven market. I guess the question is how quickly do we tap out these regions from a power perspective in particular, because unless you tell me otherwise, I think that tends to be the thing that maxes out first, unless you tell me maybe it's labor, but like, let's take, we talked about nova, right? And then you, how close to tapped out are we there? How much more can we possibly build in that region? Yeah, so you bring up great points, right? Where tapping out is the right word where in a lot of these markets power has been tapped out. I mean, it is a phenomenal market. I mean, some of the most recent stats, it has 0.5% vacancy rate, which is phenomenal. I mean, it's a multi gigawatt market. I think, you know, one of the things that differentiates how we view these markets is coming in and master planning, not only with like the entire market, but the market. And not only with like the entitlements and making sure you have access and the rights to the land, but that master planning arc is sometimes five plus years in a critical element that is working with a utility operator so that they know that, you know, when we say we're going to need a gigawatt, like with what we're building right now right next to the Dulles airport, that they have an understanding of that power requirement. And in a lot of the cases, they're able to meet that, but in certain cases, particularly in Northern Virginia, which has been wildly, you know, publicized is the, you know, some of the not necessarily generation, but the distribution of the grid has been challenged. And so we're always working with different solutions to overcome those shortcomings in the short term, but then ultimately working with that utility operator so that they get an understanding of the future growth associated with these markets because, again, this infrastructure and by saying this, this AI kind of secondary wave to cloud wants to be proximate to existing infrastructure. So being able to tie those things together and the power has been challenging, right? And I think it's challenging not only throughout the globe, but in a lot of these markets where you need to be working with the utility operators, which is why I love talking to the market and educating not only the in consumer around what's happening in AI and the workload, but ultimately the broader infrastructure like the utility operators and some of the other technology coming to market and solving for that power constraint. Yeah, you mentioned timelines. I wanted to ask you about that. So it's obviously location specific, but can you talk to me particularly relative to your history in this sector from, I don't know, from the beginning of development of a new site to operations of that site? What does that timeline look like? What's the range of timelines that that looks like today? And how does that compare to history? Yeah, so it is a challenging scenario where all things being equal. It takes about two years from concept to delivery to build out what I would say, versatile data center. And by versatility, I mean, comprehensive portfolio of solving for the hyperscaler needs, but also solving for the enterprise customers. So that's a 24 month window, but with the backdrop that we're experiencing, particularly with the power and the grid and just the overall equipment bottlenecks, I mean, utilities are requesting aggressive kind of four year rent projection that when you start to take that power down, you need to utilize it, which we've been very good stewards in a lot of these markets that when we do that master planning, we project that we are going to need 500 megawatts. We take down that 500 megawatts and operate that over a longer period of time. But some of these other, you know, interconnects are definitely no two markets are alike, but they're elongating even beyond the 24 months that it would take us to pull that together. So there's a lot of challenges there. And I referenced that at a high level, but some of the, you know, broader infrastructure constraints are transformer lead times are 50 plus weeks right now. Well, I was going to ask you about that. Right. Cause, cause transformer lead times and switch gear and stuff like that, that, that has been a challenge in all sorts of areas of the power sector. But I wonder whether because you have the added constraint of really long interconnection lead times, does it just mean that the interconnection is the long poll and the tent? And so you sort of, you have enough time that the transformer thing doesn't actually, or switch gear, whatever, doesn't actually delay projects because you happen to have another thing that takes even longer, or is it its own constraint? Yeah, there's, there's two high levels. It is a constraint. Don't get me wrong. There's two high level elements that we've been doing at digital for. I've been here 10 years, companies around 20 years is vendor managed inventory. So not only understanding, Hey, here's our portfolio in a single market, but really operating at a point where we're buying that switch gear, buying that infrastructure ahead of time where we can alleviate some of the bottlenecks. But yeah, that secondary constraint. And this is why I referenced earlier the master planning, showing and signaling to the utility operators and being a good steward of having top tier customers and credit worthy customers in our portfolio, which want to operate with us 10 plus years in that asset, balancing that together is everything. And so that interconnect from the utility has become constrained and some markets were always investigating different types of solutions, gas turbines. And even those are backlog plus 2029, right? Like that's, that's an extensive background as well. Yeah, we're definitely going to talk about bridge power because that is super interesting. Before we do though, one of the question I have for you about sort of how the market has developed is about scale of data centers. And we sort of alluded to this when we talked about, okay, that the training models need really big scale data centers. But you know, over history, right? Like you guys probably were developing 20 megawatt data centers 10 years ago, right? And now it's hundreds of megawatts. Or you mentioned a gigawatt. What does the demand picture look like for you? Does everybody want the biggest possible data center that you can build them? Or like, what is the nuance to that? Yeah, it's a good piece to dig into, right? Where there's a lot of noise. Like one of the things we've been joking about is a lot of bragawatts. Oh, I have a gigawatt. I have a gigawatt. And there's just so much noise out there that you really want to get underneath the workload and the durability of the company behind the workload. And that's where, you know, being a publicly traded operator, we're constantly watching that. And not everybody needs a hundred megawatt data hall. And there's certain use cases where a contiguous set of GPU infrastructure, which requires a very discreet capability, which we have some of the strongest heritage of engineering talent within digital. That have been solving this for the clouds. And now it has grown, but they want a contiguous hundred megawatt GPU array. So it's not just about the total capacity block, but then it's the densification of that capacity within the asset. And so we're always watching that. But what we're really seeing is inference can come in in like five ish megawatt blocks. And you can solve for it a bit differently. Now the densification is still there. And then the private AI pieces, there's hotspots where it can be, you know, a couple of months. But they want to be embedded in their existing portfolio of assets and balancing those two things as something we're always eyes wide open. But so this gets to an interesting question I've been wondering about, right, which is mostly what you hear about these days is right. You're hundred plus megawatt data centers getting built mostly hundreds of megawatts, if not gigawatts, right? And that that's what all the news is about. But if the individual inference workloads need to be, they can be five megawatt chunks, albeit you want some degree of densification, could you employ a strategy where you go build a hundred twenty megawatt data centers all in a region? Is that a viable approach? Because from a power perspective, my suspicion is that in the regions with a lot of data center activity already, it might soon, the scales might soon tip where it could actually be easier to build a hundred twenty megawatt things than a single two gigawatt thing or a single two one gigawatt things. Yeah, one of the most challenging things represented by AI is the ambiguity in the workload, right? Like there may be one workload that are like, yep, okay, I can take five or fifty one megawatts and I don't care where they are. That's less than, that's the outlier of what we're seeing is that to operationalize, they would really like it to be more of a contiguous scenario where they do logically think about them as five megawatt chunks and they want a bit of resiliency. You would, you would set it earlier. Reliability becomes increasingly important with inference because that's the consumption of that capability. What we're seeing is having a hundred megawatt hall where a bunch of five megawatt deployments could come in and be represented as inference. That has a higher viability for a lot of the hyperscalers plugging in, you know, a multitude of capabilities because it's not one AI workload. It could be a bit of, you know, I always challenged it not to reference a specific workload that I'm working on with a specific customer, but just some of the stuff spoke about publicly, but you look at like some of the most recent capabilities. I mean, when I was in an announcement from Google and like Gemini and the, and Vio three, those are probably compromised, like, like coming to market as a composition of multiple, you know, inference capabilities coming to market to meet that customer demand and you have to solve for the peak, right? Like people always forget like we learn this in the web scale, hyperscale. It's like the grid. It's the exact situation is the grid. Right. Exactly. We build the grid for the peak. We also build data centers. You have to. Absolutely. You mentioned the Braggle lot thing. I mean, that's the other thing that feels to me like we're clearly in a moment. Like two things can be true at the same time. There can be explosive actual demand growth for compute leading to actual need for lots of gigawatts of new data centers. And also it can be true that the volume of data centers quote in development and certainly the volume of load interconnection requests going to utilities is like an order of a low-income data center. Is like an order of magnitude more than is actually going to happen. Like both of those things can be true. But I wonder the degree to which that presents a challenge for folks like you because you're, you know, you need to get stuff built. But on the other side of the table from you is utility who's inundated with load interconnection requests and needs to figure out which things are real and which things are not. And I imagine that sort of gums up the works a little bit. Yeah. No, you're spot on, right? And I view that as three elements, right? Where, you know, the power, the amount of power and even the amount of financing required to meet these upper end projections, it doesn't exist. Right. And so you can't solve it all even if you wanted to. But then double clicking on aligning to your customer, right? And not all customers are equal and really understanding what their goals are and what they're trying to achieve. That's the heritage of digital reality, right? And that's where we've been doing that in pretty much every theater on Earth over multiple cycles, right? So AI represents a new cycle in a new way that's bigger and faster than we've ever seen before. But it takes partnerships to really pull that off correctly. And I think, you know, I couldn't say it better in that, you know, some of the works that we've been doing together collectively and also with utility operators having a communication with them to show that, you know, we, they won't over the course of the world. We, they won't overbuild unless they have a level of comfort that you will take and utilize that infrastructure they brought to market. So that ramp is everything to them. Working with that customer to show them that we have the right customers, we have the understanding to support the workload is everything because there's going to be some probably written about very big challenges and failures where they wanted a total capacity block, but they couldn't support the densification or they were just building for the spike. It was very spiky, but the longer term utilization is much lower than anybody had projected that will have very negative impacts on them to operate in a longer term horizon. So we're always focused on right types of customers, right types of partnerships to meet that peak load demand and the finance is required to hit it. Okay. So you mentioned bridge power. I want to, I want to hear how that is playing out in the market. We hear a little bit about it, right? There are folks who are saying, I mean, the famously the grock data center employed this substantially where you just say like, okay, the grid interconnection timeline is too long. And so I'm going to throw a bunch of generators on site and operate off of the generators as a bridge until the grid comes along for me. How common is that actually? Yeah, I think there's some outliers. Very few of it gets covered in the press because nobody wants to really go on to the market where the grid can't meet the customer ramp demands today. I mean, full stop. And I think, you know, one of the things that we're always looking at and I keep harping on this is that customer ramp requirements are an absolute key driver, but bridge power is one tool among many that we're always looking at, right? And, you know, natural gas, which is what you were referencing earlier. I think it has a solution in a shorter term horizon, but we're always looking at what are the longer term power generation capabilities that could potentially coming online and working with the utility operators on if it is grid constrained and they couldn't get the resiliency in the grid. Or if it's a generation challenge, we're always looking at how do you hit that peak demand with some of the batteries and some of the other technology that we've been seeing come to market. But yeah, it is an outlier. I think a lot of the utility operators are starting to understand that, hey, this is real demand and all aligned to it. They're not chasing the noise because nobody wants to invest in a bubble, right? Like I'll go on the record to saying that we're always looking at to ensure that we're not aligned to a bubble and that long term durable workload is there. But yeah, we too investigate net gas turbines. We investigate all options within the grid to overcome some of the shortcomings so that we can service our customers because I think it's often missed. You know, I talked to the utility operators that if our data center, you know, goes dark and is dormant, it doesn't allow our customers to grow and hit that next capability they need represented as AI or not or even cloud services. They have to be able to get revenue out of that very expensive infrastructure going forward. So they're looking for that long term master planned alignment to the utility operators. My sense is that there's kind of two different things you can do. If you think about bringing assets beyond the assets you would normally bring, right? You're always going to have backup power or whatever. But if you're going to think about bringing anything beyond that alongside behind the meter at a data center, you can either do the pure bridge power thing, which is we will supply. Our own generation and operate the data center off grid or partially off grid until the interconnection arrives or there's this other thing, which is, okay, we will proactively strike a deal with the utility, wherein we will bring our own generation or we'll bring our own batteries or whatever it might be and we'll have an interruptible tariff or something like that. And in so doing, we will get faster time to power, but it's a negotiated deal with the utility as opposed to a bridged utility. My sense is that the latter version is more common, more prevalent than the former. Is that right? Absolutely. Yeah, absolutely. And it's because we don't want to be all things to everyone because we won't be good at anything, right? Where you want to invest in the utility to allow them to do what they were good at and get over this challenge of the spike in demand, but that longer term environment should be with the utilities. And that's why we form long term relationships with the utility operator and act as a good customer to them on behalf of our customers. It's that chain of value that we're always watching because doing behind the meter, becoming a power generation capability, I think, is a short term gap. Nobody would be investigating that if we didn't have the constraint. And so that in itself tells you that a lot of individuals want to stay to their core stitching of, hey, what are we good at? What is our core capabilities we're bringing to market? But I think there's a short term. Let's meet the demand and the longer term who would be better at managing and operating these things going forward. The way I think about the archetype of what are the energy resources added to the center historically was you would have a UPS system and you would have backup power. And those are basically you had a diesel generator and EPS system and that like every day the center had those things. Do you think that'll change? Is there a new archetype? I was hopeful early on, but I've never seen one built with the right type of SLA. So set a bit differently. Yeah, I want a straight in facility where the software would have resiliency to fail over, where I didn't have to invest in all the diesel generators or some backup system if the utility failed. I haven't seen one come to market, but we've always been watching that because there is a lot of, I mean, believe me, I don't like to admit the secondary piece, which maybe a lot of the listeners already recognize. We operate almost three gigawatt diesel generators today. And so finding that balance and we do utilize some of those for peak loads and peak shaving and things like that. So we'd love to build an environment where for certain workloads we can build a different type of data center, but just because of the SLAs because of the requirements associated with these chips where they're liquid cooled, right? And that liquid cooling, you want almost three in worth of reliability where if that pump goes down that those hot set of infrastructure that accelerated compute continually has the right type of liquid for a certain period of time where it doesn't damage that infrastructure and we're talking about billions and billions of dollars for 30, 35 megawatt build up to 50 megawatts, it's very expensive infrastructure that we're watching. So high hopes, but it never really came to fruition. That's a really interesting point, one that I hadn't fully appreciated, that liquid cooled actually possibly increases the need for reliability, if anything. That thermal doesn't go away. Even if you had a workload that didn't need it, right? Because the promise, the thing that people talk about sometimes, it feels like it's this like ethereal concept. That never occurs really is like, oh, but there are some workloads that don't need three nines of reliability or five nines of reliability. It's like, okay, right? We're tagging, the cloud example used to be like, we're tagging photos for Google images or whatever. You should be able to operate those in a different manner. You shouldn't need the diesel generator. You should be able to place it wherever you want it. We should relieve all these constraints. I think there was this concept that, look, if the grid is as big a bottleneck as we think it will be, and it gets just harder and harder to find sites to build data centers at the scale that we need, then naturally, the market is going to start to separate out those workloads and put them in the places that you can still build. But there are all these other constraints. It's not just about the workload. It's, though that is challenging for the reasons you said before, it's also like you don't want to fry the chips, basically, and you've invested a lot of CapEx in those chips. Yeah, and you can double click on the infrastructure. There's probably some components that could go in and have a little bit more resiliency. But if you're liquid cooled, the thermal doesn't just displace itself. So you have to continually run liquid through that so you don't damage the chips. But to your other point, the availability zones have grown, right? Because of the constraint, lack of land, lack of entitlements, they've grown, but they're still proximate to that kind of CBD within these critical markets that we see evolving quite in the foreseeable future. We just see so much demand with as more customers start to utilize AI and the complexity of these models come to market, you only see that increasing. All right, final question for you, Chris. What are you most excited about? Like, what's the coolest thing that might be coming on the horizon in data center technology world? Yeah, so I think some of the newer designs we see and some of the efficiency factors. What's awesome is, you know, having two small children myself, we want to be good stewards of the power we take from the grid. So the P we ease increasing, shifting to liquid, you know, liquids 800 times denser than air, so you can get more efficiency factors out of that. I think that's going to be a net positive. And then, you know, I'm a technologist at heart. Some of the designs of the hardware coming to market are just phenomenal, right? And working with our partners that, you know, across the broad spectrum, watching and working within video, Vladimir Troy, who runs R&D there, we spend a lot of time with him, not just on the current generation that the public gets to see, but what are the two and three generations out? The ability of the token production against the Watts associated with that is going to be phenomenal. And hopefully everybody, all of our listeners here today, they understand the value of not only the data center, but these tokens and AI. I'm a very pro AI kind of individual. There's some, there's always going to be some negative associated with it, but what AI is going to be able to do for us, not only as individuals, but as a society. I mean, I'm pretty excited about some of the use cases and workloads that we've seen. One case I would leave you with is Gefian, a project we did out in Copenhagen. It's one of the largest DGX pods for Novo Nordisk. Just the amount of pharmaceutical work associated with that one deployment is just what it's going to be able to do for me. And it is very exciting to me. All right, Chris, this is a really fun conversation. Appreciate the time as always. I appreciate it. Thanks for the opportunity. Stay safe out there. Chris Sharp is the CTO of digital reality. This show is a production of Latitude Media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. Prelude backs visionaries accelerating climate innovation that will reshape the global economy for the betterment of people and planet. Learn more at PreludeVentures.com. This episode was produced by Daniel Waldorf. Mixing and theme song by Sean Markwand. Steven Lacey is our executive editor. I'm Shale Khan, and this is Catalyst.",
    "release_date": "2025-06-12",
    "duration_ms": 2064000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI5249436938.mp3?updated=1749695228",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-06-22T01:59:12.050559"
  },
  {
    "title": "The gas turbine crunch",
    "description": "Demand for turbines is growing fast, but so are lead times \u2014 causing serious headaches for developers. In Texas, one of six projects that pulled proposals from consideration for a valuable financing program cited \u201cequipment procurement constraints\u201d as the reasons for its withdrawal.\n\nLead times are stretching to four years and sometimes more. Costs are climbing. So what\u2019s behind the bottleneck?\n\nIn this episode, Shayle talks to Anthony Brough, founder and CEO of Dora Partners, a consulting firm focused on the turbine market. Shayle and Anthony cover topics like:\u00a0\n\n\n  \nWhy previous boom-bust cycles in turbine manufacturing have left the industry skittish \u2014 and why Anthony says leaders are approaching this new peak with \u201cguarded optimism\u201d\n\n\n\n  \nThe competing demands on the turbine supply chain, including from power, oil and gas, and aerospace industries\n\n\n\n  \nHow lead times have ballooned to four years and, in some cases, even longer\n\n\n\n  \nFactors affecting the market beyond load growth, like renewables, storage, affordable gas, and coal retirements\n\n\n\n  \nHow investment in tech innovation has raised turbine efficiency\u00a0\n\n\n\n  \nHow the industry is preparing for hydrogen \u2014 if hydrogen scales up\n\n\n\n\nResources:\n\nLatitude Media: Engie\u2019s pulled project highlights the worsening economics of gas\n\nLatitude Media: High costs, delays prompt withdrawal of five more Texas gas plants\n\nPower Magazine: Gas Power's Boom Sparks a Turbine Supply Crunch\n\nMarketplace: Will we have enough natural gas turbines to power AI data centers?\n\nCTVC: \ud83c\udf0e Gas turbine gridlock #236\n\n\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is executive editor.\n\nCatalyst is brought to you by Anza, a platform enabling solar and storage developers and buyers to save time, reduce risk, and increase profits in their equipment selection process. Anza gives clients access to pricing, technical, and risk data plus tools that they\u2019ve never had access to before. Learn more at go.anzarenewables.com/latitude.\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.",
    "summary": "The podcast discusses the evolving gas turbine market, focusing on factors like lead times, pricing, and market dynamics. It delves into historical trends, recent developments, and key players like Mitsubishi, Siemens, and GE Vrenova. The conversation touches on the impact of raw materials, aerospace industry demands, and market drivers such as grid-scale battery storage and data center growth. Insights are shared on the current state of the industry, including timelines for turbine delivery, pricing trends, and the varying scales of projects being pursued. The discussion highlights the interplay between the electricity generation and oil and gas sectors in shaping the gas turbine market landscape.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shail Khan and this is Catalyst. What's your outlook on timelines? Do you think that the lead times just get longer and longer and longer? Are we at the peak there? Is it going to get worse? Do we know? Good question. I actually don't think they're going to get much worse. I think all of the OEMs are working like crazy to try and shorten up their lead times or at least make sure they don't get worse. Coming up, it's due time we talk about the gas turbine market. Do you want instant access to energy storage supplier pricing that's project specific? Or the ability to compare domestically made battery and PCS options across the market? ANSA now offers the industry's first battery energy storage data and analytics platform to make better development and procurement decisions. ANSA provides in-depth commercial, technical and risk data and analytics to help developers choose the best equipment for any project. Improve your returns and save months of evaluation time with ANSA. Learn more about ANSA's energy storage subscriptions at go.ansarunuables.com slash latitude or click the link in the show notes. Imagine a world where connected devices like EVs, home batteries and smart thermostats work together to support a more efficient and reliable power grid. Well, you don't have to imagine it anymore. This vision is a reality today thanks to Energy Hub. With Energy Hub's Edge Derm's platform, utilities can create virtual power plants through customer centric flexibility programs, making it easy to manage distributed energy resources and balance the grid. Unlock grid flexibility and reliability through cross-DER management with Energy Hub, the trusted Edge Derm's leader. Visit energyhub.com to learn more. I'm Shail Khan. I invest in early stage technologies at Energy Impact Partners. Welcome. So it's a good time to be in the gas turbine business between the relaxation of emissions constraints and the rapid load growth that we've discussed in numerable times on this podcast before. Perhaps the biggest winners are the companies like Mitsubishi, Siemens and GE Vrenova who make turbines. Of course, one result of that is that they're pretty well sold out and they have a lot of pricing power. So it's an interesting moment where momentum is clearly flowing toward natural gas power generation, but it's also actually pretty difficult to build any more of it, especially in the near term. Anyway, it's a really interesting market and one we haven't really talked about here. So let's fix that. Directify the situation. I brought on Tony Bruff. Tony is the president of Dora Partners, which is an Energy and Gas consultancy specializing in what's going on with the gas turbine industry. Here's Tony. Tony, welcome. Thank you. Glad to be here. Start by you giving me a little bit of a recent history lesson on the gas turbine market. How has it been developing over the past? I don't know. You tell me what the relevant timeframe is, but a couple of decades. That's a good question. There's been a lot of dynamic change over the last few decades. I mean, it used to be in the 70s and 80s, there were pretty much just two major OEMs, General Electric and even Westinghouse at the time, now owned by Siemens. But really, the number of OEMs have been, have gravitated towards three major OEMs, MHI and Mitsubishi Heavy Industries, Siemens and General Electric or now it's GE, Vernova. There are other strong players in the market. For example, Solar Gas Turbins, a division of Caterpillar is a significant player in the small gas turbine market. So how has it changed? It's really evolved not just in terms of the OEMs, but also there's been several column bubble periods. There was a big bubble period in 1998, 1999 through 2001 and then the market basically fell off a cliff and it slowly built back up to a really good set of years back in 2012 and then it kind of fell off again. Now we're kind of at another peak, but I would call today's peak more of a real market-driven, realistic set of scenarios that's driving the market today. That's interesting that you say that. I knew it was characterized historically by these sort of boom and bust cycles and I think we've seen this in other sectors in the electricity market as well. We've talked before on this podcast about Transformers, for example, where you have these very long lead times and one of the reasons that there are still such long lead times as the transformer manufacturers have gotten burned in the past by building out more capacity and being oversupplied into a market that turned out to bust. I had a sense that there's a similar dynamic in the turbine world, but it sounds like you're saying this one seems like it's different. What drove those bubbles that then burst in recent history in the market? Was it over exuberance about new gas generation build that just didn't come to fruition or something else? No, actually, there's actually several different dynamics and that's a really good question. If you go back to that first big bubble back in 1998 through 2001, that was really being driven by an artificial demand created by Enron. I mean, they clearly were sending artificial signals to the marketplace that were driving up the cost of electricity significantly in several regions of the country, California, Texas and other areas. That was also right around the same time that deregulation was coming into play. Those two factors created a lot of panic in the marketplace. Keep in mind, the large utilities in the 60s and 70s, everything was regulated. They were pretty much just, they only built when they could get the public utility regulators to approve investment. As deregulation came into play, deregulation came into play, everybody was just basically learning, okay, how do we make money now that there's regulated, deregulated and semi-regulated markets to deal with across the country and even to a degree in areas outside the country in Europe and Asia, for example. And then the Enron thing just created a significant, I would say, artificial signal to the marketplace. So those two factors really drove a bubble in the market and a little bit of it was unreal. I would say at least half of the volume was artificial. Maybe to put a finer point on that then, because this ties to both the deregulation and Enron, which obviously are tied to each other, but is what was happening there a lot of speculative development of what would be merchant gas projects that never came to fruition? I want to draw that distinction because what's interesting about today's moment is that like, I don't know, I don't think there is a lot of new merchant gas being developed. Mostly what's happening is it's having utilities saying we need it for, because we need more capacity or it's data centers and they'll be the long-term offtake on the project. So you're actually not like subject to the merchant risk. You are subject to the will this data center ever get built risk, which is kind of a different thing. Well, that's true. But most of that activity was not merchant. Well, there were IPPs. There were a lot of IPPs and independent power producers that were speculating without a doubt. But there were a lot of orders that were canceled even by large, regulated and semi-regulated utilities like Southern Electric. They had a huge set of orders and a lot of that stuff had to get either canceled or bought and then resold on the marketplace. It was a real disaster for everybody when the bubble burst. So we'll get into the market today in a little bit more detail. But do you think that there is, given that history, given that there is some boom and busts and some cycles that the market has gone through, does that lead to a more conservative approach from, as you said, basically the three big OEMs that control what 70% of the market or something like that to expand capacity? Or do you think that they share the view that you expressed, which is actually this one's real. I'm not too worried about being overextended if I expand capacity. Now I'm sold out through whatever it is, 2029, 2030. And so I should just build as much as I possibly can. Like where do you think they are on the spectrum? Yeah. Well, I think there's guarded optimism, very guarded optimism. I mean, certainly all of the OEMs are investing in the future for new production capabilities, particularly Siemens and General Electric, or GE, or GE, or know why I should say. The other thing to keep in mind is about half of the gas turbines that's ordered in the marketplace aren't even for the electric power utility market. They're for the oil and gas market. And so all of the supply chain that's feeding those three OEMs and others are also competing for supply chain resources going into the oil and gas market. And some of those OEMs are also delivering into the oil and gas market. So there is a lot of interesting dynamics going on. And it's important to look beyond just the power generation or the utility sector when you think about what's happening in the marketplace. Yeah. Can you say more about that? I think that's one thing people don't always appreciate on the outside. What does that supply chain look like and what are the big categories of sort of end markets that these products gets sold into? Right. Well, that's a great question. I have basically, I described the supply chain for the gas turbine industry in four different levels. I call level zero is raw materials. So you know, you talked about transformers while copper is clearly a big raw material when it comes to transformers. But for gas turbines, it's the super alloys, nickel based alloys, chromium, all those other expensive key ingredients, titanium, all those things that are involved in the raw materials for gas turbines. That's what I call level zero. Level zero level one is actually manufacturing the raw pieces of product, for example, blades and veins and things of that nature that are being cast or forged. Level two is where they're actually manufacturing the gas turbine from all those components that were developed on level one. So that's where the OEMs are producing a, you know, what I call flange to flange gas turbine. And then level three, which is the fourth level, is where it all gets put together into a final package and delivered to an operator site, installed, commissioned after market activities, all that sort of thing. So all of those and then when you keep in mind, levels zero and level one are also being impacted by the aerospace industry. You know, there's something like 40,000 aircraft in backlog right now in the world. So guess what? All of the same level zero material suppliers and all the level one forgers and casting shops and things of that nature in what I call level one, they're all supporting the aerospace industry at the same time. So these, you can't look in isolation at the electric power utility market for gas turbines in isolation because you have to consider what's happening in the aerospace industry and what's happening in the power and the oil and gas industry. Because as I said, 50% of the industrial gas turbines that are delivered in any given year approximately aren't even for the electric power utility sector. They're for the oil and gas sector. And in terms of the market dynamics today, I guess, obviously we have this booming demand for gas turbines in the electric sector, whether on grid or off grid. Some people are doing gas turbines for bridge power, for data centers or whatever. But let's call that all in the electric sector ultimately. Agree. Is the demand, I mean oil and gas prices are low right now. Does that mean that there's low investment on that side? And so most of the demand is shifting to electric power generation or is that not sort of how the cycle works on the oil and gas side? Well, you know, that's a great question, Shala. The good news is for those that are involved in oil and gas industry is by and large, most of the large oil and gas players have long-term thinking in mind. So they're making five, seven and 10-year strategy developments for strategy. Now, well, in any one year, they might reduce their order activity because the oil and gas prices are down. Absolutely, that's correct. But in the long run, oil and gas companies basically stick to a strategy, that an investment strategy that keeps them investing. And typically what we see are what I call seven-year cycles in the oil and gas industry. It'll go up peak at about year seven and then come back down, slowly come back down and then go back up again and another seven-year cycle. And it's all driven by upstream activity for development of oil and gas, midstream for transmission and then downstream where you have a lot of LNG, refinery activity, all that sort of stuff. And all those things are somewhat independent of each other. So it does level out the market for the oil and gas industry a little bit, which means that the investment stays. And when you look at the midstream oil and gas market, most of the players midstream, they're making their money not on the price of oil and gas, but on transmission of oil and gas. So they're very much, I don't like to use the word immune, but the sensitivity to the price of oil and gas is really low. They're still going to make money because everybody's still using the oil and gas, albeit maybe at a lower price, but they're taxing fee for moving the oil and the gas through the pipelines is still pretty robust and they're making their money. So they're investing. Catalyst is brought to you by ANSA. ANSA offers a one-of-a-kind data and analytics platform and advisory services to support better project development and procurement decisions. For energy storage developers, ANSA's platform provides crucial information that you never had easy access to before. Now at your fingertips is real-time pricing for a long list of system configurations to suit any project. ANSA provides a 360-degree view of the market with lifecycle cost analytics and commercial technical and risk data. With ANSA, developers can easily determine which products to use in their designs, finance models, and RFPs. Learn more about how ANSA helps save time and maximize profit at go.ansarunuables.com slash latitude. Catalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. The EnergyHub platform takes the guesswork out of balancing energy supply and demand. It uses machine learning to control customer-owned distributed energy resources, like EVs, home batteries, and smart thermostats, to precisely shape load profiles for grid flexibility and reliability. As the industry leader, EnergyHub helps more than 80 utilities manage 1.6 million devices. That's more than any other edge derms on the market. Click the link in the show notes to learn more or go to energyhub.com. I want to talk about, I guess, two primary things with gas turbines in the market, particularly for electricity generation, where I spend a lot of time right now. One is timeline and the other is price. We hear a lot right now in the news about both of those things. On the timeline side, we hear about folks like Givernova being sold out through 2029 with an order book behind that that they can sell as much as they can build, at least at the moment it seems. Then, on price, I don't know visibility into the actual market pricing, but what an interesting data point that you might have seen recently was, I think it was John Ketchum, or somebody from Nextera said, a decade ago, I could have built a new natural gas project for $750 a kilowatt. I think I'm going to get the numbers close, but not exactly right. Today, it would cost me $2,500 a kilowatt. I don't know how much of that is the turbine itself, but I'm interested in the relationship between how long it takes to get new turbines and how expensive they are becoming. Yeah, good question. That $750 was for a combined cycle plant. I think the $2,500 is a bit aggressive, but it's definitely up around 30 to 35% over the last five years. The price is definitely up. I track all of that very closely. Is it purely a supply-demand thing? No. Yes and no. Again, raw materials at level zero, raw materials are up everywhere. Even before all of the tariffs come into place, you were seeing demand on aluminum, nickel-based alloys, titanium, all of these things are all interrelated. Again, I'm coming back to the aerospace industry. When you've got the aerospace industry ordering 40,000 aircraft, that's at least 80,000 gas turbines. They're all drinking from the same supply chain, for the most part. No, it's not just supply and demand. It's also being driven from, well, it's of course, supply and demand is related to the cost of raw material. I don't want to discount that. But certainly raw materials is a big part of it. If you look at some of the US government's tracking of producer price indices on all of these different elements, you'll see a pretty significant bump in the last three years that is very indicative of what you and I are just talking about. What's your outlook on timelines? Do you think that the lead times just get longer and longer and for a while? Where are we in the cycle of the lead times having been getting longer? Are we at the peak there? Is it going to turn back the other direction? Is it going to get worse? Do we know? The good question, I actually don't think they're going to get much worse. I think all of the OEMs are, in fact, I know all the OEMs are working like crazy to try and shorten up their lead times or at least make sure they don't get worse. Part of the reason why is customers are eventually just going to get weary. Say, OK, we're just going to put things off because as it is, they're putting down 15, 20, 25 percent non-refundable deposits. All of those things are very painful for customers. These OEMs have been living through these busts and booms before, and they don't want to upset their customers too much. So they're all working hard to at least flatten out the timeline and if not improve it. And I'm seeing signs of that across the board. Today's timelines are in the lead times are in the four to five year range. Do I have that about right? I would say between 36 and 48 months, I suppose there are some OEMs that are claiming up to 60 months, but I would say on average, it's around the 48 month period. Got it. The other thing I'm curious about is size. There's obviously, it's not a monolithic market. Even within the power generation, there's different products that serve different use cases and at different scales. And I think the scale question is sort of an interesting one because the question is what's getting built or what is being designed to get built. Large scale generation, gigawatt scale type of stuff is the fact that data center is driving a lot of this, changing the desired scale of the end customer and what does that mean for the products in the supply chain. Good question. Well, I actually look at the market drivers. I think there's at least five major market drivers. And in each one of those market drivers, small, less than 20 megawatt gas turbines, turbines 20 to 100 megawatts are seeing a different set of dynamics. And then what I call jumbo sized units, which are 150, 250 megawatts and above those I call jumbo units. They're all being affected differently, driven by the different market drivers. And I say there's at least five market drivers in the marketplace. One is grid scale battery storage. Number two, cold plant retirements. Number three, grid scale renewable energy expansion. Number four, the development of rapid development of data centers and artificial intelligence exploitation or expansion. And then just the availability of natural gas and its affordability is I'd say the fifth driver. And if you look at each one of those different drivers, those three sized units are all being affected differently. And if you want, I could actually walk through each of the different drivers and then explain how each one of those three different markets are being affected. Yeah, I mean, it's interesting that you described that, right? Some of those drivers, I would think would be a suppressant on demand. So I agree. The growth of grid scale energy storage, right? Grid scale energy storage is sort of a gaspeaker replacement product on the grid, right? Predominantly. So I would presume that suppresses the market to some degree. But maybe are you saying it results in smaller units being developed on the grid? Or what's the dynamic? You're a great lead in the shell. Actually, you would think, just and generically, you think off your head, Oh, well, grid scale battery storage, that's got to drive down the demand for gas turbines. Actually, in some cases, the answer is exactly right, but not in all cases. So I mean, if you actually look at the market and what's happened with grid scale, I would say large jumble sized units. Absolutely. They are being it's a negative, it's a negative dynamic. If you look at gas turbines, say 40 to 100 megawatts, actually it's a it's an opportunity because there are several of the developers are counting on gas turbines to recharge or develop what I call hybrid systems that use gas as a when it's cost is low to spin up the gas turbine and recharge their grid scale battery storage. So they're not just relying on renewable energy to recharge their batteries. So, so and then when you look at the real small gas turbines, generally, they're not being quite as affected by the grid scale battery storage segment. But clearly, as you correctly pointed out, or you felt the intuitively, yeah, large power plants, jumbo units, it's a negative, but for gas turbines, 40 to 100 megawatts, it's actually a little bit of a positive influence. And then I imagine right, coal plant retirements, big projects coming offline, presumably get replaced with big assets, at least if you're trying to do one for one. So I assume it's that that is all things equal a positive signal for larger scale turbine. Yeah, for for coal plant retirements, it's really for all three segments, the the less than 20 megawatts, the 40 to 100, and the large jumbo, it's a positive influence, but mostly for the large jumbo units. But interestingly enough, you see a lot of mobile power and peaking units being installed as support for the grid where coal plant retirements are occurring. Well, you see that in the context of some of your other drivers, right? Like, I know of some projects that are coal plant is retiring, we're going to replace it with like a big solar plus battery installation. And then we probably need some smaller scale peaking gas to supplement that. Yeah. Right. It's like that kind of thing. Well, yeah, if you look at if you look at grid scale, renewable energy, I mean, the amount of grid scale activity is going up just explosively, it's expected to double in the next five years and the cost of a levelized cost of electricity for for solar power is way, way down. But so that has a negative impact on the large utility or jumbo size gas turbines. But but definitely it has a positive influence on mobile units, peaking gas turbines, just because when the when the sun goes down and the wind stops blowing, you know, you've got to have backup power. And those units, I would say from about 15 megawatts up to 100 megawatts are actually very good investments for, I call it, renewable offset. And when you mentioned the mobile thing, I mean, those types of installations, you don't necessarily, you're not looking for mobile generators. I think of the mobile generators as being a good fit for either like an off grid type application, you see a lot of this in the oil and gas world or for bridge power type situations where you're looking to this is what you see now where look, we need we need power now because we're building a data center and the grid connection is going to take three to five years. So we need a bridge, but we don't need it forever. Am I wrong to think that that's where the mobile power segment ends up? Well, you're not wrong, but you're not 100% right either because clearly when it comes to data centers and artificial intelligence, mobile power and even permanent onsite power is as a backup to the and supporting the demand for data centers is a very strong influence on both mobile power and permanent onsite units. But believe it or not, there's a lot of utilities who will buy mobile units. They'll put and they'll locate them in a what they call a grid sensitive area. And over the course of five to 10 years, they'll improve their infrastructure. And then they'll move those mobile units to another sensitive grid, grid sensitive area. And so the mobile power is just been a fantastic opportunity for basically three companies, solar gas turbines, the Division of Caterpillar, the GE, Vernova for their mobile units and for MHI arrow power for their mobile units. Those three players have done extremely well with mobile powered units for a variety of reasons, even in oil and gas. But for the reasons that you and I have just discussed in the last 10 minutes, absolutely. And I don't see that market going away at all. Yeah, if anything, it's getting supercharged by additional use cases as we've just absolutely, which gets to that sort of 100% on. Yeah, which gets to that sort of the one that seems to be the biggest net new thing that's happening right now, but like is is a huge deal is all the gas turbines being developed for data centers, whether mobile or stationary, right? But you see like, you know, there's that partnership between Chevron and engine number one where they've they've secured gigawatts worth of GE, Vernova turbines, they're going to go use those to develop a bunch of data centers. And then I'm not sure whether those are actually intended to be permanent or just bridge power. But like, that's one example amongst many. And it seems to me is the is the factor that's kind of tipping this market over the edge from just being a generally tight market to like a historically tight market. Yeah, well, you're making a good point. I mean, if you look at data centers, there's like 11,000 data centers serving the digital commerce and artificial intelligence community already around the world. And because they many of them have been around the average electrical loads around four megawatts. But there's like 1400 new data centers planned in the United States alone. And over 1000 of those are all large scale. They're going to need a lot more than four megawatts. I mean, some of those data centers, their electrical load is more than the community around them. Yeah, there's a there. I mean, it's just like, electric co op, I think it's Susquehana co op or something like that in Virginia, that like, I remember seeing some some filings and regulatory filing, where they were projecting their load growth to like more than double based on purely a couple of data centers that are coming into the territory. Yeah, in fact, you you just touched on a good point that whole region around Virginia, Washington DC, that whole area, there's more data centers in that area than anywhere else in the world. Right. It's just a mecca of data centers. But but these dynamics are really interesting around data centers. And I don't think it's going away. I think it's you see people using artificial intelligence more digital commerce is just booming and it's not going away. To me, two things can be true at the same time. I think it can be true that this is the demand from the gas turbine OEM perspective, the demand is real. The market will buy an enormous volume of new gas turbines to serve these markets. And that's also true, by the way, of like utilities who are trying to manage interconnection requests and so on. Like it can be true that that is real and also that we are in a speculative bubble on the development side, because there will not be, I mean, as you correctly said, there are a thousand large scale data centers in development in the United States. And I will state categorically, I don't think there will be a thousand new hyperscale data centers in the United States anytime soon. I don't think there's actually that much demand for it. So like both things are true. There's all these there are cowboys out there trying to take advantage of the moment. So the challenge, of course, then if you are on the supply side, whether it's a utility or you're a gas turbine OEM, is how do I make sure that the buyers I'm signing up with are real? And that gets to your point of like these big non-refundable deposits. If you have all the market power, that's sort of how you take advantage of it. So it seems like they're doing the right thing in that regard, at least. Yeah. Well, and the other dynamic to keep in mind, Shell, not only that is, it's not just one of these market drivers that's making things happen. It's all five of these market drivers that I've mentioned, including the price of natural gas, which is very affordable in the United States. So when you combine all of these what I call market drivers, it creates in a situation where these OEMs are relatively comfortable building out a supply chain strategy to support the market because they're not just relying on one dynamic. Back when I was executive at one of these large OEMs 20 years ago, we were basically counting on only one of those market drivers to happen and one of them didn't happen. And so it hurt our strategy. But now you have a situation where you have three to five key market drivers that are all paying the market. And so it creates a little bit of, I would call, risk comfort for the OEMs because they know it's not just one thing they're counting on to make the market move. Okay, final question for you, I guess, is on the technology side, is there any these are pretty mature technologies? Is there any significant innovation that either we have seen recently or that you expect to see in the next few years? Will the market change as a result of technological innovation or is this just a rinse and repeat and stamp them out as much as we can kind of a situation? Yeah, good question. I would touch on two areas. First off, all of the OEMs have spent an enormous amount of money trying to get, and they've been very successful in slowly increasing the efficiency of their combined cycle plants. I mean, it used to be combined cycle plants, average efficiency was about 55%. And they slowly crept it up to 60. And then they kind of hit a dead spot and they couldn't figure out how to get above 60. And then they started to evolve developing their, I'll call it a very holistic strategy to the power plant. So it wasn't just the gas turbine, it was the HRSG, it was a whole, all sorts of different technical factors that they were lovers that they were pulling to try and squeeze more efficiency out of their power plants. And they crept it up to 60. And they got to 60 and a half, 61, 60.3. I mean, they're starting to push 62% efficiency and more. And I don't think they're going to quit because if you look at the, if you look at the levelized cost of electricity, and that's a big factor that these utilities are using and assessing which OEM they're going to use, fuel is a big, big element in the levelized cost of electricity. So the more efficient, and the more efficient, the more effective that the OEM is in convincing that customer that they have a more efficient unit and even guaranteeing it, the better for them, they'll be more competitive. So efficiency is, it's not coming up by leaps and bounds, but it's a gradual increase over time. It's been quite remarkable, you've asked whether you got to hand it to all three of the major OEMs that they've been able to make some very significant improvements in efficiency, albeit very difficult. The second big area of technology development is converting their combustion systems over to using hydrogen. Now, I put all of that into a big, big set of quotes, because while they're all working on hydrogen, and they can, and they've all demonstrated to a degree some capability of operating in hydrogen, the biggest problem is where are they going to get it? The amount of hydrogen that you need to run one of these large jumble sized units, it's just an enormous amount of gas, and where are you going to get it from? So while they're all spending a lot of money, and their engineers are working very diligently and doing some fantastic development, I have some doubts as to whether the market will actually see a significant increase in purchases of gas turbines that actually are using hydrogen. But clearly, they're all working on it. Yeah, it's a, you know, the bet is if we build it, they will come. If we build hydrogen ready gas turbines, then the market will show up for them and the hydrogen will be there. And of course, it's a dynamic market for hydrogen at the moment. So we'll find out whether that plays out well for them. But good point on efficiency, it's like a steady grind, but it adds up a lot over time. Yeah. Tony, this was awesome. Really appreciate the time. Thanks so much for joining. Thank you. Tony Bruff is the president of Dora Partners in Energy and Gas Consultancy. This show is a production of Latitude Media. You can head over to LatitudeMedia.com for links to today's topics. Latitude is supported by Prelude Ventures. Prelude Backs Visionaries accelerating climate innovation that will reshape the global economy for the betterment of people and planet. Learn more at PreludeVentures.com. This episode is produced by Daniel Waldorf, mixing and theme song by Sean Marquand. Stephen Lacey is our executive editor. I'm Shale Kahn, and this is Catalyst.",
    "release_date": "2025-06-05",
    "duration_ms": 2404000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI6893048605.mp3?updated=1749250187",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-06-22T02:03:23.156053"
  },
  {
    "title": "How geothermal gets built",
    "description": "Geothermal seems to be nearing an inflection point. With rising load growth, clean, firm power is more valuable than ever. Next-gen geothermal players like Fervo Energy and Sage Geosystems are signing PPAs with major tech firms. Even U.S. Secretary of Energy Chris Wright \u2014 a known critic of renewables \u2014 has praised the potential of geothermal.\u00a0\n\nThe size of the U.S. geothermal resource accessible through next-gen geothermal technologies like enhanced-geothermal systems is enormous \u2014 potentially thousands of gigawatts. But tapping into it hinges on figuring out the economics.\n\nSo what does it actually take to develop a geothermal project \u2014 and how are new tools reshaping the process?\n\nIn this episode, Shayle talks to Carl Hoiland, co-founder and CEO of geothermal energy company Zanskar, which uses AI for enhanced geothermal exploration. Shayle and Carl cover topics like:\u00a0\n\n\n  \nWhy geothermal stalled \u2014 and what\u2019s changing now\n\n\n\n  \nThe full step-by-step process of developing a project\n\n\n\n  \nHow to avoid exploration risk, also known as dry hole risk\n\n\n\n  \nMethods for estimating resource size and managing depletion risk\n\n\n\n  \nThe geothermal supply chain\u00a0\n\n\n\n  \nHow permitting is speeding up\n\n\n\n  \nCarl\u2019s outlook for when and where development is likely to happen\n\n\n\n\nResources:\n\nLatitude Media: Geothermal could meet 64% of hyperscale data center power demand\n\nLatitude Media: Why geothermal might benefit from Trump\u2019s tariffs\n\nThe Green Blueprint: How a text message launched a geothermal revolution in Utah\n\nLatitude Media: The geothermal industry has a potential ally in Chris Wright\n\nLatitude Media: Why California lawmakers are warming to geothermal\u00a0\n\n\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is executive editor.\n\nCatalyst is brought to you by Anza, a platform enabling solar and storage developers and buyers to save time, reduce risk, and increase profits in their equipment selection process. Anza gives clients access to pricing, technical, and risk data plus tools that they\u2019ve never had access to before. Learn more at go.anzarenewables.com/latitude.\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.",
    "summary": "Latitude Media is hosting the Transition AI conference on June 12, 2025, focusing on energy infrastructure in the AI-driven load growth era. Key participants include FERVO Energy, Form Energy, and more. The podcast delves into geothermal power's history in the US, its challenges, and resurgence through enhanced geothermal systems (EGS). Carl Huyland, CEO of Zansgar, discusses geothermal exploration steps, from temperature gradient holes to drilling production wells, highlighting the shift towards EGS for engineering geothermal resources. The podcast emphasizes the need for improved exploration methods and the potential of geothermal energy in the evolving energy landscape. Energy Hub and ANZA are featured sponsors providing innovative solutions for grid flexibility and energy storage analytics.",
    "transcript": " Mark your calendar for June 12, 2025. Latitude Media is holding its fourth transition AI conference in Boston. This year's theme, energy infrastructure in the era of AI-driven load growth. We're going to bring together investors, developers, researchers, and tech companies to talk about the creative ways to meet data center demand. And companies include FERVO Energy, Form Energy, Scale Microgrids, Spark Fund, KKR, Generate Capital, Orenia, FlexGen, National Grid Partners, and more. Plus, we're going to have a live open circuit episode featuring Caroline Golan from Google, and a live green blueprint episode featuring Rick Needham from Commonwealth Fusion Systems. Get your ticket at latitudemedia.com slash events, podcast listeners, get 10% off their ticket, use the code latitudepods10 at checkout latitudemedia.com slash events. We will see you at transition AI. Latitude Media, covering the new frontiers of the energy transition. I'm Shail Khan, and this is Catalyst. When you look at the full stack of kind of near-term EGS and conventional, we really are talking about hundreds of gigawatts to terawatts of resource potential. There's much potential to give as, say, the entire Gulf of Mexico from an oil point of view. Coming up, the heap beneath our feet. Do you want instant access to energy storage supplier pricing that's project-specific, or the ability to compare domestically-made battery and PCS options across the market? Anza now offers the industry's first battery energy storage data and analytics platform to make better development and procurement decisions. Anza provides in-depth commercial, technical, and risk data and analytics to help developers choose the best equipment for any project. Improve your returns and save months of evaluation time with Anza. Learn more about Anza's energy storage subscriptions at go.anzarenewables.com slash latitude, or click the link in the show notes. Imagine a world where connected devices like EVs, home batteries, and smart thermostats work together to support a more efficient and reliable power grid. Well, you don't have to imagine it anymore. This vision is a reality today, thanks to Energy Hub. With Energy Hub's Edge Derm's platform, utilities can create virtual power plants through customer-centric flexibility programs, making it easy to manage distributed energy resources and balance the grid. Unlock grid flexibility and reliability through cross-DER management with Energy Hub, the trusted Edge Derm's leader. Visit energyhub.com to learn more. I'm Shail Khan. I invest in early-stage climate technologies at Energy Impact Partners. Welcome. So, is geothermal having a moment? Here's the case for its clean-firm base load power, which is a hot commodity right now. Hyperscalers of all expressed interests, some of them have signed PPAs. Fervo Energy notably has PPAs, both with utilities and with Google for hundreds of megawatts of new development. The Trump administration, in particular to the Secretary of Energy, Chris Wright, came into office with very positive rhetoric about geothermal, in contrast to other forms of renewables. The case against is the big, beautiful bill that just passed the House last week, which throws the geothermal baby out with the wind and solar bathwater, basically. So, all of that enthusiasm is not currently reflected in legislation, at least, though, let's see what happens in the Senate. Anyway, I think regardless, the case 4 is a lot stronger than the case against here, to be honest. And so, I wanted to bring on Carl Huyland to talk a little bit more about geothermal at a high level. Carl is the CEO and co-founder of Zansgar, which is a startup that's leveraging AI to enhance geothermal exploration and ultimately production. But beyond that, Carl is basically an encyclopedia of geothermal, as you will soon see, and I have taken great advantage of that myself. So, it's your turn. Here's Carl. Carl, welcome. I shall. It's great to be here. All right. I want to start with you giving me a history lesson, as you have given me before. But walking through the history of geothermal power in the United States, in brief. Fantastic. So, humans have been using geothermal energy for many purposes for a long time. But, really, you see the origins of this power industry emerge in the United States in the 1960s, with the initial development being in the geysers field in Northern California. And it's really ushers in this early mover experimentation phase. You start ushering in this new phase of early geothermal developments, and they're really exploring for the first time the ability to use this resource to generate electricity. And it's fairly basic at the time. Just take the steam that's coming out of the ground, drive it through a steam turbine to generate electricity. And usually, they were evaporating it at that point. But we see it. Most of the United States growth actually happens in those first one to two decades. And for a while, it looks like geothermal is just going to take off. It's scaling faster than any other renewable at the time. And through the 1980s, we had gigawatts of capacity in the United States. But then, things kind of come to a halt. And you go through this period through the 90s and 2000s, where you really see almost no growth. And then another tip up in the late 2000s, early 2010s, and then it's been flat almost until just recently. And even that tip up in the late 2000s and early 2010s, how much did we build during that period? We added hundreds of megawatts, but they were really in some ways offsetting some of the losses that we saw in some of the early steam fields. And so in terms of total installed capacity, it's meaningful, but it's relatively minor, and not as much as we were hoping. I think a lot of people know this to be true of nuclear. We built a lot of it decades ago, and then we stopped building new stuff in the US. I think a lot of people don't appreciate that the same thing is true of geothermal, and actually, interestingly, on a roughly similar timeline, which I find kind of intriguing. Not exactly the same, but similar kind of story. So what happened? Why did it stall out? Well, I think there were a couple of things that happened in the early days. The early technologies could really only work with very high temperature steam. And so they were looking for exceptional locations in the Earth's crust, where this was 200 Celsius and often higher. And it turns out those were relatively rare. And the further down in temperature, you go the more abundant they become. But the other part of it is that we had so many failures in trying to drill into these resources, where there was a hot spring or geyser at the surface. They thought this was a no-brainer. And when they come in and started drilling those deeper wells, they would not find the resource they were expecting. And so this is what we call exploration risk, or dry hole risk in geothermal. And it led the industry to start having enough failures to scare capital investors to say, whoa, should we really be throwing more money after this? And this kicks off really a race, a lot of it funded by the Department of Energy, to solve the problem in one of two ways. We were either going to get better at finding these systems, so better exploration methods and data types, or we were going to avoid the exploration problem altogether by just engineering in place the things that we needed to make that system work. And so you see the beginnings of both the unconventional enhanced geothermal industry starting at that time, as well as the beginnings of some of the modern exploration methods. Before we talk about the process of exploration and development and so on, from a technical standpoint, what is happening there? What is going on when you have steam at the surface, what looks like it should be a perfect resource, and then you drill down, and it's a dry hole? What's actually going on under in the subsurface? Yeah, so at the geology or geothermal one-on-one level, everywhere on the planet is you go deeper. It gets hotter, usually, or at least in general. And in most places, that's, let's say, 25 Celsius per kilometer. So you'd have to go four or five kilometers or so to get to where you'd have steam temperatures. But in certain locations, that temperature is actually elevated, either because of magnetic or volcanic processes that may have brought heat closer to the surface, or in many places in the western United States, even in the absence of volcanism or magnetism, you can have fractures or permeable zones within the earth that will allow it to start convecting hot water from greater depth to closer to the surface. And hot springs are usually that kind of manifestation where there's hot water circulating, often in a convective nature, to bring that water to where you see it. What we've since learned in the decades since is that where you see hot springs at the surface, those are kind of the outliers. That's the tip of the iceberg. Most of these convective cells of hot water underground are not coming to the surface. And we now know that the majority of them are actually what we call blind. There's no hot spring, no volcano, and you wouldn't have even known they existed had you not, in most cases, drilled into them accidentally. And so with the Geyser's projects, for example, which by the way, is still producing power, some of them, right? It's amazing. It's a great resource. We just kind of got lucky in that case, was that just such a good resource? I guess what you're saying is that most of the good resources do not show at the surface, and many of the things that show at the surface are not actually good resources. Is it just that that first time around in the Geysers, it just happened to be the overlap? I think that's exactly right. And so the first pass, and this is true for almost all natural resource industries, the first pass is the low-hanging fruit. The really obvious stuff at the surface. There's copper, there's gold, there's steam, there's oil seeping out, let's drill there. And the Geysers was just one of those world-class resources. And there may be more of those around the globe yet to be developed, but at least here in the United States, it's unlikely that there's another gigawatt scale conventional geothermal resource to be discovered of that type. But there, you're right, there were Geysers at the surface fumaroles. In fact, the early explorers, a lot of them came from oil and gas. You had chevron, unical, phillips, hunt, and others that entered into the space in the late 70s and early 80s, and they actually spent hundreds of millions of dollars going out and drilling test holes, looking for more Geysers-like fields. And the Geysers was such a unique field in terms of its size and scale. They thought, oh, we just have to drill every few miles, and we'll see something like that if it's out there. And it turns out they didn't find anything like that in all of their searching. But in the process, they did find some of these other geothermal systems, some of which are now being turned into EGS fields, and some of which are being developed for conventional. I think they just underappreciated how narrow and small they could look at the surface and yet still have meaningful power potential of depth. Can you just give a little bit more detail on the difference between a conventional or hydrothermal field and an EGS field? What are you looking for in each? Yeah, in a conventional geothermal field, you need to find the temperature. So it needs to be hot enough to boil water or working fluid. You need to have porosity or permeability in the rock so that that fluid can circulate through, extract heat, you'll bring it out at the surface, then you'll re-inject it so it can circulate again. And you need water. So that working fluid that's going to sweep that heat through the system. And in the conventional field, all of those exist naturally. That's what we call a hydrothermal system. EGS was based on that early recognition that we drilled a lot of holes or wells that were hot but didn't necessarily have the water or the porosity and permeability to be able to circulate the water. And EGS was this hope that we could stimulate or engineer the rocks to have that permeability and maybe even add the water in some cases. And so this in many ways, I think, is analogous to what you see in oil and gas. The division between conventional oil and gas and unconventional is the ability to just drill a well and have what you need versus needing to modify the subsurface in some way. Okay. So the failing, the reason that the market stalled out, was we weren't great at exploration at the time. It turns out we sort of lucked into some great resources and in geysers and then couldn't replicate that success. And in the process of failing over and over again to replicate that success, it became harder and harder to finance new exploration. And then everybody kind of just fell out of love with geothermal. Now, obviously, we have these resurgence and as you said, it's coming in sort of two different categories. One is the, can we do better at finding the existing hydrothermal resources? And then the other is, can we engineer them via EGS? Let's talk about conventional hydrothermal development. Can you kind of walk me through what that like the actual steps in the exploration and then development process? And when you said they drilled a bunch of test wells, what is a test well? What does it cost? Like what is, you know, right? Yeah. So the first thing you're usually looking to confirm is temperature. You want to see that there's a resource here with enough heat in place to make a meaningful resource. And the standard tool of the industry is what's called the temperature gradient hole. And so you're literally going out and drilling a hole into the ground. Sometimes it's 100 feet, might be hundreds of feet or a thousand feet. And you're going to come back and measure the temperature gradient in there. And based on those gradients, estimate how much heat is in place and what might be at greater depths. One question I've always had about this, um, you ultimately, if you're finding a resource, you're going to be drilling deeper than 100 feet or 1000 feet. So it must be true that the temperature gradient that you find even pretty near the surface is highly correlated. It's like the temperature gradient is a spectrum that is consistent. And so you can infer from 100 foot depth well, what the temperature gradient, what the temperature expected would be at a kilometer or something like that. Is that right? I think directionally, it's right in that heat has a harder time hiding than other types of resources. Say like oil that might be underground. And so it is diffusing through the rock. But there are geologic processes that can obscure that or make it difficult to see. You might have a lot of cold water sweeping through from the climate or rainfall in an area that obscures the surface of it. And so there's large parts of Idaho, for example, where there are deep geothermal resources that you don't see at all in the first few 100 or even 1000s of feet because of that obscuring. But in drier areas, yes, you're right. You'll often see pretty distinct anomalous caps above these systems. Okay, so you drill this temperature gradient whole, and that's presumably pretty cheap to do. You're not drilling that deep and depth is the main cost of drilling. And you're not drilling, you're not putting casing or anything like that, you're basically just drilling a hole with a sensor measuring temperature gradient. So I assume that that is a lowest cost part of exploration, at least a physical lowest cost. In terms of the drilling to really confirm a resource, before that you will have deployed even lower costs, shallow and geophysical methods, to help you identify the areas that are worth drilling. But at this point, if you're drilling temperature gradient holes, you're deploying tens of thousands, maybe hundreds of thousands of dollars to test a certain target area. Catalyst is brought to you by ANSA. ANSA offers a one-of-a-kind data and analytics platform and advisory services to support better project development and procurement decisions. For energy storage developers, ANSA's platform provides crucial information that you never had easy access to before. Now at your fingertips is real-time pricing for a long list of system configurations to suit any project. ANSA provides a 360 degree view of the market with a life cycle cost analytics in commercial, technical and risk data. With ANSA, developers can easily determine which products to use in their designs, finance models and RFPs. Learn more about how ANSA helps save time and maximize profit at go.ansarunuables.com slash latitude. Catalyst is brought to you by Energy Hub. Energy Hub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. The Energy Hub platform takes the guesswork at a balancing energy supply and demand. It uses machine learning to control customer-owned distributed energy resources like EVs, home batteries and smart thermostats to precisely shape load profiles for grid flexibility and reliability. As the industry leader, Energy Hub helps more than 80 utilities manage 1.6 million devices. That's more than any other edge derms on the market. Click the link in the show notes to learn more or go to energyhub.com. You drill your first well, which is your temperature gradient hole. How easy is it? Is it binary? I assume it's not binary. What is the... So how much art versus science is there in the interpretation of that data? Is it easy to determine go no-go or do you have to do something sophisticated? In the early days, there was a lot of uncertainty. There really just weren't enough success cases or even failure cases to help them understand what some of these data types meant. They often use very high thresholds. If it's not boiling, I'm not interested. Increasingly over time, our experience has taught us, like you said before, that even a semi-anomalous or readings at a shallow level might indicate that it's worth drilling deeper. It's often an estimation of, given what I know now, is it worth investing additional capital to drill into that resource at greater depth, to gain greater confirmation. You can start with some probability distribution of possible outcomes. In the deeper you go, and the more capital you invest in the project, the tighter that distribution of outcomes becomes, and the higher your confidence is and what kind of resource you're working with. Okay. So let's say you drill your temperature gradient hole, you confirm you see what you're looking to see, and at least your interpretation is positive there. What's the next step? At that point, you're going to need to put together, if you haven't already, a pretty detailed conceptual model or understanding of what might be driving the system. Is it a volcanic system? Is it a sedimentary system? Is it a fault-hosted system? And that's going to give you a better predictive ability to go deeper into the resource, at least with classical methods here. And you're ultimately going to then want to say, okay, if I've proven temperature, now I need to prove permeability, or the ability to flow water through the wells that I would drill here. And so you're going to step up in size and complexity of your drilling program and drill slim wells, or small, you think of those mini production wells, that are going to be able to allow you to pull water out of the system. At which point you're flowing, like you leave that well open for a while and flow it, presumably. Is this when you're also able to start to determine what a decline curve would look like, or is this too early for that? It depends on how well you engineer, how large that well is, but the initial step is just showing that you can flow it at commercial scale. And then what you really want to do is be able to flow it long enough to run a flow test and indicate that over time it's not declining too fast, and that you'll be able to manage this resource sustainably. And like rough order of magnitude, what is the cost of one of these wells? And depth. Yeah, in this case, you're going to be going to a few thousand feet, maybe as much as five or six thousand feet. And your cost is going to be in the million plus range. So call it one to two million, maybe three or four, depending on the more complex wells to prove that out. So this is where you, I presume, like historically, when it became more difficult to finance, the cost of capital got higher and higher. This is the step where like real money starts to show up. I would assume. That's right. At this point though, you also have a little more confidence because of your earlier drilling and exploration. So your conversion rate is also a little bit higher. And so yes, you're putting more capital to work, but you're a little more confident it's going to be worth it. Those earlier stages, it is less capital, but you have to pursue more projects in parallel, which all sum up to also meaningful amounts of capital. But when we talk about dry hole risk and what happened historically and so on, is this the stage where the dry hole shows up? Basically, I mean, you might have gotten your temperature gradient, but then you drill down and you can't flow anything. Yeah, you would start seeing it here. And actually, in the early days, they would often skip that intermediate and what I was calling a slim well or more miniature well, and they would go straight to production well. We've got great temperatures, let's drill into this and they might drill the five, 10, 15 million dollar well, only to realize that there was no permeability or porosity in the rock. And we call that a dry well. So hot, but dry, no water coming through it. Okay, so next step. So you drill this well, you're able to flow, you confirm permeability and porosity, you've confirmed temperature. Are you de-risked at this point? Do you know what you've got? You're much further along the route of de-risking, but until you can also drill the injection well, which is going to be the way that you reinsert that water back into the system and let it circulate through the rock or through the ground network, you're not actually going to know that full decline rate to be able to build a robust reservoir model or estimate of the long-term potential of that resource. Can you describe what I know I brought up decline rate, but I realized we didn't describe what causes it. What causes the decline? Like, you could imagine a scenario where, look, it's hot underground, you just keep recirculating water and it should work infinitely. Why doesn't it? Yeah, so you are pulling heat out of the system, right? You're taking that to the surface, you're extracting it either through your turbines or through heat exchangers. And when you re-inject it, the water is going to be a little bit colder or quite a bit colder. And because of that, it needs to extract more heat from the rock before it returns to the production well. And you can think of these two wells. If your injection well is too far away, it actually might not ever return and you can start to draw down the pressure in the reservoir. If it's too close where it maintains good pressure in that reservoir, it might return too quickly. And you could think of that as then not having enough time to recharge in temperature. And part of the challenge was finding that optimal distance where it has enough time to fully recharge while also maintaining pressure in your system. Right. And then kind of moving ahead in the development process, I imagine that the other challenge related to that is, okay, so let's say you're successful, you drill your production well and your injection well, and it's working. Actually, give me context here. How much power might you generate out of a single pair? Let's see. So we recently actually drilled a new production well operating power plant in New Mexico. And that single well, the larger diameter well, going to about 8,000 feet depth, and it can produce about 15 megawatts net. So enough to power about 15,000 homes day and night. That's sizable. 50 megawatts is sizable. But ideally, probably you want projects that are multiples of that size or in order of magnitude bigger. That's right. In an ideal world. So in order to do that, now you're drilling another pair. And I presume if you're drilling another pair into that same reservoir, you're obviously extracting even more of the heat. And so I assume there is a fair amount of magic in the question of how close together can you put well pairs, first of all, and second of all, basically, how much can you extract from a given resource without accelerating the decline? Yeah. And this is an area of research and really just resource understanding that matured a lot over the past few decades as the industry was dealing with their existing resources and looking to expand or preserve them. And this is really where reservoir modeling becomes key. So there's certain data types like your flow and pressure information, but also we can put trace chemical tracers into the wells that will help identify how long it takes for them the water to return from injection to production. And based on these, we can build pretty robust models that are bankable in terms of the feasibility that they provide. And this is where you can start to estimate if I had two or three or four wells here, how much will that impact my decline versus just doing one or two in the same location? Okay. So this is the end. I mean, you drill the well pair. It works. You drill your however many additional well pairs you're going to drill. Now you've got a resource. What are you putting topside? We haven't talked about that yet. You get the heat out, but obviously heat is not the end of the story. Could be the end of the story, I suppose. Is anybody done just geothermal, like ground geothermal for, I guess, ground source heat pumps are this, but... And the most shallow ground source heat pumps. But in terms of direct use geothermal, there are a number of locations around the world that do use it in a direct way. In Europe, they're looking to repower many district heating systems by just bringing in hot water from underground. And even in the United States, the city of Boise, the city of Klamath Falls, they've been running district heating systems with geothermal where they're just directly taking that heat. At Zanskar, at our company, we're actually working with large mining companies now to also provide heat for industrial applications. And so I think there's a lot of exciting applications there, even before you convert to electricity. Okay. But let's assume you do want to produce power, which is what most of the projects end up doing. What is the topside infrastructure that you require? The topside, in many ways, looks like many other thermal plants. You're taking heat, you're generating steam, and that steam is going to drive a turbine, which then drives a generator and puts electricity onto the grid. In geothermal, especially in the western United States, oftentimes we're working with such a low temperature starting fluid that it's more efficient to put that heat into a working fluid, something that boils at a lower temperature. So think of isobutane or isopentane. And for that, we actually use heat exchangers. So most modern systems are going through heat exchanger. We call this binary. And then that working fluid on the other side goes through the turbine system. And you re-inject your fluid back into the ground. And that working fluid just cycles through the system. Okay. So I think we've reached the end of the development process. I'm curious about the timeline, both historically and maybe today. We're in an interesting moment now where there's plenty of demand for new power period, new sources of generation period. And then in some circles, particular demand for clean firm, which is what geothermal is. But everything is slow right now. It's hard to get anything fast. The fastest thing you can get maybe is renewables. But even that is gummed up by supply chain challenges and all sorts of tax credit issues and so on. But gas turbines are backordered for five years. Nuclear takes nuclear timeframes. What is the time frame of exploration and development for geothermal historically and how much opportunity is there to compress it? Historically, it was also a fairly long lead time type development. Historical projects took usually over five years and oftentimes as much as 10 years from start to COD. And major part of that is the slow decision making. As I mentioned, the incremental de-risking of a resource. We collect data, go back to the drawing board, decide if we're going to move forward. But another part of it was the permitting timelines is that a geothermal development project would have to go through five NEPA reviews if on federal lands. And the ability to accelerate a lot of that permitting is another area where we're seeing a lot of progress in the industry. Geothermal was recently given a categorical exclusion for the exploration activities of confirming and verifying a resource. And there's potentially still permitting reform ahead for the construction stage of the project. If you just take it down to the bare bones of you need about one to two years to explore and confirm the resource, and about one and a half to two years to construct that power facility and tie it into the grid. So the ideal scenario would be three to four years is realistic. And we're now seeing that as a possibility in certain locations in certain states where the regulatory frameworks are clear enough. And an example, not necessarily the Greenfield build, but of at least being able to come in and do meaningful work in a short period of time is work that we did recently in New Mexico. So we acquired in May of last year the Lightning Doc geothermal field, which is a field that had in many ways I think been seen to have underperformed and was no longer believed that it had much upside left in it. We based on data sets that we had and the models that we had really came to a conviction that there was a lot more there to give. And so shortly after acquisition, we permitted engineered, designed, and constructed a new production well to a zone that was four times deeper than the prior production zone. We built new pipelines, the electrical, installed the new line shaft pumps, and we were able to tie that into the grid in less than 12 months from acquisition. So in certain locations, we can actually move pretty quickly. And in our Greenfield projects, we have several that are in areas where we believe four years is a realistic timeline to bring those projects online. So you mentioned locations. I mean, that's the last thing that I want to talk about, I guess, with you, which is, talk to me a little bit about the history. I mean, we talked about geysers and geysers in California, but actually most of the geothermal that has been developed historically is not in California so much as Nevada and places like that. What's your view on how much geographic expansion should we be expecting for this next wave of geothermal development? How wide is the geographic aperture that people are looking at? Yeah, I think in terms of right now, the technologies that work today and that are on the precipice of commercial scale up in just the next few years, which is really conventional hydrothermal and EGS, we really think you're still going to be limited to tectonically active areas or areas with higher heat flow. And that's about a third of most continental land masses. So think the Western Third of the United States and many other tectonically active areas around the globe. And the main reason for that is because even with EGS or with conventional, you're still drilling as a primary cost driver. And if you can find that heat closer to the surface, it's going to have meaningful impact on economics. As drilling costs come down or as demand for clean firm power continues to increase, we see the economic shifting to where you could start to justify new build geothermal using some of these new methods and even more unconventional locations. We think that timeline could be on the order of decades though. Can you give me an order of magnitude of how much power we might... Well, let's say we stay in the Western Third of the United States. What's the total resource size that we expect? When you look at the full stack of kind of near-term EGS and conventional, we really are talking about hundreds of gigawatts to terawatts of resource potential. That to me is super exciting in terms of the United States' unique resource potential because you can think of this as a resource that has as much potential to give as say the entire Gulf of Mexico from an oil point of view. This is a real national treasure. And even just focusing on the conventional geothermal resources that I mentioned before, which is where a lot of our near-term work has gone, there are tens of gigawatts. And by some estimates, 100 gigawatts or more of that, which can have a meaningful dent right away without any first-of-a-kind technology risk. And so in terms of adding low-cost firm renewable energy in the next five to 10 years, we really think there's a chance to add more with geothermal than any other competitive form. All right, Carl, always appreciate you schooling me on geothermal. Thank you so much for joining. Thank you, Shale. Great to be here. Carl Hoyleand is the co-founder and CEO of Zanzcar. This shows a production of latitude media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. Prelude Bex Visionaries accelerating climate innovation that will reshape the global economy for the betterment of people and planet. Learn more at PreludeVentures.com. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Markwand. Stephen Lacey is our executive editor. I'm Shale Khan, and this is Catalyst.",
    "release_date": "2025-05-29",
    "duration_ms": 1981000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI1880502245.mp3?updated=1748466184",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-06-22T02:07:08.468747"
  }
]