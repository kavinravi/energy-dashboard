[
  {
    "title": "Will inference move to the edge?",
    "description": "Today virtually all AI compute takes place in centralized data centers, driving the demand for massive power infrastructure.\n\nBut as workloads shift from training to inference, and AI applications become more latency-sensitive (autonomous vehicles, anyone?), there\u2018s another pathway: migrating a portion of inference from centralized computing to the edge. Instead of a gigawatt-scale data center in a remote location, we might see a fleet of smaller data centers clustered around an urban core. Some inference might even shift to our devices.\u00a0\n\nSo how likely is a shift like this, and what would need to happen for it to substantially reshape AI power?\n\nIn this episode, Shayle talks to Dr. Ben Lee, a professor of electrical engineering and computer science at the University of Pennsylvania, as well as a visiting researcher at Google. Shayle and Ben cover topics like:\n\n\n  \nThe three main categories of compute: hyperscale, edge, and on-device\n\n\n\n  \nWhy training is unlikely to move from hyperscale\n\n\n\n  \nThe low latency demands of new applications like autonomous vehicles\n\n\n\n  \nHow generative AI is training us to tolerate longer latencies\u00a0\n\n\n\n  \nWhy distributed inference doesn\u2018t face the same technical challenges as distributed training\n\n\n\n  \nWhy consumer devices may limit model capability\u00a0\n\n\n\n\nResources:\n\n\n  \nACM SIGMETRICS Performance Evaluation Review: A Case Study of Environmental Footprints for Generative AI Inference: Cloud versus Edge\n\n\n\n  \nInternet of Things and Cyber-Physical Systems: Edge AI: A survey\n\n\n\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.\n\n Catalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting\u2060 \u2060\u2060BloomEnergy.com\u2060.\n\nCatalyst is supported by Third Way. Third Way\u2019s new PACE study surveyed over 200 clean energy professionals to pinpoint the non-cost barriers delaying clean energy deployment today and offers practical solutions to help get projects over the finish line. Read Third Way's full report, and learn more about their PACE initiative, at www.thirdway.org/pace.",
    "summary": "The podcast discusses the potential shift of AI inference compute from centralized data centers to the edge, exploring the implications for energy consumption and grid infrastructure. Dr. Ben Lee explains the technical reasons behind centralized data centers for model training and discusses the trade-offs in shifting inference compute to smaller, local data centers. The conversation touches on latency requirements for applications like autonomous vehicles and robotics, highlighting the need for edge computing. The episode also mentions the energy challenges posed by large spikes in power demand during model training and solutions like on-site power solutions provided by companies like Bloom Energy.",
    "transcript": " A very brief word before we start the show. We've got a survey for listeners of Catalyst and Open Circuit, and we would be so grateful if you could take a few moments to fill it out. As our audience continues to expand, it's an opportunity to understand how and why you listen to our shows, and it helps us continue bringing relevant content on the tech and markets you care about in clean energy. If you fill it out, you'll get a chance to win a $100 gift card from Amazon, and you can find it at latitudemedia.com. If you're not a survey, or just click the survey link in the show notes. Thank you so much. Latitude Media. Covering the new frontiers of the energy transition. I'm Shail Khan, and this is Catalyst. We could be getting 80% of our compute done locally, and leaving 20% of the heavy lifting for the data center cloud. Of the 80%, I would say most of that will be on the edge. I think maybe on the word of 1% ends up being put on your consumer electronics. Coming up, could the age of edge inference blunt the big data center boom? The AI boom is here, but the grid wasn't built for it. Bloom Energy is helping the AI industry take charge. Bloom Energy delivers affordable, always on ultra reliable on-site power. That's why chip makers, hyperscalers, and data center leaders are already using Bloom to power their operations with electricity that scales from megawatts to gigawatts. This isn't yesterday's fuel cell. This is on-site power built to deliver at AI speed. To learn more about how fuel cells are powering the AI era, visit bloomenergy.com or click the link in the show notes. Building electricity demand is testing the limits of the grid, but Energy Hub is helping utilities stay ahead. Energy Hub's platform transforms millions of connected devices, like thermostats, EVs, batteries, and more into flexible energy resources. That means more reliability, lower costs, and cleaner power without new infrastructure. Energy Hub partners with over 120 utilities nationwide to build virtual power plants that scale. And how the industry's leading flexibility provider is shaping the future of the grid, visit energyhub.com. Clean energy is under attack, and it's more important than ever to understand why projects fail and how to get them back on track. The center left think tank Third Way surveyed over 200 clean energy professionals to answer that exact question. Their newest study identifies the top non-cost barriers to getting projects over the finish line, from permitting challenges to nimbism, to read Third Way's full report, and to learn more about their PACE initiative, visit thirdway.org slash PACE, or click the link in the show notes. I'm Shail Khan. I lead the early stage venture strategy and energy impact partners. Welcome. Okay, so here's an energy question disguised as an AI infrastructure question. What proportion of the world's AI compute in 2035 will be cloud, i.e. in large centralized data centers, versus edge, versus edge edge, i.e. on device. It's an energy question because the answer today is effectively 100% in that first category, cloud. And that's why we have this crazy dynamic in the electricity sector, and actually in the natural gas sector too, where hyperscalers and neoclouds and developers and real estate speculators and crypto miners turned AI companies and more are hunting for sites that can accommodate hundreds of megawatts or gigawatts of power. And the whole thing, as we know, is crashing through the electricity sector, affecting generation and transmission distribution, prices, now politics, and so on. But there's a narrative that I've heard a number of times that if borne out would potentially present a very different future from the present. This is one where AI workloads, first of all, shift significantly from training to inference, and then where those inference workloads become highly latency sensitive and are also able to be executed in a more distributed fashion. And as a result, much of that compute and thus the power demand shifts from these big centralized data centers to the edge. That could mean it shifts to 10 megawatt data centers clustered around an urban core or an autonomous vehicle corridor, or at the limit, it could mean inference compute happens on device. And centralized data centers fall back into a pure training position. Any version of this that takes significant share of the market would have profound implications for the energy question and for the grid. So it's worth exploring, which is what I'm doing today with my guest, Dr. Ben Lee. Ben is a professor of electrical engineering and computer science at the University of Pennsylvania. He's also a visiting researcher at Google. By the way, this edge AI infrastructure world and the energy implications thereof is super interesting to me as you will hear. So if you are building something in the space, please come get in touch. In the meantime, here's Ben. Ben, welcome. Great to be here. Thanks so much. I'm very excited for this conversation because this is the topic that I, in my energy circles that I travel in, I've heard Scuttlebutt about a bunch of times, but I've never actually spent the time to really try to understand the topic basically being how much of inference compute might move from central cloud infrastructure to the edge and then how far to the edge, of course, being another question. I think we should start by actually defining those categories a little bit. How do you think about the categorization of like where compute can occur? Then we'll talk about each of those categories individually. Right. So even before we talk about generative AI, there, for classical compute, cloud computing in general, all of the services we love to him and changed the way we live and work today. There are three levels generally I think about for compute. The first is massive hyperscale data centers, the ones run by Microsoft and Google and Amazon, hundreds of thousands of machines, massive facilities. That's what most people think about when they think about cloud computing. The other extent of the extreme would be personal devices, consumer electronics. So you think about your phone, you think about your tablet, your laptop. Plenty of compute can happen there as well. There is a perhaps less understood middle layer or intermediate layer called edge computing. And edge computing really means that there are times where you don't want to go all the way to this remote massive facility and wait for the data to go out to that data center and then come back. You might want to access some compute that's a little bit closer to you, maybe in the same city, maybe in the same geographic region, that's edge computing. So they're still going to supply really capable of high performance machines, these servers. But you don't suffer those longer communication times or latencies that you might if you would go to that remote massive data center. My recollection is that there was, I think, okay, so the advent of cloud computing meant the build out of lots of big centralized data centers. There was a fair amount of conversation some number of years ago in the first wave of excitement around autonomous vehicles in particular that you might see a fair amount of edge infrastructure get built because of the latency tolerance requirement for AVs. I mean, I'm on the outside, so tell me if I've got the kind of narrative wrong here. Then it seems to me that because AVs were generally delayed or maybe the need wasn't as high like what we've got today, if you just look at the infrastructure today, it seems like the vast, vast majority of classical compute even except for stuff that's sitting in like mainframes that companies is in the cloud and the big centralized data centers. Do I have that right? That's right. This is a decades long trend. I mean, we've seen this progression, this adoption of cloud computing over the last 15 to 20 years and there are a couple of reasons we are seeing that shift or we have seen that shift. The first is that computing in a massive data center run by the hyperscaler companies, the big tech companies is much more energy efficient. They know how to deploy these facilities. They know how to cool them and build HVAC systems efficiently. They're in current very small overheads per watt of compute. There's this industry standard metric called power usage effectiveness or PUE and that's the ratio of the power you're using compared to the power that's going to compute. Google's PUE is close to 1.1, which is to say for every watt going to compute, there's an additional 0.1 watts going to the overheads of power delivery or cooling or whatever. That's really incredibly efficient. Most mom and pop data center operators, most enterprise data center operators don't get the scale and efficiency that these hyperscalers do. The scale also gives a second key advantage, which is the ability to share hardware. You buy the hardware once and you have lots of users sharing the same physical hardware. That allows us to drive the cost down, allows the hyperscaler operators to drive the costs down and that essentially gets a massive increase in efficiency. Most compute now is being done in these large data centers and in the cloud. Let's talk about the world of AI now, which is where all this growth in compute is happening. AI workloads, of course, divided into two major categories, one being training of models and the other being inference. I think we'll spend most of our time today talking about inference probably, but let's spend one minute on training. Is there any movement or argument that training should take place anywhere other than large centralized data centers? It seems very clear to me that the trend right now is just build the largest possible data center to train the largest possible model. Is there anyone who thinks that that might turn in the other direction? Some, but that really hasn't gone much traction. The reason why we see most training happening in massive data centers is because of the scale. You need large data sets, you need lots of GPUs all closely coordinated learning the model parameters. The only scenario that some people have explored for training away from the data center is if you've got private data and somehow you want to refine your model or somehow fine tune your model with that private data. You don't want to share it with the hyperscalers. That has been primarily a research question rather than a production system that people have deployed. Let's assume then that the vast majority of training compute is still going to happen in centralized data centers. As it stands today, I don't know if you know the numbers, but just high level of all AI workloads, how much is training versus inference? I think the other big point people have made is over time, the proportion of workloads going toward inference is going to increase. The proportion of workloads going toward training may decrease as we sort of asymptote the next model or something like that. Today, it's mostly training still. I would agree with that. I think to first order, the training costs are historically what people have cared about the most because the data sets are massive and they're talking about these massive 1,000 megawatt data centers for the training workloads. There was a study we did when I was a visiting research scientist at Meta where we found that energy costs for AI were roughly broken into three categories. There's a data pre-processing aspect as well. That's about a third. The training is another third and then the inference or the use of the model is the last third. Clearly, those fractions are evolving rapidly. I would agree with you when you're saying that the training costs are probably flat lining. They were reading a plateau and how quickly they are growing, perhaps. If the optimism about our AI is to be justified, you're going to have to see inference costs go way up because that will be an indicator that adoption has gone up in a fairly significant way both among individual users but also among companies and enterprise users. I think it's true to say that inference costs are large and potentially will grow very rapidly. Then we're getting to the crux of our question today, which is inference workloads, inference costs increase over time. Usage of the models increases over time. That's the presumption of everything going on in AI world. The question is, will that inference compute predominantly still take place in these big centralized cloud data centers or will some or much of it potentially shift either to one of the other two categories you described, sort of edge localized or fully localized on device? Let's talk about the edge version first, which is essentially smaller data centers, still data centers, but smaller and more local. What's the argument for why that might happen and what are the limitations? The argument in favor of edge computing is mainly the proximity to the end user. We have been conditioned in an era before generative AI that when we access internet-based services like a search engine, we expect the answer to come back in the order of 100 milliseconds. That is the order of magnitude that we're talking about. As a result, to get those 100 millisecond latencies, oftentimes you require computation closer to the user. You don't have to travel across internet. You don't have to travel from the west coast out to the east coast and back again, the data. And get that answer back in a timely way. What is interesting with generative AI is that we are being reconditioned to tolerate much longer delays. If you use something like GPT or you use something like Claude or your favorite chat bot, oftentimes it's just thinking for seconds and seconds, maybe tens of seconds before it gets you the first token. The question there is to what extent we care about that latency and need that really fast responsive access to the answer? Yeah. I think we've been especially trained even further in that direction with the introduction of things like deep research, where even in the name, you think, well, of course that has to take time. It is deep research that they are doing. So it's an interesting point that maybe we are becoming reconditioned to allowing more latency. The argument that I've heard for why latency is really in a matter apart from just wanting search queries or chat queries to come back quicker is the next wave of applications for AI. And so maybe we go back to autonomous vehicle world and things like that, where like latency, making decisions in near real time does become really important. Robotics being another category that could be a major user of AI compute, but needs really, really low latency. Is that part of the argument for shifting some compute to the edge? Yes, absolutely. So the classic computer you mentioned, not honest vehicles, robotics, fit into what we call a cyber physical AI. So cyber physical systems are those that have a cyber component, a computational component, but also interact with the physical world. And once those interactions with the physical world arise, then we care about responsiveness because with that, it underpins safety guarantees and the ability to make sure that your robotic arm is able to respond quickly enough to hazards, your autonomous vehicles are able to do so. So I agree that there will be cases where we will need those really low latencies and that is going to require edge computing much closer to the user. So we have much shorter internet delays, network delays. I'm curious to understand the trade-offs here, right? Like I know with model training, there are technical reasons why you want all your compute as clustered together as closely as possible. You want every GPU as close to every other GPU as you can make them minimizing the copper between them or the optics or whatever it is that's communicating between them. And that for some reason that you can explain to me makes model training more effective. Is there a similar dynamic in inference? Is there a technical reason why that you're paying a penalty if you shift to smaller data centers at the edge? Or is there no technical reason why it's suboptimal? Right. Yeah. Let's talk about the training piece first. The reason why we need 1000 megawatt data centers where we have hundreds of thousands of GPUs connected so closely together is because the data sets are massive and the models are massive. We're trying to learn on the order of a trillion parameters for these machine learning models, these AI models. And we're trying to do it on the wealth of data we find in the internet. There's no way that any single GPU can handle that much data. So what we end up doing is partitioning the data into smaller pieces and then handing each GPU a slice or a partition of this data. And each GPU will turn on its own share on its own partition of the data and learn the models that work best for its piece of the data. And all the other GPUs and the data centers are doing the same thing on their partitions of the data. Periodically, what they will do is they will compare notes. They will share the weights that they've learned. And this sharing is really, really expensive. And some of the people in the energy space may know that there are massive energy fluctuations or power fluctuations we will see in data center usage when the GPUs go from this computational intensive phase where you're learning the model weights to this communication intensive phase where they're comparing notes and sharing their intermediate results with each other. So as a result, that's why we're talking about these massive data centers for training. They all need to communicate frequently to share what they've learned from their own data sets. For inference, we don't see that effect. Just to add the craziest thing to me about how model training data centers operate right now, the absolute craziest thing is as you said, there are these, there are surprisingly large spikes in power demand as a result of how the models are trained. But they do in large part because those spikes are actually problematic, not just to the grid but to the equipment inside the data center as well. So what they do at least sometimes to manage that is they create dummy workloads. So they keep the power profile basically flat, but you are literally just wasting energy on absolutely nothing. Nothing is happening during those times. They're dummy workloads. At that scale, the fact that that is happening is wild to me. Absolutely. I think we've seen this in other contexts as well, but not perhaps at this scale, this notion of an electrical engineering, we'll call it the DIDT problem, the change in current divided by a change in time. If large current swings over very short periods of time, you could imagine building batteries to sort of adapt things out or decouple. And certainly a lot of people are thinking about that. But the easiest thing to do might be to just modulate the software as you say, because we have very precise control over what the software does. So that is an active and ongoing area of research and then you say it further develop. You heard the phrase speed to power a lot lately, but here's what it really means. AI data centers are being told that it will take years to get grid power. They can't afford to wait, so they're turning to on-site power solutions like Bloom Energy. Bloom can deliver clean, ultra reliable, affordable power that's always on in as little as 90 days. Bloom's fuel cells offer data centers other important advantages. They adapt to volatile AI workloads, they have an ultra low emissions profile that usually allows for faster and simpler permitting, and they're cost effective too. That's why leaders from across the industry trust Bloom to power their data centers. Ready to power your AI future? Visit Bloom Energy dot com to learn more. The grid wasn't built for what's coming next. Electricity demand is surging from data centers to EVs and utilities they need reliable affordable solutions that don't require building expensive new infrastructure. Energy Hub helps by turning everyday devices like smart thermostats, EVs, home batteries, and more into virtual power plants. These flexible energy resources respond in near real time to grid needs, balancing supply and demand. Plus they can be deployed in under a year and at 40 to 60 percent lower cost than traditional infrastructure. That means more reliability, lower costs, cleaner energy, you can't get much better than that. And that is why over 120 utilities across North America trust Energy Hub to manage more than 1.8 million devices. Learn more at energyhub.com or click the link in the show notes. Do you ever wonder why it takes so long to get clean energy projects up and running? Do you have permitting reform on the brain? Are you NEPA reform curious? The new paste study from Third Way pinpoints the non-cost barriers that stand in the way of clean energy deployment and keep new solar and transmission projects in limbo. They surveyed more than 200 industry professionals to understand what's slowing down deployment and offer practical solutions on how to fix it. To read Third Way's full report and to learn more about the PACE initiative visit thirdway.org slash PACE. Okay, so then on to inference. So you're saying inference does not contain that same challenge. So is there any, what is the downside to shifting inference workloads to the edge? To my knowledge, there isn't much of a downside because the reason why inference is amenable to edge computing is because when you send a prompt to, for processing by a large language model, that prompt is probably handled by one GPU or maybe eight GPUs inside a single machine. So and the reason that it is is because the model sits in that machine, the data sits in that machine and all of your prior conversations with that bot are sitting in that machine. And it's a very localized piece of compute that needs to be done. And you don't need tens or hundreds of GPUs to be coordinating to give you an answer back. You've got that one GPU or a tightly coupled GPU is giving you that answer back. And that is amenable. That is great for edge computing and we can certainly supply that. So a thought experiment that I've given people recently in thinking about this is let's just say that you need a gigawatt of inference compute in five years from now or seven years from now, something like that. You think you need a gigawatt? Wherein the demand for that gigawatt is geographically centralized somewhere. Let's just say you need a gigawatt of inference compute to serve the Dallas metropolitan area, whatever it might be. At that point, a few years from now, this is back to the power perspective. Is it going to be easier for you to find and cite a one gigawatt site or 110 megawatt sites within that geographic region? Today, I think it is still probably easier to find the gigawatt site or at least the past couple of years it has been. There are that many gigawatt sites out there from a power availability perspective. So at some point, is that going to flip? And is it going to be easier to build 110 megawatt sites, which sounds really hard to do and indeed is, but these are all hard problems. So if that happens, do you think that we are going to see a significant portion of that inference workload move to that type of scale? Is that the right scale? Like should we be looking at 10 megawatt sites, 100 megawatt sites, one megawatt sites? How far to the edge do we want to go? Yeah, absolutely. And I agree with the premise of that question 100%. I think that there are two reasons to go to smaller and many smaller data centers. The first is the one you mentioned, power, power provisioning and connection to the grid. The second is the fact that you don't need a massive GPU coordination for an inference workload. I guess the catch might be that if you are thinking about existing edge data centers, maybe you've got data centers in downtown Los Angeles or something like that already serving workloads, those workloads may not be configured to handle GPU and AI compute. They may have power delivery infrastructure that was optimized for CPUs. They might have HVAC systems optimized for the much lower power density of CPUs. So it's not simply a matter of pulling out your CPU's and replacing them with GPU's. You may have to retrofit the facility itself to support that. But I agree, I think finding capacity there may eventually become easier than finding the next thousand megawatts. Is there any limitation? I can imagine. I'm trying to think of why you wouldn't do that. You need to have a fair amount of memory and you need to house all the model weights and so on in every individual data center if you're going to do that at the edge. There's got to be some minimum viable scale, I assume. Maybe to give you a sense of the type of data centers we were talking about in the past. Again, in a study that we had done with Meta, we looked at 15 of their data centers before generative AI and the scale of those facilities were somewhere between 15 to 50 megawatts. So less than 100 megawatts. And certainly that was fairly conventional, uncontroversial to build those sites of data centers in the past. So that's the starting point, I think, in terms of the scale. Now as you scaled down towards, for example, one megawatt, not clear at what point things are making less sense. I guess the other point here, the way that the data center build out has gone historically, just like the cloud data center build out, it's been fairly clustered in these regions. And there's a reason why Northern Virginia is the data center hub of the world and there are others as well, Chicago, Dallas, et cetera. And that, as I understand it, is largely because the cloud providers needed to offer a certain level of reliability to their customers. And so they could have redundancy within a given region and that was helpful to them in terms of what they were offering. Do you think that this future world, wherein a bunch of inference compute moves to the edge, let's call it 15 to 50 megawatt data centers then instead of hundreds or thousands of megawatt data centers, does it look similar? Is that you have a bunch of a small number of regions that have a really high concentration of those 15 to 15 megawatt data centers? Or could it be much more dispersed because the whole point of this is really low latency and local and you don't need them to be as clustered? I think there are lots of different aspects at play in terms of data center siting. I think the redundancy is definitely one of them. And I have trouble disentangling the role that some of these other factors play as well. Some people talk about tax breaks and incentives from local companies and local states. Some people talk about proximity to internet exchange points. So not only are you talking about congestion-free power movement, but you also talk about congestion-free data movement into and out of the data centers, Northern Virginia has that. And then of course the availability of the power itself. I guess I would say that when you start talking about many of these smaller data centers, from a redundancy perspective, it might be okay that they're not all geographically clustered as long as you have a strategy for rolling over the compute or rolling over the workload to spare capacity somewhere within that region that has a similar performance profile or some sort of similar latency or delay characteristic. So that's really the concern whether you have robustness, geographical redundancy and resilience there. Is this happening? It's interesting. I was thinking, okay, so it sounds like you're saying there's not a big downside. We already have significant inference workloads. So it's not like we're waiting on workloads to show up that could accommodate this. And yet, if you look at most everybody, building data centers, certainly the hyperscalers and I think the colos and folks as well, the focus continues to be on we got to find big sites for big data centers. Why don't we see more development of this smaller scale edge AI inference world? I think it really depends on the workload and the application and we don't know. I view AI as a more fundamental basic technology and we don't necessarily know what application or capability will be layered on top of it. I'd say that we've been talking about edge data centers a lot. There are other words for this type of data center. Content distribution network is one of those examples of CDN or a point of presence, a BOP that the facilities are sometimes called and they exist in fairly significant numbers. Content distribution networks ensure that when you want to access, for example, newtimes.com or WSH.com, your web page is not being served from the other end of the country. Those web pages are saying close to you because the content distribution network took those updated web pages and moved them to facilities near you, data centers near you. Likewise, companies like Meta, when they have Instagram or when they have these social media applications, they also have these points of presence that supply data from local points of presence rather than retrieving content for your feed from across the country. We already see that but these are application level performance requirements whether they be for social media or for other news content. Once it becomes clear what applications of AI really drive further inference deployments, then we'll know what sort of performance requirements are needed, what we call caching techniques or strategies might be useful so that we can keep fresher data or more recent more frequently used models closer to these users and then serve them more quickly. I think we'll become clearer as we see which models really get traction, which applications really get traction. Right, so maybe the state of affairs today is, look, anybody who's developing data centers, we know we need the big centralized data centers because there is currently essentially endless demand to train models at least relative to the availability of compute today. We know we need to build the big centralized ones. We might as well use those big centralized ones that we know we need right now for inference workloads such as they are today but we don't have enough certainty yet about what the inference workloads are going to be long-term to invest that kind of capital and time expenditure that it would take to build out the network of 110 megawatt data centers in a particular geographic region, something like that. But that's right, and I would say maybe that my crystal ball is as clear as anyone else's crystal ball but I feel like there's a huge amount of GPU capacity being discussed in the pipeline in these large data centers and if it turns out that maybe there are diminishing returns from training larger and larger models or maybe we run out of data because we've exhausted all the data that's available on the internet. When those things happen it may be that demand for these GPUs in these larger data centers will flatten out and will get an aspect of capacity at which point as you say they will be used or repurposed to serve and inference and then it will be hard to make the case for building yet more data centers smaller ones with GPUs closer to the users. I think the catch there will be if one of these model providers or one of these application developers makes performance a distinguishing feature of their offering. If they start competing on performance rather than on capability then we're going to see well I may have a thousand GPUs in the middle of Nebraska that are already deployed but I really want to break into the San Francisco market. I've got to build my GPUs right there and have them available. All right, so speaking of performance let's transition to the full extreme version of this which is also I think theoretically the most disruptive from an energy perspective which is shifting any significant portion of these inference workloads all the way onto the device. Skip the either skip the middle ground of edge five megawatt data centers or 15 or 50 or include them but but you know shift workloads that would have gone to a big data center that requires a lot of power straight onto your iPhone or your iPad or whatever it is. We've heard some climmers of this as well. Give me this similar sort of like pros and cons of shifting that workloads straight onto the device. Right pros primarily two things. One is performance right you don't have to go across internet the models right there and the compute is right there assuming that you get really capable of hardware and your device as well you get really quickly responsive answers from your AI. The second is also something we've mentioned earlier which is the notion of privacy that you don't necessarily need to send your data out into this hyperscale data center where it gets blended with lots of other users data and you want you have made fewer guarantees about what happens to it. Localized compute is certainly more private than compute on shared systems. So those are the two key advantages and then I guess third would be that it gets more entirely integrated with the capabilities on a particular platform. So for example Apple's ecosystem. Right and Apple seems like the obvious candidate to do this clearly. You mentioned privacy. Apple is particularly focused on privacy. They have the hardware the device right like Apple is notoriously or at least reputationally behind in the AI race and so like this it's not hard to picture that like if somebody's going to move a lot of this inference on device it's going to be Apple. Okay but there is a real trade-off here I assume. Yes and the trade-off is primarily with respect to the capabilities of the device. So if we have a very large model we're going to have to deploy that model on a much more capable hardware platform than we've got today. This means having some number of gigabytes of memory to hold the model weights and then also some additional gigabytes of memory to hold the context as you develop this conversation with the model. In addition to the memory you're also going to need the compute. You're not going to have this high performance GPU sitting inside your phone. So you're going to have to have specialized chips. Those specialized chips on your hand are going to be less powerful, less capable than the ones in the data center. So all of this speaks to not getting exactly the same model that you would get into the data center. You would get a shrunk down model. Maybe in the data center you would have a trillion parameters, this massive GPT-5 model for example but on a personal consumer electronics device you might only have 7 billion parameters so orders of magnitude smaller and that smaller model will be less capable. It will give you less capable answers. It will be capable of doing fewer tasks but maybe that's okay because you've identified only a handful of tasks that you really care about on your personal device. So that is really the trade off. As you go towards the device you're going to have to shrink the size of the model down. You're also going to get less and less capability out of your AI. The final thing of course is the power and energy profile. At the data center scale we care primarily about power because power influences infrastructure and power delivery and influences thermal and so on. Thermal and management. For device level compute there are two considerations. We care about energy rather than power because that affects battery life. Even if you could deliver a really capable GPU chip onto your phone the question is how long would your phone last if you were using that chip on a fairly consistent basis. So the energy aspect will continue to be challenging and then the thermal aspect will also be challenging. If you have a really powerful device that's going to be a hot brick inside your pocket and that's going to be a deal breaker as well. So when you say deal breaker is there progress toward on device? I mean to your point on performance that strikes me as like okay this is now we're now again in the context of like specific workloads. Certain types of workloads like a 7 billion parameter model might be fine and others it wouldn't be and so maybe there will be some on device, an on device chip and some inference that you could do on device but you pull up your chat GPT app or whatever and of course it's going to send you back out to the cloud or maybe to the edge. These other challenges of thermal management things like that are our hardware challenges. Where are we in the progression of on device inference? Is it coming? Is it not coming? Do we not know? I think the assumption with on device inference is that you'll be able to shrink the model without loss in performance for the tasks you care about. That is the primary strategy the computer scientists have been taking. On the hardware side we have made strides in developing custom chips, custom silicon for specific types of tensor algebra that are required for machine learning models. So we know how to build those chips and that gives us energy efficient computer higher performance. We know how to build really capable memory systems or solid state disks. When your phone now has hundreds of gigabytes of memory on it or hundreds of gigabytes of storage on it, so there's a question of well maybe you'll end up using less of it for your photos and more of it for your AI model or something like that. So I think there are fairly significant resource constraints but I don't think that they are insurmountable in the sense that more intelligent hardware design and more intelligent hardware management could go some ways in terms of making these AI models feasible on the device. Okay, so I'm going to put you on the spot and we promise not to hold you to these numbers but just to give a sense of where we think things are heading. If we're fast forwarding 10 years, let's just say we're in 2035 and imagine there's a total volume of inference compute in the world or whatever that's let's just say it's 100 megawatts total. What would be your guess of the ranges of how much of that compute is going to take place in large centralized data centers or versus at the edge? We'll draw a line, let's say 100 megawatts and above is large centralized, some 100 megawatts but not on device is edge and then the third category of course being on device. How much of it can go anywhere but the centralized data centers? So I would go straight to this idea of having a 2080 rule because we see this all the time in computer systems where you have 20% of your tasks being extremely popular. Maybe there are 20 things that you always want to do and that you spend 80% of your AI compute doing those things. That could be email processing, that could be photo analysis, that could be, so we can identify what those really compelling applications and tasks are and we're going to be spending most of our time doing that. And then for the remainder of the long tail long heavy tail of other tasks that people might want to do that will always be backup capabilities residing in the cloud data center. So I would say that we could be getting 80% of our compute done locally and leaving 20% of the heavy lifting or the more esoteric, the more corner case compute for the data center cloud. Of course excluding the training and the training will continue to all reside in the massive facilities. But in terms of the inference, I think there's huge potential. Right. But yeah, that's actually a very significant shift of 80% of the inference workload appreciate that that does include training. But still, 80% of the inference workload could end up local. That's a significant shift and has pretty profound implications for the energy picture as well. Are you saying that 80% just to pin you down even a little bit more, is that local in the sense of being at the edge or is that local in the sense of being on device? Or what do you think the split ends up being there? Yeah. So I think of the 80%, I would say most of that will be on the edge. Like I suspected is today, I think that if you look at what we talked about earlier, content delivery networks, points of presence, they've probably identified 20% of the content that 80% of the people will be looking at most of the time and they're putting it at the edge. I think maybe on the word of 1% ends up being put on your consumer electronics. Actually, even for today's compute, when we set aside AI, there is a trend towards consumer electronics hiding that flow of data back and forth between the device and the edge for you. So sometimes they'll like if you use a cloud storage service like Dropbox or if you're using a photo storage service, they will let you pretend that you have access to all of your videos or all of your photos and all of your documents and they will transparently behind the scenes, move things back and forth between the data center and your local device. So you may think you have all of it, but maybe you've only got a tiny sliver, less than 1% on your local device. Certain things open up in my box instance, certain things open up much faster than others when I try to open them and it's occurred to me that that is why. If I step back then, okay, so it sounds like what you're saying in this scenario, you're painting of the future, roughly 80% of the inference workloads are edge, very little of it actually on device and then the other 20% or so sitting in cloud, big cloud data centers. So when I think about the energy implications of that, there's I think a couple of ways to think about it that are pretty interesting. One is this, okay, so maybe a fair amount of the energy consumption of at least inference compute is going to shift to these 5 megawatt, 15 megawatt, 50 megawatt type local sites. That has big implications for the grid in ways that are, I don't know, both good and bad, probably harder to manage than some ways easier to manage in other ways. But the overall energy consumption of inference compute, I would expect, and you can tell me if I'm wrong, would actually be higher in this scenario than it would be if it was all centralized, because I assume the PUE that you get for these edge data centers isn't quite as good as it is for the large centralized data center. So like on balance, this probably means more overall AI energy consumption. Do you think that's right? Yes, yes, I think you get economies of scale. When you go to a gigawatt or two gigawatts, you have a single facility, you're managing it in a highly optimized coordinated way, and you've got hundreds of thousands of these machines all managed very precisely. I think as you shrink the system down, you will lose an efficiency. You will be trying to build these 20 megawatt data centers and maybe footprints or facilities that weren't designed initially for those workloads. So yes, I think total energy costs may go up as a result. We're talking about inference workloads, to some extent, as a monolith. I'm sure they are not. So are there big distinctions in your mind in terms of the different types of inference workloads and how that influences where they should be housed? Right, yes. So that's a really great question, actually. I would say that there are fundamental limits that a number of inference queries a human user can actually produce because we're ultimately limited by the speed of our typing. The number of tokens we can actually produce to query the models. So there is some of that where humans will continue to send requests to agents. But I think increasingly, most of the inference workload will come from other software agents. This could be a search engine retrieving web pages and then asking the large language model to summarize it for individual coherence discussion for you. This could be your photo app learning something about your images or this could be your mail app doing something with the mails and helping you compose messages. So all of that is done behind the scenes. And those inference workloads are potentially much larger because of course, software can generate those requests at much, much higher rates. From the perspective of where that computation happens, to the extent that the data center already has servers running your mail workloads or to the extent that your search engines are already running in the same data center, the communications that the model will be a bottleneck. So if you have a data center in Nebraska running your search engine for you or doing some of these other big heavy software jobs, then potentially they could query and execute inference in these largest hyperscale data centers. All right, Ben, this was super interesting. Really appreciate your time. It was my pleasure. I really enjoyed the conversation. Thanks so much. Dr. Ben Lee is a professor of electrical engineering and computer science at the University of Pennsylvania. He's also a visiting researcher at Google. This shows production of latitude media can head over latitudemedia.com for links to today's topics. Latitude is supported by Prayly Adventures. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Markland. Stephen Lacey is our executive editor. I'm Shane O'Con and this is Callist.",
    "release_date": "2025-12-18",
    "duration_ms": 2867000,
    "url": "https://traffic.megaphone.fm/PSMI8071520231.mp3?updated=1766013978",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-12-24T01:50:19.645904"
  },
  {
    "title": "Can AI revolutionize EPC?",
    "description": "Big construction projects in the U.S. are notoriously unpredictable, often finishing over budget and behind schedule. Part of the problem is the inherent complexity of these kinds of projects, like data centers and first-of-a-kind plants. But there\u2019s another problem: the companies that actually build these projects \u2014 called EPC firms for engineering, procurement, and construction \u2014 often lack strong incentives to control costs or deliver on time.\u00a0\n\nThat\u2019s the thesis behind Unlimited Industries, a new startup focused on using AI to develop multiple project designs upfront and reduce project risks. So what would it take to actually cut costs and shorten construction timelines?\n\nIn this episode, Shayle talks to Alex Modon, co-founder and CEO of Unlimited Industries. The company recently announced a $12 million fundraise as it emerged from stealth. Shayle and Alex cover topics like:\n\nHow EPC incentives and contract structures drive cost overruns\n\nWhy bespoke projects prevent learning and standardization\n\nThe role software and AI can play in design and risk reduction EPC\n\nManaging risks \u2013 including geopolitics, contractor reliability, and supply chains\n\nResources:\n\nCatalyst: FOAK tales\n\nThe Green Blueprint: Shortening the nuclear development cycle from decades to years\u00a0\u00a0\n\nCatalyst: The cost of nuclear\u00a0\u00a0\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.\n\nCatalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting BloomEnergy.com.",
    "summary": "The podcast discusses the challenges and misaligned incentives in engineering, procurement, and construction (EPC) projects, particularly in the energy sector. It explores how the current EPC model leads to cost overruns, time delays, and inefficiencies due to the lack of alignment between developers and contractors. The conversation focuses on leveraging modern technology, like AI, to improve project delivery, reduce uncertainty, and optimize designs. The discussion also touches on the importance of continuous improvement in project management and the potential for cost savings by refining the engineering process to drive down overall project costs.",
    "transcript": " A very brief word before we start the show. We've got a survey for listeners of Catalyst and Open Circuit, and we would be so grateful if you could take a few moments to fill it out. As our audience continues to expand, it's an opportunity to understand how and why you listen to our shows, and it helps us continue bringing relevant content on the tech and markets you care about in clean energy. If you fill it out, you'll get a chance to win a $100 gift card from Amazon, and you can find it at latitudemedia.com slash survey, or just click the survey link in the show notes. Thank you so much. Latitude media covering the new frontier of the energy transition. I'm Shale Khan, and this is Catalyst. As a third-party contractor, your incentive is really, we only make money when the project costs more and takes longer. So, yeah, high level what's broken is the incentive structure. Coming up, is there a better way using modern technology to build big capital projects? The AI boom is here, but the grid wasn't built for it. Bloom energy is helping the AI industry take charge. Bloom energy delivers affordable, always on ultra reliable on-site power. That's why chip makers, hyperscalers, and data center leaders are already using Bloom to power their operations with electricity that scales from megawatts to gigawatts. This isn't yesterday's fuel cell. This is on-site power built to deliver at AI speed. To learn more about how fuel cells are powering the AI era, visit bloomenergy.com or click the link in the show notes. Surging electricity demand is testing the limits of the grid, but Energy Hub is helping utilities stay ahead. Energy Hub's platform transforms millions of connected devices, like thermostats, EVs, batteries, and more into flexible energy resources. That means more reliability, lower costs, and cleaner power without new infrastructure. Energy Hub partners with over 120 utilities nationwide to build virtual power plants that scale. Learn how the industry's leading flexibility provider is shaping the future of the grid. Visit energyhub.com. Learning energy is under attack, and it's more important than ever to understand why projects fail and how to get them back on track. The center left think tank, Third Way, surveyed over 200 clean energy professionals to answer that exact question. Their newest study identifies the top non-cost barriers to getting projects over the finish line, from permitting challenges to nimbism. To read Third Way's full report and to learn more about their pace initiative, visit thirdway.org slash pace, or click the link in the show notes. I'm Shale Khan. I lead the early stage venture strategy at Energy Impact Partners. Welcome. All right, so here's a cycle that I have seen play out many, many times. Developer X of Project Y wants to go build a big new project. And somewhere in the development process, they engage in EPC, EPC being an engineering procurement construction firm. There are many of them, many of them are very, very large, and they're kind of ubiquitous if you're going to build a big capital project. They engage the EPC, and then there's like a fairly painful process from there on where it's slower than they want it to be. They have to go through multiple cycles of iteration. There's value engineering. The cost comes in higher than they expect. And then at the end of the day, there are a bunch of change orders anyway, and there are cost overruns, there are time overruns. It's not necessarily the fault of the EPC, right? The cost of things can happen in the world. But the process of actually getting big projects constructed is notoriously hard. And some folks have been wondering whether there is a way to leverage modern technology, AI included, to do it better. Certainly that is the opinion of Alex Modan, who is our guest today. Alex is the CEO and co-founder of Unlimited Industries, which is a startup that reasonably unstealthed to basically be a modern EPC. They're starting to start from scratch to build a company that will do engineering, procurement, construction with new technology with a different business model to try to get projects built faster, more on budget, potentially cheaper. It's an interesting question, and it applies to everything from data centers to power generation projects, battery storage projects, new chemical plants, anything that we might build that's a big capital project. So it's relevant to basically everything else that we talk about here. Anyway, here's Alex. Alex, welcome. Hello. Thanks for having me. Thanks for being here. Let's talk about EPC, engineering, procurement, construction. You founded a company based on the premise that it could be done better. So I want to start with what's wrong with it today as you've observed EPCs and how they work. Like fundamentally, what is the problem that you see? Yeah. It might be first contextualizing like the what is EPC and where's the play in the value chain. So to start this, let's just imagine we're going to build, I don't know, let's just say a data center project. So if we want to go build a data center project as an owner of a project or a developer for project, the first thing that we do is we kind of find the land and we contract that and we start to figure out what permits we need. We figure out a power we need to buy. And then you typically go find an off taker or some sort of tenant for that actual data center project or again, whatever the product is that comes out of the facility. And then lastly, big contract that you're trying to do is go find the third party that's going to design and build the thing. EPC, engineering, procurement, construction is basically the vertically integrated or all three of the kind of design, manage the supply chain, actually manage the build of the project, all three of those in one contractor. And for all intents and purposes, they are the people who design and build the thing, that EPC piece. And so I think when I got into it, I was actually really excited the first time we went to go work with an EPC thinking that there's going to be all these different kind of disciplines and institutional knowledge under one shingle. I think the biggest thing that's challenged with the industry today is that the incentives between the developer, the person who actually wants to build a thing better, faster, cheaper, some sort of metric like that versus the third party, the incentives are so fundamentally misaligned because of how these contracts get produced. And so you either have the vast majority of these contracts have some element of cost plus where we'll give some sort of guidance about how much the project's going to cost. But as a third party contractor, your incentive is really, we only make money when the project costs more and takes longer. Or there's sometimes these contracts that run on a fixed firm basis where that third party will actually say, hey, we'll build it for this much. But they define the contract so tightly that inevitably what happens is when you learn something through the design process or when things change, which inevitably they always do, you get issued these massive change orders. And that's at that point, that's where that third party EPC typically makes all their margin. It's because you're already working together and you can't really easily leave. So yeah, high level, what's broken is it's the incentive structure. The incentives between the third party or the actual developer who wants to build better, faster, cheaper and the third party who's in control of making all the decisions about what gets designed and what actually gets built. If I'm being charitable to EPCs and I asked the question of like, why is it cost plus typically or fixed firm, but we're change orders or the norm, I would imagine in part, is because there's one, there's this like time lag, right? Like they're signing the contract on day one, but they're actually procuring the commodity materials six months later, a year later, 18 months later, whatever it might be. Those prices are volatile. They would have to hedge them every single time if they weren't going to do that. And not to mention labor availability at labor costs and all this other stuff. So is it because they just can't wear that kind of a risk, the like time risk or is your experience that there's a different reason why it ended up being cost plus or fixed firm with all sorts of change orders? No, I mean, definitely just steelman. It's because it's ultra complex to build these projects. There is hundreds of people that will be evolved in how the thing gets designed. That manages the entire supply chain to get it built and then manages all of the like boots on the ground of the project that's actually doing the building. So there is tons of complexity. And the reason that these contracts are structured in this inherently flexible way is because it's really hard to know at front how much exactly is the thing at a cost. So yeah, it certainly is like evolved this way because when you think about these projects and I mean, the small version of these are like $100 million. The more normal version of these is many hundreds of millions and then the bigger ones are in certainly in the media billions of costs. So because you have all that risk, the contracts effectively are this big like risk mitigation strategy. And that the easy way for the EPC not to take any risk is just to say we're going to get a margin on top of what it costs us to build a thing. But the problem is that there's no motivation then to make the thing cost less. If anything, there's a motivation the other way to make a cost more. Yeah, you mentioned there's a lot of people and a lot of uncertainty. I mean, I guess one thing, one way I think about it is that there's a spectrum of different types of projects on one end of the spectrum is a thing that looks the same every time you build it more or less. I mean, maybe a version of this would be, I don't know, utility scale battery projects or utility scale solar or something like that where like, you know, we've built hundreds of them. They generally look the same wherever you go. You would imagine that on those types of projects, it would be pretty well known. Maybe there's some commodity risk. Maybe there's some labor risk, but like broadly speaking, the designs are going to be similar and the experience is high. And then in the other end of the spectrum, there's like a first of a kind thing that's never been built before. And they're obviously your error bars on the cost and the time, the labor and all that is going to be highest. Do you? And then there's a bunch of stuff in between. I imagine data centers are in between actually, because in some ways they look the same. There's like a powered shell. The shell is a building. But then inside the data center, right? Like there's all sorts of innovation. Like this one liquid cool. Is this one using which in video chips is it using? How is it designed? There's so much innovation in that space that it's changing all the time. Which also changes the facility too, right? Like it's these changes trickle out for sure. Right. So I guess as you think about this, like the incentive structure being misaligned, do you view it as being more misaligned on the, I guess, relatively speaking, easier stuff to build? Because there you should be able to have high level certainty on what the cost should be a priori. Or is it more misaligned on the esoteric stuff because the error bars are even higher. And so the risk aversion of the EPC leads them to like way over price, for example. Yeah. Yeah. Like the contingencies or the greedlessness of this like cost misalignment is certainly true with the more custom the project gets. So the first of the kinds are a great example of that. Or like you even mentioned data centers that are shifting in their requirements, of course, have more and more of this baked in. It's basically just correlated to risk. Like how much risk is there in this? Solar has less and less risk as we deploy more and more the exact same way. Same thing with utility storage. But I do think that like where you see, so you definitely get. Missile line contracts or misaligned incentives, which kind of balloon costs. And that is the kind of status quo for how you contract these things anyways. But the other other thing that kind of falls into this is how projects broadly are approached. And so if you think about how we build products, like we have a metric for it, right? We say, hey, we're going to make this widget. It's going to come out of a factory. We'll have some sort of cost per widget metric. And that allows us to kind of approach these projects with this continuous improvement type approach and drive costs down. That's like our learning rate, I think arguably like that's why solar has fallen so aggressively is because we keep kind of making or at least I should say the module costs, which is even different than this is maybe a side tangent, but it's like different than the actual installer costs. Yeah. Well, it's actually the problem with solar is that the module costs have fallen much faster than the install costs have, right? We haven't solved it on the construction and all the other commodity materials and stuff. Yeah. And that's because all progress happens with this like, you know, highly iterative process and this could continue improvement process that we get out of manufacturing a thing. But when we move into projects world, everything's a snowflake. It's like, you know, everything's N of one. Even a project that we're building, you know, we built lots of refineries. That next refinery that we build is still an N of one project. Because it's approached in that way, you don't really get any sort of benefits of learning through that process. And, you know, by definition, each project is going to be unique. It's a different piece of land. But yeah, and again, maybe this is a tangent, but that's like another big problem in the industry is everything's done from scratch. So even if you're going to go design a new, you know, a slightly different data center or solar project or something else, that sort of every tweak or change that you make effectively reduces the entire redesign of the process. And so yes, there's some stuff that gets lifted. But the overall amount of work that happens is not the kind of delta of the change. It is like a significant rehaul of work. And it keeps, it just keeps us from like learning and iterating to improve how something gets designed, not just in terms of saving engineering costs, which is actually a pretty small piece of the cost anyways, but actually like kind of having a better design that improves the overall cost or performance of the facility. I guess one thing I wonder is, okay, so let's take EPC, right? Like in engineering procurement and construction, just taking out the simplest base layer of what the work entails. Let's just say you could dramatically improve because what you're describing is like the learning, I think, I guess maybe is on all three, but predominantly on like engineering, can you design this thing so that it's fast and easy to design is definitely going to work up front? My guess is that in most of these big capital projects, the E out of EPC is the smallest slice of the pie in terms of total costs. And like the bulk of the cost probably comes from a combination of procurement, the actual physical materials, much of which are going to be like steel and cement and aluminum and, you know, commodities that have prices that vary. And then construction, which is sort of a function. It's an outcropping of how the engineering occurs, but certainly is also just driven by construction process and labor and so on. So as you think about, like, where is there an opportunity to deliver cost reduction and also cost certainty? Like how much of it is the E versus the P versus the C? Yeah, totally. Okay. So those are two separate things, right? Like, so let's say cost certainty versus like an overall reduction cost. On the certainty side, everything, it's really kind of planned out or teased out in the engineering portion. Right. And so generally, again, life cycle of these projects are, you know, you start with some conceptual design, you understand maybe plus or minus 50%, how much the thing is going to cost. Then for the most part, you dig into this like front end engineering design and you will get to totally changes per project, but say you get to like plus or minus 10% of cost. And that gives you enough confidence to go to a lender, an actual bank and say, hey, let's underwrite this project. We think you're going to get the IRR you need. And then we move forward. But in the vast majority of projects, when you go to final investment decision and you have plus or minus 10% estimates, you're only maybe 30% of the way through your engineering. 70% you still have to go do. And you just, it doesn't make sense to do it upfront because it takes a lot of time and it costs a lot of money. And it's a lot easier to finance that with the actual lenders money rather than the development capital. So the certainty that you have going into a project before you actually take the money from the bank to go build it is actually pretty low. Like there's so much more work to go do. And you just don't do it because there's a high marginal cost at the engineering. So one way that you really reduce all the risk, if you will, or all this like uncertainty from how the project is going to work is if you could instead just take these projects further through the design process so that by the time you're, you know, you're doing your, your earthworks or you're moving some sort of ground, you're actually 100% done. You have your issued for construction package, which doesn't happen today. And so there's, there's that big piece on like, how do you remove uncertainty? How do you remove costs? I mean, yeah, you're totally right. The engineering cost is anywhere from like three to 10% or 15% of the project costs. So it's not a huge lever in itself. But if you think about the way these projects are designed today, because again, such a high marginal cost to engineering, what you end up doing is you only really design something one pass through. There's no sort of iteration. There's no optimization of design. And that leads you to a spot where you kind of make early decisions, you freeze those design decisions, you kind of do the next stage gate of design, of your design process. And you eventually get stuck in these like very like low local optimums rather than ever understanding what the global optimum of that design processes. So, yeah, I mean, like I can get more into how we approach this problem, but the there's huge gains and opportunities to use the E to rapidly reduce the P and the C. You heard the phrase speed to power a lot lately, but here's what it really means. AI data centers are being told that it will take years to get grid power. They can't afford to wait. So they're turning to on site power solutions like bloom energy. Bloom can deliver clean ultra reliable, affordable power that's always on in as little as 90 days. Bloom's fuel cells offer data centers other important advantages. They adaptive volatile AI workloads. They have an ultra low emissions profile that usually allows for faster and simpler permitting in their cost effective to. That's why leaders from across the industry trust Bloom to power their data centers ready to power your a future visit bloom energy.com to learn more. The grid wasn't built for what's coming next. Electricity demand is surging from data centers to EVs and utilities. They need reliable, affordable solutions that don't require building expensive new infrastructure. Energy Hub helps by turning everyday devices like smart thermostats, EVs, home batteries and more into virtual power plants. These flexible energy resources respond in near real time to grid needs, balancing supply and demand. Plus they can be deployed in under a year and at 40 to 60 percent lower cost than traditional infrastructure. That means more reliability, lower costs, cleaner energy. You can't get much better than that. And that is why over 120 utilities across North America trust Energy Hub to manage more than 1.8 million devices. Learn more at energy hub.com or click the link in the show notes. Do you ever wonder why it takes so long to get clean energy projects up and running? Do you have permitting reform on the brain? Are you NEPA reform curious? The new pace study from Third Way pinpoints the non-cost barriers that stand in the way of clean energy deployment and keep new solar and transmission projects in limbo. They surveyed more than 200 industry professionals to understand what's slowing down deployment and offer practical solutions on how to fix it. To read Third Way's full report and to learn more about the pace initiative, visit thirdway.org slash pace. OK, let's talk about how to use technology, which is I think what you're alluding to here, right? Like this is an area where my suspicion is generally it's not the first category of AI adoption, or at least it hasn't been historically. Where do you see the biggest opportunity? Like where are the step function improvements that one can achieve by leveraging modern technology, be it AI or something else? Yeah. So we have built a lot of technology to help accelerate that pre-construction phase. So this is basically the E and lots of the procurement management side of things. And we have a software and kind of like an AI native version of this that helps accelerate the chemical design, mechanical, electrical, civil, structural controls. And in doing that, we basically are building a platform that allows us to remove that risk, but at a very, very low marginal cost. So that helps you accelerate through the design process, not just saving time, but exploring a much wider search base and getting to much greater levels of definition before you actually have to freeze designs. And that has a big impact to the previous point, right? It's just like removing uncertainty, you know, reducing costs from through optimization. Can you go level deeper on that? Like, what does that actually mean to reduce uncertainty using software? Like, what are you actually doing? Yeah. This is, you know, if you start a project today at like zero, you know, it's completely uncertain, right? And as you start specifying, we're defining the bringing definition to that project, which is the process of engineering it. You start to see more, you start to remove risk, right? You say, hey, you know, we can actually size a pump that will solve that, you know, moving the fluid from point A to point B. Or, you know, actually we can design a tank like this to hold this amount of volume. And once you start providing that level of definition, you remove a lot of the kind of ambiguity, but there's still tons and tons that goes into, you know, the thousands of documents that you'll kind of generate to design a plant fully. So this is what we kind of built. Like it is a platform that our engineers use. I think the important distinction here is this is not like a software technology that we sell. It's like we use this as an internal tool, ourself. So our engineers have this kind of AI platform that has all of their data in one place, all the kind of requirements about the project, all of the potential vendors that could be used. And they use it to basically accelerate that design process really significantly. We haven't like redefined how you're engineering these projects. We've kind of augmented it, like significantly augmented the people that are doing that work so that they're able to just with the same amount of time and capital, they're able to define way more and provide way more kind of engineering definition, as well as like time to optimize those designs so you can remove kind of suboptimal design. Can you articulate a little bit like what is distinct about this from the, you know, that EPC has been using engineering software that is super mature for a long time. Maybe that's the problem, but like what's distinct about what you just described from what you could just get off the shelf to design a project? Most of the software in this industry really hasn't changed much in the last like maybe 20 years. And I again kind of always come back to the incentive structure. If there's no real incentive structure for an EPC to want better software, it's the same reason your law firm doesn't necessarily want like AI software to help them accelerate and reduce their billables. So the way these software work now is that there is a bunch of kind of distributed software solutions or maybe decentralized solutions. So you'll have a CAD tool that you use to actually like do the designs. You'll do almost all your kind of rough engineering or hand calcs in Excel. You'll use dedicated simulation software for some sort of fluid design or structural analysis or whatever that might be. And then you use another tool to like review those PDFs, you know, with the tool to put red lines on them and go through design cycles and then push to drafters and then other tools to manage, you know, how do you reach out to your vendors. And anyways, there's all these different solutions that software solutions that you're using in order to like kind of quote unquote like do engineering. And what we built kind of from day one was just a modern version of the software. So we put all those different disparate tools into one underlying platform. And we gave modern software to them, which is like, you know, automatically doing version control and collaboration and everything that you would expect out of just like a modern tool. And what that allows us to do is not just kind of have much more streamlined workflows, but it allows us to kind of build AI in as a core primitive to the solution. So now sometimes our engineers will do some sort of work manually within one of our softwares, but often they will dedicate a kind of delegate a task to an AI that knows how to use all those tools in our software so that it can do that design task and then surface that back to the engineer who delegated it. So it's at least in significant part, it's like integration of a bunch of what otherwise are kind of disparate workflows to allow for faster iteration and more automated iteration. Like you do a change somewhere in one piece of software right now, you'd have to go to another piece of software, like make the change in that software, see how it changes the other thing, do that over and over again. So it's like, it's like, build everything into a single comprehensive suite, basically. Yeah, totally. There's like one underlying data model. There's not, you know, the same piece of data that shows up in 10 different softwares. And that's really helpful, not just again for streamlining your own workflows, but it's so that an AI can do it. And it doesn't have to like manage all these different tools and interfaces that, you know, the same information is repeated, but out of date or edited by the wrong person who didn't have permission or something else. When you say AI in this context, what do you, what do you mean? Like, is it LLM type AI where you're like, I want to be able to, in my single pane of glass, be able to ask, you know, like, what would happen if I changed all the screws and this entire project from X to Y or something like that? Like, is it LLM AI? Is it what is the AI? Yeah, it's also a version of what you're describing that's like enterprise software for sure, but doesn't, and like there's a big data component to it. It's not inherently AI. Yeah, no, it's like an exact use case, the one that you gave that, you know, you would do. So, so we'll like record all our meetings. And that meeting you're having with a handful of different disciplines of engineers or people who understand the commercial, the agreement, and what that'll trigger through that recording is like a bunch of tasks for AI to go do. And it's, you just basically mentioned a trade there, right? It's like, what if we had, I don't know, one type of material over another type of material, one type of conveyance over another type of conveyance for this project, this material? RAI will go explore that different design path. And that's a trade that you would give to an engineer and you say, go spend a week, like assess the trade off between using, you know, belt conveyance or pneumatic conveyance for some sort of material handling problem, and then tell me back what do we feel like is the right kind of total cost of ownership trade. And then it'll just explore that. And so that's an LLM who's doing a lot of that work. And importantly, you're not just kind of asking a chat GBT or something in a sort to go explore that and produce something, which it'll do. And honestly, it'll do in a pretty impressive way. But it's the way that we've built it is that it's grounded both inside of our, inside of the data for the project, because it has all that in one place, and then inside of the tools that we've fed it. So it knows how to use a, you know, how to down select actual vendors or how to read spec sheets that we've fed it or how to use simulation software that we fed it in order to derive these answers. What did you mention data centers? And I think you guys put that in as one of the categories you're most interested in in in your announcement when you guys unveiled. Is that because we're building a lot of data centers? Or is that because there's like something specific to data centers and data center construction and engineering that makes it especially attractive here? Or is there something else that you would describe as like the perfect prototypical use case for this in the early days is X. So these projects have a handful of different, a handful of different reasons of why you would apply this technology. In the data center use case specifically, speed is really an important lever for them. And so you can take, you know, instead of the design portion taking, call it six to nine months of your critical path, we can massively collapse that period of time. And that's really important for these data center projects to kind of accelerate how quickly we can actually build them. The same piece though with the fact that, you know, if you right now budget six to nine months for your design portion, almost none of that you're going to start to figure out, well, how do we optimize that design or do some sort of value engineering to save costs or to design for long lead items or to design for constructability? So really strong value prop in these data center projects. As we as we end up kind of increasing the capacity that we have on our team, though, it'll make sense to pick up basically many of these types of large industrial projects, of which we've already pushed the kind of technology to be able to demonstrate. And it will end up being like really applicable across all types of construction. So if you can deliver more certainty up front, then that presumably is the thing that unlocks a different kind of business model that hopefully solves the incentive problem. Is that just a fixed price contract with no change orders allowed? Or what does that look like? That's exactly it. We literally put like no change orders in our contracts. So you wear the risk, like you wear all the risk, basically. If there's a cost overrun, you're going to you're going to go negative on the project. And then the premise here is that because EPCs notoriously don't have huge margins, right? Construction firms are like high volume, low margin type businesses. And so I guess that the thinking on your side has got to be because we're reducing the cost of everything along the way, we naturally have higher margins, which would allow us a little bit of a buffer if things do become a little more expensive than we expected. At least we're not like in the red. Do I have that concept right? Yeah. I mean, we definitely will have a different margin profile than the industry, but it's not that because even when you think about it, the engineering margin, it's like it's pretty small because the spend of the engineering is actually really small relative to the larger construction scope of the project. The important part of what allows us to absorb that risk without the kind of risk that we go into the red or at least in a risk adjusted way is because we've been able to do so much design upfront. And we've been able to not only kind of like remove all that extra risk by taking the definition instead of doing that at 30%, we can take that definition all the way through 100%. You just remove all this ambiguity of how much the thing should actually cost. So when you, again, traditionally, when someone gives a contract to go build a project, because they have such little core definition, they don't know all the vendors that they're going to buy from, you even mentioned, I think this example earlier, there's like a tariff risk. By the time that you fund the project and you commit to a cost, tariff can change. In six more months, you need to finish the design in order to order the parts. That doesn't exist in our world because we've been able to just very, very quickly remove all this risk before we actually commit to a price. That's very unique. Wait, how does that work? So that's a good specific example. So I guess what you would do is issue the purchase order for all of the material before you commit to a price to the customer. Like, how do you avoid the tariff? Yeah, exactly. It's timing it all out. So when, just to compare this again to how it works today, is you'd only, when you say, hey, we're the EPC, we're going to give you a performance guarantee or some sort of fixed firm price when you go to buy this or fund this project with a notice to proceed or final investment decision. At that point, you literally don't even know all your bocks. So you're all your commodity equipment. You're the steel that you need, the wire that you need, et cetera. We will know all that at this point in time because we're not 30% definition, we're 100% engineering definition. And because we have this entire equipment list at this point in time, we know either how to say, hey, let's purchase everything at the same day that the project gets funded. Let's go either purchase everything or let's design hedges from a cost perspective so that we're not exposed to some sort of volatility or risk. That's complex. I'm thinking of these big construction projects, and you're either on the day that you sign the contract, you're either buying all of the steel, just as a random specific example, or you're hedging steel prices at the project by project level to make sure that you don't overpay for steel. That's what you need to do. Yeah. That's what allows you to truly have a very different risk assessment for how these projects get built, and that what gives you a ability to do a fixed firm and actually mean it. Most of these fixed firms aren't really like industry themselves. We've seen time and time again where it's like a fixed firm doesn't really mean a fixed firm because there's inevitably some way that in the thousand page contract that you assigned with your EPC, you went out of scope and invalidated that price. And that's the kind of change order trickery. I guess the other thing that I wonder how you manage is subcontractors, right? It's another area where change orders come from not just the general contractor, the EPC, having to make a change order, but also because some subcontractor made a change order. So do you have to flow this all? First of all, are you going to use subcontractors? You're just going to do everything in house and have a ton of people on staff. And if you are going to use subcontractors, do you have to then flow this change in a structure down through all of them where they can't issue any change orders to you either? Yeah. Certainly long term, we will do everything in house. We will vertically integrate as much as possible because that's where you can fix the incentives all the way through. Yes, for these initial projects as we take, we are working friendly with like the local GC or even the trades, even the actual folks that we will subcontract a lot of this workout to. For that, it is making sure that we prior to entering into a contract. You have the kind of the same thing where often those contracts are not as the level of definition that you have when you're asking someone to actually give you a fixed price on what a scope of work is, is very hard. We don't have that same problem because we take things way deeper through definition. We have a much better understanding of what should this actually cost to go build. So yeah, there's a handful of different ways that we kind of approach working with the actual subcontractors as we do this to hit our performance goals. But it is this kind of like a continuing loop of how do you remove more and more risk through design? All right, so wrap it up. You don't have to give me exact numbers, but when should we expect to see the first COD? When should we expect to see the first project constructed by you guys? I love it. Yeah, well, hopefully see you next year. So hopefully by next year. Yeah. Fast is the rule. Alex, thank you so much for the time. Really appreciate it. I appreciate it. Thanks, Joe. Alex Modon is the CEO and co-founder of Unlimited Industries. This show is a production of latitude media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prayly Adventures. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Marquand. Stephen Lacey is our executive editor. I'm Shale Khan, and this is Catalyst.",
    "release_date": "2025-12-11",
    "duration_ms": 2101000,
    "url": "https://traffic.megaphone.fm/PSMI7662370006.mp3?updated=1765427496",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-12-24T01:53:59.578951"
  },
  {
    "title": "Who benefits from the AI power bottleneck?",
    "description": "The bottleneck holding back AI is a scarcity of power, or so goes the story. That may be true \u2014 and plenty of reporting backs it up \u2014 but different actors in the space face varying incentives to play up or play down that narrative.\u00a0\n\nSo what incentives are at play, and how do they shape each player's story?\n\nIn this episode, Shayle talks to Shanu Mathew, senior vice president and portfolio manager-analyst for U.S, sustainable equity at Lazard. Last month on X, he posted a breakdown of the actors \u2014 including hyperscalers, chip makers, utilities, and others \u2013 and how the different incentives they face shape how they talk about energy and AI. They cover topics like:\n\n\n  \nHyperscalers\u2019 mixed incentives: the benefits of building their own capacity vs encouraging others to overbuild\n\n\n\n  \nWhy equipment makers, chipmakers, and land developers benefit from talking up the bottleneck to boost demand for their services\n\n\n\n  \nHow independent power producers and gas players benefit from high prices\n\n\n\n  \nHow the power-bottleneck narrative has shifted over time\n\n\n\n\nResources:\n\n\n  \nLatitude Media: ERCOT\u2019s large load queue has nearly quadrupled in a single year\u00a0\u00a0\n\n\n\n  \nLatitude Media: The power bottleneck is changing data center financing\n\n\n\n  \nLatitude Media: Early-stage data centers are driving up US power demand forecasts\u00a0\u00a0\n\n\n\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.\n\nCatalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting BloomEnergy.com.\n\nCatalyst is supported by Third Way. Third Way\u2019s new PACE study surveyed over 200 clean energy professionals to pinpoint the non-cost barriers delaying clean energy deployment today and offers practical solutions to help get projects over the finish line. Read Third Way's full report, and learn more about their PACE initiative, at \u2060www.thirdway.org/pace\u2060.",
    "summary": "The podcast discusses the importance of understanding incentives in the tech industry, particularly related to power constraints in AI growth. Hyperscalers like Amazon and Microsoft have incentives to both promote power scarcity and potentially benefit from it. OEMs and EPCs involved in data center hardware also benefit from promoting the power scarcity narrative to drive demand for their services. The podcast emphasizes the complexity of these incentives and the need to critically evaluate information from various market players. It also highlights solutions like on-site power solutions from companies like Bloom Energy and Energy Hub's platform for transforming connected devices into flexible energy resources.",
    "transcript": " A very brief word before we start the show. We've got a survey for listeners of Catalyst and Open Circuit, and we would be so grateful if you could take a few moments to fill it out. As our audience continues to expand, it's an opportunity to understand how and why you listen to our shows, and it helps us continue bringing relevant content on the tech and markets you care about in clean energy. If you fill it out, you'll get a chance to win a $100 gift card from Amazon, and you can find it at latitudemedia.com. We'll just click the survey link in the show notes. Thank you so much. Latitude Media, covering the new frontiers of the energy transition. I'm Shale Khan, and this is Catalyst. Who has incentives to talk up the power constraints, and who may not have incentives to do that and maybe take the opposite view where it's actually not as big of a deal, and where my economic incentives actually inform or influence the reviews more than is appreciated by the market. Coming up, show me the incentive, and I'll show you the behavior. The AI boom is here, but the grid wasn't built for it. Bloom Energy is helping the AI industry take charge. Bloom Energy delivers affordable, always-on, ultra-reliable on-site power. That's why chip makers, hyperscalers, and data center leaders are already using Bloom to power their operations with electricity that scales from megawatts to gigawatts. This isn't yesterday's fuel cell. This is on-site power built to deliver at AI speed. To learn more about how fuel cells are powering the AI era, visit bloomenergy.com or click the link in the show notes. Emerging electricity demand is testing the limits of the grid, but Energy Hub is helping utilities stay ahead. Energy Hub's platform transforms millions of connected devices, like thermostats, EVs, batteries, and more into flexible energy resources. That means more reliability, lower costs, and cleaner power without new infrastructure. Energy Hub partners with over 120 utilities nationwide to build virtual power plants that scale. And how the industry's leading flexibility provider is shaping the future of the grid, visit energyhub.com. Clean energy is under attack, and it's more important than ever to understand why projects fail and how to get them back on track. The center-left think tank, Third Way, surveyed over 200 clean energy professionals to answer that exact question. Their newest study identifies the top non-cost barriers to getting projects over the finish line, from permitting challenges to nimbism, to read Third Way's full report, and to learn more about their PACE initiative, visit thirdway.org slash PACE, or click the link in the show notes. I'm Shail Khan. I lead the early stage venture strategy at Energy Impact Partners. Welcome. All right, so this episode is a reminder to always monitor incentives. More specifically, we're talking about the narrative and certainly the reality today around power scarcity as the rate limiter on the growth of AI. Everyone is talking about it. I actually saw a chart recently that showed the frequency of mentions of the word power or energy on S&P 500 earnings calls over time. And you can imagine the shape of this chart. It had been pretty steady, actually trending slightly downward over time until 2025 when the line literally went vertical. There's basically no question in my mind that power is actually a bottleneck for data center growth today and perhaps therefore for AI growth. That's sort of a truism. But it could also become a comfortable scapegoat depending on where things trend in AI world. And because it's convenient and because it's widely discussed, it's important to think one level below the surface about what you hear from various players in the market. That's why I really loved this recent post on X initially from Shanu Matthew that we're discussing today. Sean has been on this pod before to talk about public market stuff because he's a senior vice president and portfolio manager for US Sustainable Equity at Lazar. But he recently went online and went meticulously through basically every player in the AI slash energy world and laid out their incentives. Should they drum up concern about power shortages? Should they downplay them? Should they capitalize on them? This conversation for me is foundational and should act sort of as a reference and a reminder for me anyway. Anytime I hear something new about energy and AI from one of the major players in the market. So let's go through it. Here's Shanu. Shanu, welcome back. Hey, Shell. Thanks for having me on again. So what got you started thinking about the incentives of all the different actors involved in data center power nexus? Like what sparked the thinking? It's a good question. I feel like everyone has been thinking about this for the last two years or at least hopefully so I don't feel as lonely. But I think the premise for this tweet or thought that I had was that there's general understanding that there's a few ground fruits and ones like AI is a foundational technology that's going to persist in society. Two is we need a lot more energy infrastructure. The magnitude and duration of the cycle is of the debate. But it introduces a duration mismatch where technology is often done on cycles that are one to two years, whereas energy is typically multi-year or even multi-decadal depending on the type of asset. And what I was trying to gear in is psychologically humans or organizations have incentives and incentives drive behaviors as everyone knows. And how could we take that lens and apply it to the power debate? Because so many times I have folks coming to me from earnings calls or from events or conferences and saying I heard this from X-parties. Or this from Y-party. And that's completely at odds with what they heard from another player down the stack. And so what we were trying to do is a clean sheet exercise was looking up and down the supply stack for AI. Who has incentives to talk up the power constraints and who may not have incentives to do that and maybe take the opposite view where it's actually not as big of a deal. And are there economic incentives that may inform or influence their behaviors? The goal wasn't to call out companies being misleading or anything like that. It was just namely to acknowledge where my economic incentives actually inform or influence or views more than is appreciated by the market. Yeah, I read the original post and it was like in some ways it's kind of obvious. But at the same time it was sort of like a lightning bolt to me. I was like, oh yeah, I hear this stuff constantly as well from every corner. Everybody is talking about the power bottleneck. And it is important to examine the incentives that drive anybody's behavior or the narrative that anybody's telling about it. Not because there isn't a power bottleneck. I think you and I probably agree. There is for sure. 100%. But the magnitude, the duration and certainly it probably won't always be one. And so as this goes through whatever cycle it's going to go through, we need to be paying attention to who is saying what and examining the incentives that drive that. So I thought it was super interesting. What I wanted to do with you is kind of talk through some of them. What we are hearing from these areas parties and then the incentives that they have to say what they are saying or maybe to say what they're not saying. But let's start by categorizing. Walk me through at the high level. There are a bunch of, there are a couple of groupings of actors that have similar incentives. So how do you think about the groupings? Yeah, at a high level, I'll try to keep it simple. But again, there's various degrees and nuance required. But at a high level, let's start with the hyperscalers who are the folks that are driving a lot of this CapEx and investment cycle. These are, think of it as the cloud service providers, the Amazon, Google, the Microsofts of the world as well as including Meta which is another hyperscaler even though they don't have a legacy cloud business. That drives investment spend on GPUs and that's the equipment. You have the technology hardware, if you will, that includes GPUs, CPUs, custom silicon offerings. That moves upstream into the supporting equipment which includes electrical and cooling equipment. And then if you go outside the data center, then you start to get into who supplies that power equipment as well as the overall actual power of the facility. And moving upstream there is who actually builds power which is like utilities as well as the labor and the EPCs and the engineer and that goes into those types of facilities. So if you just go from who spent in the capital and follow that down the stream, that's a general way to think about some of these bigger pockets. And so I happen to jump into any of those. Alright, so let's start with the hyperscalers. They're obviously at the epicenter of the data center build out. How do you think about their incentive when they talk about power? Definitely. And I think this is the most interesting from a game theory standpoint. So just a backup for the audience that may not be entirely clear, right? The hyperscalers have really strong incumbent businesses, you know, oftentimes depending on your talk about cloud or digital advertising and they think that the spend on AI allows them, you know, a better product opportunity in the future such that they can accelerate growth of cloud or advertising offerings or enhance their product offerings, which you know, leads to better returns over time. And so you have two options to go out and get more capacity, right? One, they build their own capacity, but then two, they also can go out and procure capacity on the external market. And that comes from neo clouds or co locators or things that we can get into. But if you think about it from their perspective, right, building your own capacity is typically what they prefer because they have control and we're doing integration when they have the chance to because they have these, they have teams internally, they have the expertise. But when you go back that core issue, right, is that I think everyone agrees that we need more energy. I think what the uncertainty comes in is how much energy do we need? And if you think about it from the perspective of them, if I have to go out and build all this infrastructure and this infrastructure is, you know, beyond the next few years, I need to potentially potentially be comfortable underwriting it for the next 10 20 that is a natural mismatch, right, where it's like, okay, do I want to be the long term risk taker of the assets that I'm underwriting? I might be comfortable doing that to an extent, but beyond that, do I perhaps let the market figure out the excess capacity? So if you're a hyperscaler, the argument forward be if I really think that this is like the greatest demand super cycle and I'm severely constrained, I might try to go out and build as much as possible in the procure all the extra. The, you know, the skeptic or the pessimistic view would be they see line of sight into the demand they need and that's all they're going to be comfortable building to and the rest of the market sort out and such. So if you think about that from a power standpoint, my incentive in that scenario would be to talk up power constraints, right, saying that there's not nearly enough of it. We need to build so much like this is the greatest cycle because that will incent speculators, developers, other companies like neo clouds and stuff that want to compete for leases from hyperscalers to go out and build infrastructure. And this one's interesting because Satya and Nadella, for example, the CEO of Microsoft kind of told you this actually on a podcast earlier this year or the late last year, he was on dark question, talked about when he was asked, why not commit more resources to building more assets? And when he ultimately said, well, I'm seeing a lot of activity from developers and co-locators and neo clouds. And I think that I might be able to procure cheap supply in the future because who benefits if there is an overbuild or too much build and conditions loosen, that means cheaper leases for them. And that might be cheaper than actually going out and building your own catbacks. And so that's kind of the game theory of one or the other in terms of why build or why when I might just wait to see how the chips may fall. They're complicated, I think, because you can see it going either way. On one hand, they clearly have an incentive to say that there is this massive bottleneck in the form of power and power infrastructure because that hopefully from their perspective incentivizes the world to build more of that stuff so that they can build more data centers. So they should be shouting from the rooftops, this is the bottleneck. Also by the way, it's convenient. It may indeed be true, but it's also convenient for them to be like, the bottleneck is definitely power. That means it's definitely not demand for their services. And again, that may be true today, but they could say that even if it weren't true, and that would be helpful to them. So to me, there's a very clear incentive for them to say this power bottleneck is huge and insurmountable and we need to have a mobilization, the likes of which we've never seen in the energy sector to solve it. On the other hand, if they don't solve it, if the world doesn't solve it, I should say they are probably better positioned than anybody to snatch up the scarce resource that is the available power supply in the sense that if they are going to be forced to build their own capacity, which many folks are doing, who's in a better position to write long lead orders for a G, vernova turbines, than the hyperscalers who have big balance sheets and can afford to pay for it and so on. So there is an extent to which if you're scared of the neoclouds, for example, and you're a hyperscaler, you don't necessarily entirely want the world to solve the power bottleneck for you because it's sort of a source of competitive advantage, at least against that other class. My guess is that between those two incentives, the one that says, no, we just want the world to solve the power problem because our job is AI, that's probably the one that wins out, but it doesn't seem pure as the driven snow to me that it's like that clear. You hit it on the head, right? I think it's really, really complex when you pull it back to layers. I think the one thing I would add, right, is that a lot of the motivator for spending all those capex is that each dollar of capacity not online that could be served, could be serving additional demand is revenue dollars that you're not making. And so that opportunity cost is so great of not having power. So to your point, you may be comfortable, especially on, like, for example, a gigawatt data center being $50 billion, spending $x dollars of millions of dollars procuring deposits for long lead items, turbines, electrical equipment, switch gears, circuit breaks, et cetera, just to make sure that the long lead items are actually settled to build. So if the power is there, then you can go ahead and plug the shells in immediately. But there is an ability to walk away or cancel or just lose it a positive contract as well. But their incentive would be to lock up as much capacity to your point as possible, right? Because they want to have the optionality. But I think they also are playing for worst case scenario, you can walk away from a few million dollar deposits. And I'm just speaking hypothetically here versus the option to cost of building a multi-billion dollar data center. So there is all these different levers they can pull, but it definitely feels that they're keeping optionality. And then just to be clear, too, they are bringing on a lot of their own capacity, right? Amazon brought on 3.8 gigawatts in the last 12 months. So they're going to double their footprint in the next two years. Microsoft said they're going to double as well. And they brought on two gigawatts in the last 12 months. So they are bringing on a ton of capacity as well. But it is just interesting to see they're also striking deals in the new cloud. So are they the bull would say that there's nowhere near enough supply and then they're forced to do that. And the bear would say, well, actually, there is want the speculative risk to go to the new clouds and then they can walk away from the contracts if market conditions deteriorate. And it's, I think no one really knows. Time will tell. You heard the phrase speed to power a lot lately. But here's what it really means. AI data centers are being told that it will take years to get grid power. They can't afford to wait. So they're turning to on site power solutions like Bloom Energy. Bloom can deliver clean ultra reliable, affordable power that's always on in as little as 90 days. Bloom's fuel cells offer data centers other important advantages. They adapt to volatile AI workloads. They have an ultra low emissions profile that usually allows for faster and simpler permitting and their cost effective too. That's why leaders from across the industry trust Bloom to power their data centers. Ready to power your AI future? Visit bloom energy.com to learn more. The grid wasn't built for what's coming next. Electricity demand is surging from data centers to EVs and utilities. They need reliable, affordable solutions that don't require building expensive new infrastructure. Energy Hub helps by turning everyday devices like smart thermostats, EVs, home batteries and more into virtual power plants. These flexible energy resources respond in near real time to grid needs, balancing supply and demand. Plus they can be deployed in under a year and at 40 to 60 percent lower cost than traditional infrastructure. That means more reliability, lower costs, cleaner energy. You can't get much better than that. And that is why over 120 utilities across North America trust Energy Hub to manage more than 1.8 million devices. Learn more at energyhub.com or click the link in the show notes. Do you ever wonder why it takes so long to get clean energy projects up and running? Do you have permitting reform on the brain? Are you NEPA reform curious? The new pace study from Third Way pinpoints the non-cost barriers that stand in the way of clean energy deployment and keep new solar and transmission projects in limbo. They surveyed more than 200 industry professionals to understand what's slowing down deployment and offer practical solutions on how to fix it. To read Third Way's full report and to learn more about the pace initiative visit thirdway.org slash pace. All right. There's another grouping of actors here where my first instinct to say is that it actually is straightforward with their incentives are which would be let's just say OEMs of hardware to data centers. That could be your selling equipment inside the data center or maybe not even hardware. Let's include like EPCs here for example as well. Like your job is to provide services or hardware into the build of data centers. My first thought would be everybody there is aligned in their incentive to promote the power scarcity narrative. It's just it just furthers their the argument that there is limitless demand for their services and we're just going to sell out as much as we possibly can. Do you think it's more complex than that? I think that one is pretty straightforward to your point is that just going back to our prior point about long lead time items and the most constrained a lot of this equipment as well as the labor. Those are the direct beneficiaries. What do they get? They get higher backlogs. They get longer duration backlogs. They get better pricing in typically industries where you don't get you basically get middle pricing or as much as the market will bear. Whereas now you have pricing power for the first time and in generations for certain equipment providers or certain labor producers. So this one is pretty straightforward and you see that across the earnings. You can look at the earnings transcripts of companies like Quanta, like record backlog, MossTech talking about record activity in their pipeline business as well as really a strong even clean energy pipeline. You can go to equipment or you see something like a verdive which has a backlog that was up 30% organically over year. You have Eaton which is up 20% backlog organically over year on billion dollar books of business. I think there that's what you'll see to your point like a very limited narrative on talking down the potential market opportunity because if you think about it from their point of view, it's just more business to your point. Equipment as you are power scarce, you need the latest and greatest to be the most power efficient per square foot or per IT capacity per megawatt of IT capacity. You always want the latest and greatest and often the highest priced equipment as well as you want to continually get the latest generation which requires you to size up the low-duty equipment. In terms of the labor, if labor is the most constrained element, that means that you'll agree to perhaps a higher degree of pricing that you normally would where if conditions were scarce, you may be forced to just take whatever conditions that your buyers are willing to take. For example, electricians and plumbers, we're incredibly scarce there right now. There's market chatter of companies like the hyperscalers flying out electricians from various parts of the country because they don't have enough labor. You can imagine for the CPC companies, pricing power is great because they're the only ones that have labor and can actually go out and deploy these projects. In an environment where that doesn't look as favorable, you have to agree to tighter terms, you have less overall projects to work on, you have more than enough supply of labor folks. That means that your market will slow down and your pricing slowed down so that is a vicious down cycle in terms of what happens on the other side of a growth cycle. This bucket, generally, I agree with you straightforward, talk up the business opportunity. It generally is positive and supportive for their businesses. Let's talk about another one that I think of as being a little bit more complex then, which is utilities. On one hand, utilities are in the same position as everybody else we've discussed, which is basically like, look, if there's a huge bottleneck in power, clearly demand for their services, in this case, electrons is going to be through the roof. They're going to be able to spend more capex. They should get higher earnings. This should be good for utilities. Of course, they would want to talk up the power scare city narrative. On the other hand, folks sometimes blame the utilities for the power scare city challenge. Who is there to solve it but them to some extent? There's probably some limit to the extent to which they can or place the limitation of AI growth on power scarcity. I wonder how you think about them and what you've been hearing from them when they talk in public market contexts. Definitely. I think this is one of the positions that is not enviable for a lot of different market players, especially those in the other buckets. You bring up a great point where on the one side, generational capex cycle, I can get a guaranteed ROE on my investments and I can put through more investments. The downside is we are increasingly seeing the political issues around rising electricity rates as well as folks being against having data centers in their local environment. On the positive side, it's like a potential business opportunity on the negative side. I'm getting increasing amounts of pushback. Ultimately, utilities are regulated. They need to answer to a public utility commission and get the infrastructure they need approved. Now, they're in this interesting predicament where they have to solve for all these limitations. Bring on as much infrastructure as possible but also do it at the lowest cost or at least the most efficient use of dollars such that they can't be blamed for inefficiently spending capital or artificially inflating demand. I think utilities were the last few quarters, even per se, the last one or two years. It was a lot more talking about the gigawatt pipeline and how much they're growing. You have folks throwing out tens of gigawatts in their pipeline or hundreds of gigawatts. Now, it's talking a little bit more about how can we bring these gigawatts on in a very effective way and ensuring that they have the labor, they have the equipment, the ability to execute on this quickly. Some of these are also in regions where they're impacted by regulatory impacts such as Texas with SB6 or PJM just actually went through and they couldn't get to a resolution on what's the approach to large load tariffs. But you're right. Utilities are conflicted in the sense that they don't want to miss out on the opportunity right because if they can't bring on the power or they aren't permitted to bring on the power, someone will take their load elsewhere to a different utility. They're in the business of problem solving right now. Whereas, it will last one to two years, talk about the opportunity. Now, it's in the realm of can I do this effectively and cost efficiently. How do you think about the, there's another category which is like the all group together, like real estate owners, landowners, and then the like powered land developers, the developers who are going off and trying to find sites and make them powered and then sell them to a colo or a hyperscaler or whoever. What's their incentive? Their incentive is to talk up the constraints as well because it increases the value of the yes to their owning, which is the real estate or the powered land bank. If we think about solving again, if there's a constraint, that means that anything that accelerates you through the constraint is valuable, right? Or it increases in value as a constraint gets worse or it remains. And so if I have a powered land bank, that means that I can move a lot faster than someone that has just a plot of land that has no interconnection access or anything like that, right? So I can charge a more premium price to developers or new clouds, etc. Anyone else that would want to be further along up the development cycle. And so they're generally universally talking up the opportunity that they have and the ability to move faster. I'd say a lot of what we're seeing there is at least what we hear, chatter wise, right? Is that like you have transaction values that are much higher than they've ever been, or at least in recent cycles, as folks all are chasing the similar opportunity. So pretty much any land banks that are powered that are near plentiful energy sources, something like in the Northeast with narrow gas fields or in West Texas, right? You're seeing construction activity be plentiful because people think that it's a much faster path to value realization. So you're seeing that increasingly move up. I think like one tall tell signal or maybe like a leading indicator that could like start to suggest weaknesses if you start to see real estate offerings that are powered that are near tier one cities or have really good access to energy infrastructure or fiber infrastructure that don't get a premium value or are struggling to sell in the market. That would be an interesting indicator that perhaps the market's not as strong as possible. But at least right now, the market indicators would suggest that value is considerable because people are still focused on time to power right now. All right, I want to move on to the chip players. They're an interesting one here. Like, if you think about Nvidia's incentives, on one hand, they share the incentives that most of these other folks do of like, let's play up the power constraint in the hope that it gets solved. On the other hand, they want to be making the argument that we are just going to sell effectively infinite chips forever. Like, we're just going to sell so, so, so, so many more GPUs. And if ours are real constraint on that, that's the limiting factor and it's kind of outside their control, then one would think that would affect their perceived value and their perceived growth, I would say. But my sense is that Nvidia is talking kind of like the hyperscalers and really the story they're telling is, this is a major constraint. I mean, possibly just because it's true, but maybe that's their incentive as well. Yes. And I think they actually, if again, this is all very hypothetical game theory, but if you are in the video, right, like, you are pitching that you are getting the best performance token per watt is, you know, what Jensen will call it. And basically arguing that they can extract the most tokens per watt useful energy. So they're giving you the lowest cost of efficient dollar to the highest level of performance. And so that actually gives them pricing power, right? Because since we are power constrained and that constraint is expected to get worse and power prices rise, the Nvidia chip holders actually, the TCO advantage that they have versus less efficient chips actually goes up in that scenario, right? So they actually continue to do better as power remains constrained because it reinforces their pricing power. And so this introduces like the recent, like, you know, the interesting angle where like custom silicon, for example, for those, you know, TPUs or all the raises week, even though they've been in existence for a while, there's arguments that you can find more purposeful chips for certain workloads that require less energy draws. And if, you know, you weren't as constrained on energy, you might be, or you might be willing to use some of those chips, that actually erodes your pricing power for an Nvidia, right? Because the idea is we're power constrained. I want to get the maximum amount of knowledge for every dollar I spend. And I can do that right now with Nvidia chips that I might not be able to do with custom silicon chips. And so it actually reinforces their competitive advantage. And so they have a kind of an interesting angle here where to your point, they're going to sell more and more chips and you need power to bring online those chips. But they also probably want the market to remain tight because the longer it's tight, the better their pricing power actually ends up being. So on balance, basically, everybody we've talked about so far, you know, sometimes it's nuanced, but pretty much everybody is incentivized to play up the power bottleneck. And indeed, you know, they are. It is also real, but they are playing it up. Who do you think is on the other side of this equation? Like who is incentivized to say that power is not such a big bottleneck? Yeah, I think this part's a little bit more interesting too, because to your point, I feel like a lot of people benefit if the power constraint remains. And you know, that's just like a okay thing to observe is just facts or facts. But on the other side, you know, I called a few buckets and the first thing I think is independent power producers. And this might seem counterintuitive, but let me lay out my logic is if you're an independent power producer, that means that you have a fleet of existing assets today, those generate money based on selling into electricity markets. And so, you know, the input to their revenue is electricity prices. If we're in scarce conditions and those continue to grow, they generate a really healthy incremental margin on that, right? Because they're not really building out new assets. You have slightly higher O&M perhaps by running your assets harder, but you're just flowing through a high electricity price that is extremely profitable for them. And that's actually been the thesis for a lot of owning these assets. So these are like the constellations, the talons, the energies of the world. One thing I noticed, it's interesting, if you look at the last two earnings calls, right, you have the folks like the CEO of Constellation talking about the fact that, you know, energy prices actually aren't supportive of building new gas plants, which may or may not be true. And that, you know, there's operar, like, there's different items that we have or levers to pull like flexibility or, you know, things of like tile nurses suggestion of, you know, can, you know, throttling workloads when you're at peak conditions to enable more excess capacity to be built and used today. I think that's interesting that the IPPs are talking out of that, because like, if you're game theory it out, it basically is suggesting that perhaps they don't want a lot of new supply to enter the market, right? Because what does that do? That potentially dampens prices. And they, I just laid out a path where they can generate a lot more healthy or e-bidders, if they can, you know, sell into higher or appreciating power price markets. And so I think that's someone that is probably trying to downplay the impact just because they're value in the marketplace goes up as long as it's constraint, you know, remains. And then, you know, another area that I think is a little bit more interesting too. And it's almost a derivative of how the market works, but like LNG providers, for example, a lot of them just sign very like long-term, you know, contracts. And then the basic arbitrage in US LNG is you buy cheap domestic gas and then you sell it to other markets like Asia or Europe to be able to, you know, sell into premium markets. Their model actually gets a little bit in trouble here or the ARB gets less interesting if domestic gas prices rise, right? Because you go from taking a cheap input to a rapidly inflating input that's maybe no longer cheap and that kills your ARB on the other side too. So they're probably incentivized as well to talk down some of the power issues to ensure that, you know, the supply goes to them versus going to a local market that they can be served much more profitably as well. So that's like two areas at least that came into mind in terms of the other side of the trade where, you know, perhaps trying to talk down the power constraint issue so that new supply doesn't enter the market and you can't continue to, you know, raise your prices. That also, you know, another area would be like, just think about that. It's not allowed now is the natural gas EMP is also as well, right? If everyone's just chasing this glut, then you probably bring on more production that actually keeps a lid on your prices when you want prices to be rising. So I think the way to think about this one is, who benefits from a rising input price to their business models? And they're probably in the camp of talking down expectations. Right. That's kind of where, I guess, of those three groups, I think the IPPs in the natural gas EMP companies are kind of aligned, right? They both benefit from high prices of their commodity. So they don't want a ton of new capacity to get built. That's just good for them. The LNG folks are on the other side of that trade, right? They're buying the natural gas that gets produced and selling into another market. So they don't, they don't want prices to rise. They do want more production. They want as much drilling as possible, right? And so then the R gets bigger. So for them, again, this is complicated theory, but like, but you know, with the LNG players, you'd think they'd want to say, oh, it's a, you know, not, they should be telling you that we're talking about the power bottle. That we're, it's actually a natural gas bottle, like we need to be drilling so, so much more. And if that happens, then it keeps their prices low and keeps their R big. So that's a really interesting point though, right? Because like, yes, they want natural gas production to be high because that means low. But they also don't want local sources of demand to accelerate because that means you have a lot more willing buyers in this case, gas power plants, right? So if a bunch of CCGTs go up, then all of a sudden you have a local buyer that's willing to pay a premium price that raises the input price for them. They want to buy cheap, nat gas, but all of a sudden, if you have local demand that goes, you know, incredibly higher, you're competing with another source and, you know, producer might be able to go to somewhere else versus you and that impacts your ability to serve your contracts on a longer term time horizon. So it is, it is complicated. As you start to peel the onions, the layers, the onion back, you start to see all this, like, you know, there is general incentives like for and against it within all these pockets, but it is just like, it's hard to parse out without going through these exercises and say like, who benefits in what market? And that's the, the rule or the role that investors play, right? As we're generally trying to underwrite the range of scenarios, good, bad, neutral, and then probability weight, what's most likely to happen. Maybe taking a step back, I guess, just to wrap it up, it seems clear to me that this, this narrative, this general narrative of the power bottleneck has taken hold substantially over the past. I don't know, you tell me, but 12, 18 months, something like that is when it's really taken hold. How would you describe the sentiment around that narrative today? And like, have you seen it changing at all? Is it is the fervor growing right now? Like, where are we in the trajectory of people talking about this problem? Yeah, at characterizer at a high level, the last one or two years was really just getting our heads around the issue. What's the source of this demand? How strong is it? And that's largely driven by GPUs and say, all right, we need the latest and greatest to train our models, scaling loss hold. That means we can justify more investment and we'll continue to, you know, perpetuate the cycle. I think what it turned into recently now is folks are getting into that ROIC question is like, all the dollars that we're spending, you know, which is on the order of half a trillion dollars now a year, is that generating positive ROIC? And that question mandates, like, are they generating revenue and dollars from it? And so that moves you upstream to, okay, like, our folks actually doing that. And then that leads to the area where there's the question of, if I had more supply, could I generate more dollars? And would the ROIC on each of those be positive? And that's the part that we're in right now is what is the real constraint here? If you talk to a lot of the, you know, providers that sell AI solutions, it's because we don't have enough power shells or power to get more supply online, and I'd be able to generate more dollars. But I think that's like the real question is like, if said another way, if we had unlimited power, unlimited constraints now that were all solved, would the AI trajectory still be as strong as it is today? And that's a question that we're getting into is like, which constraints are going to be more than a $1,000? And so, I think that's the point that we're getting into these are solvable. And if those constraints are relieved, does the cycle, you know, potentially a positive for that player or negative? And I think right now as folks are trying to answer that question, and that's why you see kind of the gyration that you do in the market, every other day, right? Where, for example, a lot of the AI power trade or AI plumbing trade, wherever you want to call it for the physical infrastructure has done tremendously well for earlier, like the, the EMP, sorry, the EPCs, the MEP subcontractors, the networking and optical connectivity equipment, the switch gears, the circuit breakers, the transformers, the chillers, all these people have benefited. And so now it's just about the durability of those constraints, because that's ultimately what we're trading on now is like the two, three, four years outward. Can they actually grow into what the valuations have grown to? All right, Johnny, this was fun. Thank you for helping me think through all the complicated incentives that are out there, mainly to just talk about how big a problem power is. But we'll have you back on when things take a turn, as they inevitably will. Yeah, it'll be a fun to try, tag along, and we'll see what happens here. And the narratives are always constantly changing, and the market's giving new opportunities to new narratives. So let's circle back here in a year or two and see where we landed. Sean, a Matthew is a senior vice president and portfolio manager for US Sustainable Equity at Lazard. This shows a production of latitude media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Marquand. Steven Lacey is our executive editor. I'm Shale Khan, and this is Catalyst.",
    "release_date": "2025-12-04",
    "duration_ms": 2096000,
    "url": "https://traffic.megaphone.fm/PSMI6605706669.mp3?updated=1764805694",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-12-24T01:57:41.803586"
  }
]