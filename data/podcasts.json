[
  {
    "title": "Inside a $300 million bet on AI for physical R&D",
    "description": "A big problem with using artificial intelligence to discover new materials? It struggles to predict beyond its training data. That means AI might be better at optimizing known materials than discovering entirely new ones \u2014 like a room temperature superconductor or carbon-capture sorbents.\u00a0\n\nBut since we last covered the topic in September 2024, a few things have changed. OpenAI released its powerful O1 reasoning model. Large language models have also gotten better at math, physics, and coding. And lab automation \u2014 robots mixing liquids and powders, running characterization tests \u2014 has improved, allowing for a higher volume of experiments.\u00a0\n\nSo, can these improvements overcome AI\u2019s training data problem?\n\nIn this episode, Shayle talks to\u00a0Ekin Dogus Cubuk, cofounder of Periodic Labs, which\u00a0raised $300 million seed round\u00a0in September. Last year, Dogus took a more cautious view on using AI for materials discovery. Now though, he\u2019s convinced there\u2019s a clearer path forward for physical science research and development, especially materials discovery. Shayle and Dogus cover topics like:\n\nCreating experimental and synthetic data to overcome AI\u2019s limitations of predicting beyond its training set\n\nWhy we should focus on breakthrough discoveries over easier, incremental wins\n\nThe different roles humans and AI play in the discovery process\n\nPeriod\u2019s focus on automated experimental labs using AI-generated hypotheses\n\nResources:\n\nCatalyst: Can AI revolutionize materials discovery?\u00a0\u00a0\n\nLatitude Media: This \u2018superintelligence platform\u2019 just raised $200m in seed funding\u00a0\u00a0\n\nLatitude Media: Can AI get us closer to fusion?\u00a0\u00a0\n\nThe New York Times: Top A.I. Researchers Leave OpenAI, Google and Meta for New Start-Up\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.\n\nCatalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting BloomEnergy.com. \n\nCatalyst is supported by Third Way. Third Way\u2019s new PACE study surveyed over 200 clean energy professionals to pinpoint the non-cost barriers delaying clean energy deployment today and offers practical solutions to help get projects over the finish line. Read Third Way's full report, and learn more about their PACE initiative, at \u2060www.thirdway.org/pace\u2060.",
    "summary": "The podcast discusses the advancements in AI for materials discovery and the quest for breakthroughs like room temperature superconductors. Periodic Labs, founded by Doge Chubuk and Liam Fettis, is using AI, physics, and chemistry to push the boundaries of scientific discovery. By combining digital and physical labs, they aim to automate experiments and leverage reasoning models for breakthroughs. The focus on superconductors, particularly high temperature variants, showcases the potential impact on various industries. The challenges of reasoning into breakthrough discoveries and setting reward functions for AI models remain key considerations in their pursuit of innovative materials. The episode also touches on the importance of clean energy solutions like Bloom Energy and Energy Hub in the evolving energy landscape.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shail Khan and this is Catalyst. I have to say there's a difference between winning gold medals in Matholimpiauts and scientific discovery. Like you can practice for Matholimpiauts by studying previous years problems. You can't really practice how to discover the next big theory but they were getting better at reasoning on complex problems. Coming up, can AI discover a room temperature superconductor? Volume 2. The AI boom is here but the grid wasn't built for it. Bloom energy is helping the AI industry take charge. Bloom energy delivers affordable, always on ultra reliable on-site power. That's why chip makers, hyperscalers, and data center leaders are already using Bloom to power their operations with electricity that scales from megawatts to gigawatts. This isn't yesterday's fuel cell. This is on-site power built to deliver at AI speed. To learn more about how fuel cells are powering the AI era, visit bloomenergy.com or click the link in the show notes. Surging electricity demand is testing the limits of the grid but Energy Hub is helping utilities stay ahead. Energy Hub's platform transforms millions of connected devices like thermostats, EVs, batteries, and more into flexible energy resources. That means more reliability, lower costs, and cleaner power without new infrastructure. Energy Hub partners with over 120 utilities nationwide to build virtual power plants that scale. And how the industry's leading flexibility provider is shaping the future of the grid, visit energy hub.com. Clean energy is under attack and it's more important than ever to understand why projects fail and how to get them back on track. The center left think tank third way surveyed over 200 clean energy professionals to answer that exact question. Their newest study identifies the top non-cost barriers to getting projects over the finish line, from permitting challenges to nimbism, to read third ways full report, and to learn more about their pace initiative, visit thirdway.org slash pace, or click the link in the show notes. I'm Shell Khan. I lead the early stage venture strategy and energy impact partners. Welcome. So, a year ago, a little over a year ago, I had Doge Chubuk on this podcast to talk about using AI for materials discovery, which has all sorts of interesting applications in the spaces that we talk about here. At that time, Doge had been leading efforts in that area for Google DeepMind for some time, and I thought of him as being both very knowledgeable in the space, obviously, also pretty sober about it. Fast forward a year, Doge left Google DeepMind earlier this year, and along with Liam Fettis, who was one of the co-creators of ChachiPT, started a company called Periodic Labs, which raised, wait for it, a $300 million seed round, led by Andreessen Horowitz. Nick is doing AI for materials discovery, and not just that, also physics and chemistry, and they're also very much hardware in the loop. The way I like to frame it is that they are building two kinds of Frontier lab at once. There's a Frontier AI lab and a Frontier Scientific lab, the type of lab that we used to talk about, and then they're trying to make those two things work together to make breakthrough discoveries. Notably, one thing we talked about last time was how the AI materials discovery companies at the time tended to start by going after often discovery of something like metal organic frameworks or moths for carbon capture, which I think of as less of a breakthrough opportunity, really, from a global scale. Whereas the big, perhaps biggest breakthrough to prove would be the discovery of a room temperature superconductor. Well, Periodic makes no promises, but they're very publicly working on high temperature and maybe room temperature superconductors. Based on that last conversation, to be honest, I wouldn't have predicted this. So it was time to have Doge back on and hear what changed and how. Here's Doge. Doge, welcome back. It's great to be back. It's great to see you. A lot has changed since the last time we talked. So I looked back. So the last conversation that we had was just over a year ago, September 2024, and I was having you explain to me the wild world of AI for materials discovery in particular, and work that you'd been doing Google DeepMind, but also just like the broader landscape. And I'll tell you my takeaway from that conversation, which you can tell me if I had the wrong takeaway at the time, but my takeaway was promising field, pretty unclear if and when this new wave of LLMs and the reinforcement learning, all the things that have shown up in the past few years, pretty unclear if and when that would generate a real meaningful breakthrough discovery in materials. And we talked through a bunch of the reasons why it's challenging training data, maybe chief amongst them, but I came away with a pretty, I think like a sober view of the path there. Okay, so fast forward a year and you left Google DeepMind started a company to do that amongst other things. So I guess the first question that I have for you is what changed in the last 12 months to make you, to give you conviction that like now is the time. Great question. So when we talked, I was doing research in the field of computational material science and machine learning. You know, specifically we were using graph neural networks, we were using density functional theory, and we were trying to discover materials. One thing that changed since our discussion was the LLMs have improved even further. So at the time I wasn't using LLMs much at all, but I think right around when we're talking the O1 came out, right, the reasoning models started showing up. And that was a huge update for me because you might remember that one of my big concerns is machine learning works best on the training set distribution, but in science and technology we almost only care about outdoor domain generalization. So what O1 showed is if you spend test time compute, you can get better results. So that was very exciting to me because there was one way of investing resources that was beyond the training set. So okay, if I can try to translate that into layperson terms, the reasoning model like OpenAI is a one model, introduced, unlocked a door kind of, that maybe allows you to break this challenge of the limited training data set that you have in materials discovery. That was what we spent a lot of time talking about a year ago, was like, you know, you can compare the corpus of data that an LLM trains on to do language, which is enormous. It trains on the internet, basically, versus the corpus of data that you were dealing with in trying to discover novel materials. And it was thousands of data points, not tens of billions or whatever. And so that presumably hasn't changed at least yet. But you're saying that the reasoning models have gotten good enough that they are able to sort of get around that challenge via reasoning or possibly generating their own synthetic data. Like what is it that allows them to break that? So I'm not saying that they're good enough already, but that was one step in the positive direction. And another thing, you know, we've seen is they've gotten really good at math. So, you know, since last time we talked, they started winning gold medals in metal impiades. And they're doing similarly well on coding, really well on physics for impiades. And I have to say, there's a difference between those things and scientific discovery. Like you can practice for metal impiades by studying previous years problems. You can't really practice how to discover the next big theory. But it did show you that they were getting better at reasoning on complex problems. So then what else do we need? So I think the biggest thing we need is to have our own lab. Because once you have a very intelligent reasoning LLM, you still can't discover things unless you make trials, right? Like just like humans, the LLMs will be wrong often when they try to predict things outside of their training set. But you try many things. And then at some point, you get a really cool discovery. And this is, you know, as we talked about history last time, this is quite common in solid state chemistry, solid state physics, where a lot of discoveries happen somewhat by accident, but of course with the Lado background, understanding of the physical system and a lot of trial and error. So okay, so this is what you're doing at periodic, right? You're combining the digital domain with the physical domain. You have a lab in both sense. It's a frontier lab and an LLM sense and a frontier lab in a laboratory sense and the traditional sense of the word. And you're sort of merging the two. I'm curious in practice, like how you imagine that feedback loop working. So is it a traditional, you develop a theory, you run an experiment, you generate data from that experiment, but in this case, you feed the experiment back into your customized LLM as an additional set of training data. And then that's the way that the loop works, or is it more complicated than that? Yeah, exactly. I mean, it's pretty simple, I think as you said. So the LLM can propose, for example, synthesis recipes, or it can propose simulations to run. And because the LLMs are pretty good at tool use, it can actually do it itself. And then you get some results back. So the results from experiment could be some characterization data, results from the simulation can be some trace or some simulation you did. And now the LLM can go through it with the context of its previous training, maybe the context of relevant papers, textbooks, but also now the results that it just got that no one else has ever seen. And then now it can kind of tweak the experiment, tweak the simulation for the next step. Right, you said one thing in there that I guess is worth pointing out, like you're trying to automate this as much as possible. The LLM might run the experiment. Yeah, absolutely. I mean, one of the other advances that's been happening recently that I think made periodic possible is the high throughput experiments have been getting better. There are many examples of this now across academia industry where these robots that became quite commoditized actually, just mixing powders or mixing liquids and then sending into characterization. I think one thing that isn't as advanced right now, but we feel like we can do pretty soon is automated characterization itself. So you mix powders, you put it in some characterization tool, you get the result out. What is the actual output? I think this is pretty difficult right now for AI tools, but we feel like we can improve it pretty quickly. I want to talk about one specific application that I know you're going after that we actually did talk about a year ago, but also I want to talk about it as a way to see whether one of the other things you described as the fundamental challenges has changed, which was, as I understood it, AI being pretty good at the next incremental discovery, but not necessarily good at the breakthrough discoveries you said through history. Usually that's done accidentally or often it's done accidentally because it's not, you can't like reason your way to this massive breakthrough discovery. So let's talk about superconductive materials. We talked about this last time where all these companies that existed at the time that we're doing AI for materials discovery were starting on things like discovering a novel mop for carbon capture or whatever. But we said the thing that would be the real breakthrough, the big thing would be a room temperature superconductor. You guys have since been launching been very public about superconductive materials, maybe not room temperature. I'm curious for you to tell me how likely you think that is, but high temperature superconductors is on the roadmap. So why, first of all, and then second of all, like this question of do you think you have a path to the truly breakthrough, what would the path be to a truly breakthrough discovery as opposed to finding something that is a material that is superconductive at a ever so slightly higher temperature than the best that we've got today? Yeah. So to answer your first question, I think it's still true that it would be difficult to just reason your way into a much better superconductors. I actually would guess that there's a law out there that we haven't discovered yet that says that you can't just look at your training set this different time we're trying to discover and just predict it. There's been rules that we discovered from 1800 zone where you connect energy to work. So thermodynamics is the first example. There's more recently Landauer's limit which shows that you have to spend a certain amount of energy to delete information which can be used to describe Maxwell's demon contradiction. I bet there's something similar for how hard it is to discover things. It's outside of your training domain. Okay. So I don't think that's been fixed since last time we talked. But because we have a lab internally, we can just try things and try them at large scale and often and hopefully as intelligently as possible. So even though we won't reason our way into a much better superconductor, we'll be able to push our trials in the direction that's most promising or most promising for us given our training set at the time. So yeah, I think that hasn't changed. And I think there's reason to be hopeful because in the big scheme more things is a pretty new field. I mean, if you look at Cooperates, they were from 1985. There's been a lot of advances more recently. So yeah, we're very excited. One reason we chose superconductivity is if you find a good superconductor, that's impactful immediately. Right? Like last time we talked about how long it can take to translate materials and prunes into products. One nice thing about superconductors is if somebody discovers a room temperature superconductor today, even before it makes it into a product, it has huge impact. Like first of all, it changes how we think about the universe. Second, it helps us do physics experiments that wasn't possible before. And whenever you think about a sci-fi-ish technology like quantum computing, fusion, superconductors come up because that's kind of what we need. It's kind of like one of the most exciting macro-scale quantum phenomena. So this one isn't a big picture because it kind of is exciting as soon as we succeed. The other reason is it requires all sorts of improvements to get there. You know, when we think about open AI and deep mind, I remember back in 2016, people used to make fun of these institutions for prioritizing AGI so much because they were saying we're going to do AGI. But what happened is they developed so many other tools on the way to AGI, they were useful in themselves. But today they have these alarms that you might consider AGI or like something really impressive. Superconductivity is a bit like that. To discover an exciting superconductor, we probably have to develop so many capabilities on the way there. That's by themselves very useful. For example, automated synthesis, automated characterization, being able to model or predict high temperatures for conductivity because we don't have a theory for it yet. So it's kind of like a nice goal that unites people and requires a lot of other useful things to happen on the way. And it's one of those things that physicists find really exciting. So the physicists in our company are really excited by this mission, but also computer scientists find it very exciting. It's just one of those things that I think both sides can really appreciate. So those are some of the reasons that we picked it. We've heard the phrase speed to power a lot lately, but here's what it really means. AI data centers are being told that it will take years to get grid power. They can't afford to wait, so they're turning to on-site power solutions like Bloom Energy. Bloom can deliver clean, ultra reliable, affordable power that's always on and as little as 90 days. Bloom's fuel cells offer data centers other important advantages. They adapt to volatile AI workloads. They have an ultra low emissions profile that usually allows for faster and simpler permitting and their cost effective too. That's why leaders from across the industry trust Bloom to power their data centers. Ready to power your AI future? Visit bloom energy.com to learn more. The grid wasn't built for what's coming next. Electricity demand is surging from data centers to EVs and utilities. They need reliable, affordable solutions that don't require building expensive new infrastructure. Energy Hub helps by turning everyday devices like smart thermostats, EVs, home batteries and more into virtual power plants. These flexible energy resources respond near real time to grid needs, balancing supply and demand. Plus, they can be deployed in under a year and at 40 to 60 percent lower cost than traditional infrastructure. That means more reliability, lower costs, cleaner energy, you can't get much better than that. That is why over 120 utilities across North America trust Energy Hub to manage more than 1.8 million devices. Learn more at energyhub.com or click the link in the show notes. Do you ever wonder why it takes so long to get clean energy projects up and running? Do you have permitting reform on the brain? Are you NEPA reform curious? The new paste study from Third Way pinpoints the non-cost barriers that stand in the way of clean energy deployment and keep new solar and transmission projects in limbo. They surveyed more than 200 industry professionals to understand what's slowing down deployment and offer practical solutions on how to fix it. To read Third Way's full report and to learn more about the PACE initiative, visit thirdway.org slash PACE. I guess back to this question of how do you distinguish between the incremental innovation, which to be clear, if you develop or discover a superconductor at a higher temperature than anything that we've got today, that's meaningful. But it's probably orders of magnitude less meaningful than if you discover a room temperature superconductor. I presume that the scientific challenge is commensurately distinct between those two. The way that LLM's work, as I understand it, at least in part is on these reward functions. Are you setting your AI system a goal of finding a room temperature superconductor? Then everything flows back from there, here are the steps and all the things we have to fix to get to room temperature. Or do you say, improve this characteristic such that we can incrementally build our way there? In other words, are you going to find 10 superconds? I think of it as a different thing from, but the alternative version of this is what happens in nuclear fusion, where everybody is chasing this same goal of Q is greater than one, like energy break even. Everybody is getting incrementally closer and closer and closer, and eventually, NIF breaks it or somebody breaks it. Is it going to look like that or is it going to look like we've discovered nothing until we discover the room temperature superconductor? As you said, I think there are many different ways of improving superconductors without getting a room temperature superconductor. One of them could be having a significant increase in TC, but another one could be a really high critical magnetic field, which turns out might even be more important for fusion applications than TC itself. Another one can be more mundane, like some mechanical properties. Like a superconductor that also is ductile and we can make it into devices. So we wouldn't rule out all of these very exciting developments just for a room temperature TC. But how do you set the reward function for your model? What are you optimizing it for then? I think that's an empirical question. I think one thing I should say is it's quite nice because it's hard to reward hack. And with these issues with RL and training all the answers, you might worry about reward hacking. And in simulations, again, reward hacking can be a problem, even in DFT. But for real life experimental measurement of TC, it's much harder to reward hack, which we love. So if our reward was increasing TC, that just seems like a nicer unhackable reward. But in terms of what specifically will get us there, we're not sure yet. I mean, it's an empirical question. We can probably try all of them. I'll list the things you propose and we'll try all of them. I guess that gets to the other question, which is like, what does the human in the loop look like here? Right? And again, as you said, like if we haven't solved the sort of AI is good at incremental innovation and not orthogonal breakthrough innovation thing, the humans are historically, at least better at it. Is it like, folks on your team developing a theory of something and that gets fed through the model and you get the results and you feed it back in, you see whether it's a promising category, like is the germ of the original idea of what to look for coming from a human? Or is it coming predominantly from the model and then the humans have to interpret and send it off in various directions? Yeah, I mean, that's a great question. We're not really prioritizing full automation anyway, so if we get better results with humans doing part of it, that's great. This is also actually a question for a lab, right? Like, do we want to automate every single aspect of the lab? At some point, you end up needing human noise for that. And I think that's not like Liam, my co-founder and I, we are trying to be very pragmatic about it. And I think it's just to get the best result possible on the things we care about. And you know, how much of the automation comes from the ML, how much of it comes from more traditional tools and how much of it gets done by humans, I think that's kind of, again, an empirical question. So yeah, we're not, like, I think as you said, it does seem like today, there are things that ML, AI is better than humans. But one of those things is not hypothesis generation. So I mean, there are two options that we either have to improve this all-end-on-hypothes generation, which is possible. Or the other option is we have humans providing some other hypothesis and then AI doing the execution. I guess the other question here is cost. I mean, you guys raised a $300 million seed round. So that implies on the outside that your cost structure will look similar to other companies that basically have to, are going to use just an enormous amount of compute. And so like a lot of that cost comes from compute. In your context, I can imagine maybe that being true, but also maybe that not being true because you, again, you just don't have the same corpus of data. You can't build a 10 billion parameter model right now because the data isn't there to do it. And so instead that cost is going to go more toward the robotic lab and all that kind of stuff. Like how should I be thinking about how much compute you'll use and where that cost come from? So honestly, compute is very expensive. And we are going to train LLMs. We are going to use GPUs to run simulations. So that does end up being a large part of the cost. Yeah, it's funny. Before, if you ask me this question 10 years ago, I would have thought that the biggest part of the cost must come from the lab because like physical, it's real. You're building this lab. You're buying instruments. But turns out the GPUs are so expensive and training LLMs is so expensive. So when we were thinking about how much to raise, we kind of laid it out in terms of the GPU cost, the lab cost. And this was kind of minimum number we felt like was viable. And yeah, we'll see the GPUs have been getting more expensive recently. I guess we'll see how the market dynamics continue. To what extent do you end up building generalized model or models versus models designed to a specific domain, even a specific scientific domain, right? Like you guys are doing your material discovery obviously, but physics and chemistry and these things all intertwined. But like, is this same model going to be equally capable across all domains? Is that the intent? Or is that just not how they're supposed to be architected? That's right. And this actually something we're very excited about. You know, one thing I've been kind of noticing is like in the past, say, three, four years, I had a chance to collaborate with very, you know, world class best in their field scientists. And even when you work with them, you realize that while their expertise on a few domains is, you know, incredible, maybe best in history, there's just so much more to know in chemistry and physics that they may not know all the other aspects of it. So this is why I brought up superconductivity because you might actually need to be really good at, you know, self state chemistry and synthesis of difficult novel materials just because, you know, you don't know which chemistry the superconductors going to come from. And what these ideas you might have may not be as stable thermodynamically. So you need to be intelligent about how to kinetically force it into the phase you want. But at the same time, in addition to self state chemistry, you need to be incredible at condensed matter physics, right? Because like, there's so many different kinds of superconductivity. We don't understand most of them very well. And there's nobody in the world who knows both of those equally well or sufficiently well. Turns out this is true for many different aspects. Like if you need to use robots for high triple synthesis, again, like there are only so many people who understand robots and how to use them for synthesis. So I think this was different in 1800s probably. There was probably a time when a physicist could contribute and be one of the best in the world on many fields of physics. But it's definitely not true today. And this is one of the reasons I think we are very excited about LLMs because when you talk to them, they seem like they have a pretty good understanding of self state chemistry and self state physics at the same time already. And we're trying to improve them further in the physical sciences specifically because that's where we are really interested in. And then we're hoping that there will be a good multiple of these. And then a really exciting project with that is a lot of the exciting discoveries happen to lie in between fields, right? That's why it's sometimes easier to be interdisciplinary. And there's so many of these surface areas between these different fields. It's like, I guess science is kind of like a fractal in the way it's hierarchically organized. And there's so much surface area that humans have exploited, of course. But then there's probably so much left to exploit. And we're excited about an LLM that can basically do that. It is scale that humans couldn't yet. How good are the LLMs today or are the best in class of what you guys have at generating synthetic data in this domain? And another way to ask this question is, if you've asked forward three years, you're fully up and running and you're operating, how much of the valuable insight you will generate, do you think will come from the physical data coming out of your lab versus the synthetic data that the LLMs create on top of that? Yeah, that's a great question. I obviously don't know the answer, but it's great to brainstorm about that. Because on the one hand, the lab data will be kind of our additional data that other LLMs may not have. And you might think then the lab data will only be as valuable as the results in it. But on the other hand, what's interesting about scientific data is it's not just a few bits or numbers. For example, there are certain experiments you can run where the result you get from it is just, say, three floating point numbers. But the implications of those could be tremendous. It's not just going to be a few bytes. It will actually be potentially an incredible amount of understanding just from a few experiments. And this has been how it is in human history. There are certain experiments that told us so much about how we understand about the universe. And the way to do this with synthetic data can, of course, be you run simulations that relate to the experiment. And when you get the experimental result, that actually validates or refute so much of the simulations you ran. And then that is a lot of information in itself. So it's a very interesting question. And I think there are some actually differences about how you think about synthetic data when it comes to LLM, this good science. And exactly, I mean, this is one of the reasons I don't want to work on this because this opens up questions for LLM's and LLM training that's maybe different than what the frontier labs are thinking about right now. If they're only thinking about math and logic and kind of what's on the internet, like accounting tasks, that's a bit different than if you're trying to do experimental physics, experimental chemistry. It seems like a very exciting question to explore. I want to talk a little bit about how you build a business out of this. I mean, you mentioned the superconductor example, and you said, like, there's a lot of value in this long before this novel superconductor goes into a product. But ultimately, kind of has to go to a product of some sort for you guys. And I think we talked about this a little bit last time too. There's this question of, okay, so if your job, your core job, the periodic is to discover new things that are going to be valuable in the world. Say you do it. To my mind, there's sort of a binary decision you have to make at that point. Do you try to sell the discovery, license the technology, license the IP to somebody else who's going to go produce it and turn it into a product? Or do you produce it? Do you sell the product? Do you sell the tech, the discovery or do you sell the product? Do you have a prior on which direction you want to go here? Great question. I think the two options can be correct depending on the context, depending on the timelines. But honestly, it also depends on where we are in the company. So at the very beginning, you can imagine our allowance will be very impactful for other companies doing physical R&D. Like, already they're in the community. Like contract with people. Yeah, exactly. I mean, there's a lot of interest in being able to use these LMLs. Sometimes the data restrictions don't allow it because you don't necessarily want to put your data on an LLM on the web. Sometimes the other issue is you haven't trained the LML on your data, so it's actually not as good as it could have been. That kind of improvement could be really impactful because we've seen how impactful LMLs can be in other fields where they have access to the data. So there's although I think headroom for impact there. But in the longer run, you can also imagine a case where we as a field get really good at designing materials intentionally. That hasn't been the case. But if you look at drug design, there was a time when designing drugs wasn't very profitable. And I think people were located and say, this is not a good business. But what happened with Genentech is the field got so good at designing drugs that at some point it became very valuable itself. The machine learning field has been making huge improvements in material science that was kind of hard to predict. So it'll be interesting to see how far that goes and whether materials discovery by itself becomes a very exciting business similar to drug discovery. But for us, we already see this big need and that big potential for impact by providing these LMLs to do physical identity. Yeah, almost like a, this is going to be the wrong analogy, but it's partially right. Like almost like an AWS, like you're going to have the infrastructure. In this case, the infrastructure is your custom designed LML that is smart about physics and chemistry and all these domains and also your physical lab and them being interconnected with each other. And so you have all this infrastructure and scale in that infrastructure that you can use to go convince whatever large company that's doing R&D that they should just be outsourcing it to you rather than rebuilding the whole same thing in house, which is not exactly what the cloud providers are. But there's enough of a relationship there. So that feels right, but it is, I suspect, yeah, I guess this is what you're saying. I suspect a smaller ultimate opportunity than the you proactively discover a bunch of novel materials that change the world. And then however you monetize them, you prove you're able to do so repeatedly. And then your Genentech and it's a whole different category. Yeah. And one other thing you see is people are so excited about this. They want to see LMLs not just conquer the digital world, but also really impact the physical world and impact the atoms basically. So I feel like this has to be done. And the team has been very excited. It's really amazing. We're hosting weekly seminars where the physicists will teach the computer scientists about the physics and the computer scientists will teach the physicists about LMLs. And of course, there are a lot of people in between, right? Like it's actually, again, like a fractal. So yeah, I think there's been a lot of excitement about seeing if these technologies can be used not just for the digital world, but also for constructing the atoms around us. I guess maybe the last question you said at the beginning, the thing that changed between a year ago and now in part was advancements in the big LMLs, right? The O1 model and so on. Is there a next like what could change in what could open AI release in a year or two years from now that would be a big leapfrog for you? Are you branching off now from what the big LMLs are going to do? And everything that all advancements are going to come from periodic? Or is there something else that they could offer that is a step function change in your capability to discover new materials or whatever? Yeah, great question. I think we actually basically rise with the tide, right? As LMLs get better, there's so many advantages of that to other applications. For example, the LMLs are getting very good at coding. And that's not surprising, because programming is a kind of closed environment. You can just simulate in your computer and get valuable feedback and then quickly improve. But as computers get better at coding, that's huge for science because then you can run simulations more efficiently. The simulations themselves can improve similarly with tool use experiments. So I think as LMLs improve in general, there's going to help a lot with science applications. There are maybe longer term things that can happen. One of them could be things like hypothesis generation or more auto domain generalization. But then a question there is, will that come from status quo, like how LMLs are being trained now, or will it come from actually labs that try to improve scientific reasoning for these elements? Because then maybe hypothesis generation emerges naturally or auto domain generalization emerges naturally because that's what you're trying to get at with your reward. So I think they'll be a very exciting question to see maybe next time we chat. Love it. All right. Thank you so much for taking some time again. Congrats. I'm periodic. Super excited to see what you guys discover and for when your room temperature superconductor is shooting electricity all around the world around me. OK, but thanks a lot. It was great. Cheers. Doge Chewbook is the co-founder of periodic labs and a former researcher at Google DeepMind. This show is a production of latitude media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Marquand. Stephen Lacey is our executive editor. I'm Shane O'Con and this is Catalyst.",
    "release_date": "2025-11-06",
    "duration_ms": 2186000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI9091142033.mp3?updated=1762407998",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-11-10T01:48:37.216878"
  },
  {
    "title": "Unpacking DOE's proposal to transform data center interconnection",
    "description": "Last Thursday, Energy Secretary Chris Wright directed the Federal Energy Regulatory Commission to consider rulemaking to fast-track interconnection for large loads \u2014 as long as they agree to be curtailable or colocate with dispatchable generation.\n\nSo what does this proposal actually mean for interconnection?\u00a0\n\nIn this episode, Shayle talks with Allison Clements, former FERC commissioner and current partner with digital infrastructure advisory firm ASG. Allison is also principal of 804 Advisory. Shayle also talks with Tyler Norris, doctoral student at Duke University\u2019s Nicholas School of the Environment. Allison, Tyler, and Shayle cover topics like:\n\nHow the proposal would standardize interconnection procedures for certain large loads, with study periods no longer than 60 days\u00a0\n\nThe jurisdictional shift: asserting federal authority over a process traditionally under state purview\u00a0\u00a0\n\nThe types of eligible loads, including traditional data centers as well as ones that colocate with generation, also known as \u201chybrid facilities\u201d\n\nThe duration of flexibility and whether 2-hour, 4-hour, or longer durations are needed for curtailment\n\nWhether flexibility resources should be behind-the-meter or front-of-meter\n\nThe potential disadvantages for bring-your-own-supply or bring-your-own-VPP\n\nResources:\n\nLatitude Media: Wright directs FERC to fast track large load interconnection\u00a0\u00a0\n\nLatitude Media: How the world\u2019s first flexible AI factory will work in tandem with the grid\u00a0\u00a0\n\nLatitude Media: OpenAI pushes the White House to invest in the grid to compete with China\u00a0\u00a0\n\nE3: Demand Response as a Capacity Resource in SPP\u2019s Era of Data Center Growth\u00a0\n\nCanary Media: In a first, a data center is using a big battery to get online faster\u00a0\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com. \n\nCatalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting\u2060 \u2060\u2060BloomEnergy.com\u2060.",
    "summary": "The podcast discusses the challenges and solutions in connecting large loads, particularly data centers, to the electricity grid. Secretary of Energy issued a letter to FERC proposing faster interconnection processes, particularly for collocated generation and load facilities and curtailable loads. The proposal aims to standardize and streamline the interconnection process to address the increasing demand for power. It emphasizes studying loads and generation together, allowing for accelerated interconnection, and asserting FERC jurisdiction over transmission aspects in vertically integrated states. The discussion also touches on the importance of onsite power solutions like Bloom Energy and Energy Hub in meeting the evolving energy demands of data centers and utilities.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shail Khan. This is Catalyst. 23 years ago, FERC issued Order 2003 to standardize large generator interconnection. We've now had two decades of experience of the problems and the solutions which have been kind of haphazard and piecemeal over time to get at this explosion of requests for supply to interconnect to the system. Coming up, who needs coffee when you've got an FERC A-Noper on large load interconnection? The AI boom is here, but the grid wasn't built for it. Bloom energy is helping the AI industry take charge. Bloom energy delivers affordable, always on ultra reliable on-site power. That's why chip makers, hyperscalers, and data center leaders are already using Bloom to power their operations with electricity that scales from megawatts to gigawatts. This isn't yesterday's fuel cell. This is on-site power built to deliver at AI speed. To learn more about how fuel cells are powering the AI era, visit bloomenergy.com or click the link in the show notes. Surging electricity demand is testing the limits of the grid, but Energy Hub is helping utilities stay ahead. Energy Hub's platform transforms millions of connected devices, like thermostats, EVs, batteries, and more into flexible energy resources. That means more reliability, lower costs, and cleaner power without new infrastructure. Energy Hub partners with over 120 utilities nationwide to build virtual power plants that scale. Learn how the industry's leading flexibility provider is shaping the future of the grid. Visit energyhub.com. Learn energy is under attack, and it's more important than ever to understand why projects fail and how to get them back on track. The center left think tank third way, surveyed over 200 clean energy professionals to answer that exact question. Their newest study identifies the top non-cost barriers to getting projects over the finish line, from permitting challenges to nimbism. To read third ways full report and to learn more about their pace initiative, visit thirdway.org slash pace, or click the link in the show notes. I'm Shale Khan. I lead the early stage venture strategy of energy impact partners. Welcome. All right, so this is wonky, but it is super important. We've talked innumerable times at this point on this podcast about connecting large loads, particularly data centers, the electricity grid, and how that has become like the epicenter of a huge challenge, both in AI world and in energy world. Just recently, the US Secretary of Energy, Chris Wright, wrote a letter on this topic that could have really huge impacts. It's a letter to FERC, Federal Energy Regulatory Commission, and it is a combination of asserting FERC authority in a way that has not happened historically over large load-gener connection. But also, just as, or maybe even more importantly, trying to set a process to get those loads interconnected faster, particularly when they are combined with generation, we've talked before about co-location of generation and data centers, and or if they are flexible loads and curtalable. Another thing that we've talked about before. So it ties together a bunch of stuff that I've been interested in, that everybody in the energy world has been monitoring, and it's going to play out pretty quickly because Secretary Wright requested this to be done with an actual order from FERC by April, which is basically lightning speed from a federal regulatory perspective. So we'll see if it happens. But it's one of these things that is very, very important for many folks downstream of this on both the energy side and the AI side, but that I think is actually poorly understood other than the headlines. So in order to parse out what's actually in this letter, in this proposed order, I brought on two folks who are wildly knowledgeable on the subject. One has been on the show before, Tyler Norris. Tyler is a PhD student at Duke University and wrote, I think what is now considered to be kind of the seminal paper on data center load flexibility. And the other is Allison Klemens. Allison was actually a FERC commissioner from 2020 to 2024, so she has deep experience inside the agency itself. She's now a partner with Digital Infrastructure Advisory Firm ASG and the principal of 804 here at Tyler and Allison. Allison, Tyler, welcome to both. Thanks for having me. It's great to be here. Great to be here, Shale. All right, so excited to talk to you about this letter and notice of proposed rulemaking that Secretary of Energy, Wright, sent out very recently. Allison, I want to start with you, since you have the procedural knowledge here, like technically speaking, what is this? What did Secretary Wright send? Technically speaking, what Secretary Wright did was use a provision in the Department of Energy Organizing Act, provision 403b, which is why people are referring it as the 403b letter. He sent a letter to then Chairman Rosner at FERC and all of the commissioners and said, I, the Secretary of Energy, direct you to consider issuing an advance notice of proposed rulemaking on A-NOPER around large load interconnections. It's a 14-page bare-bones document that in the eyes of the Secretary constitutes an advance notice of proposed rulemaking. That's the process. And now FERC can say, okay, great. We're going to consider it, and they either move forward and issue the advance notice of proposed rulemaking, which there has been some public indication already that they are eager to do, or they could decide not to issue the rule, but they would have to justify that decision. Historically speaking, when the Secretary of Energy does something like this, would it be common or uncommon for FERC to say no, thank you? Well, it hasn't happened very often. The one time that people remember is under the next provision, or the previous provision in that Act 403a in 2017, the administration sent over a letter with a proposed rulemaking to then chair Chaddoniel Chatterjee and the FERC requesting them to subsidize collinuclear plants with some on-site fuel supply benefit, and that FERC unanimously rejected moving forward with that proposal. Okay, so there's some possibility it doesn't move forward, but I mean reading the tea leaves, maybe Tyler, you can comment on this because I've seen you say it publicly. It seems generally, maybe outside of FERC, but in the public domain, this has been pretty well received, broadly speaking, which I think does run in contrast to, I remember that previous letter about coal, 90 days of storage and so on. So your sense is that the vibes here are good, Tyler? So far, let's not discount that there will likely be other perspectives that have not yet been represented, but no, I think it was very significant that Commissioner Rosner came out of the gates expressing an eagerness to work on the proposal on the other side of the aisle. You had, you know, Senator Mike Lee that came out, you know, strongly supporting it, and a variety of different stakeholder groups, at least I've seen and companies that are involved in this space seem to view it generally favorably. I think just to parse out a key distinction, right, there's this jurisdictional question, and on that one, I think there's obviously going to be a variety of perspectives and there will be concerns on the part of, especially of some state commissioners, officials, and certainly the investor owned utilities. But with respect to the substance, that's where I've seen the most excitement, and I know we'll get into it, but in terms of what this would actually do to sort of improve the interconnection process, that's where I think there's been the most positive reception. All right, so let's dispense with the wonky jurisdictional stuff first, because I do think it's important to talk about, but then we'll get into the meat of the substance here, which I think is where what's more interesting. But Alison, back to you, what is distinct here about what the Secretary of Energy is proposing in terms of the shifting of power and authority between FERC and states? It's wonky, but fun, Shell. The Federal Power Act gives FERC the federal regulator jurisdiction over the transmission system, right, the high voltage poles and wires and the wholesale sales of electricity, whereas the state's reserve power for generation as well as the distribution system. And there is a little bit of a fuzzy area as relates to whether or not FERC has jurisdiction over the transmission aspects of bundled rates in vertically integrated states. So if you live in a state with a vertically integrated utility, the state commission has jurisdiction over not only your generation charge part of your bill and the distribution portion of your bill, but also the transmission portion of your bill. And FERC has never exercised authority, exercised its authority to take jurisdiction over that piece. The reality is though it's hard to imagine anything, you know, FERC has jurisdiction over practices affecting transmission rates. It's hard to imagine anything more directly affecting transmission rates than new loads hooking up to the transmission system and the costs that those loads impose. And so I think the legal arguments are very strong in favor of the Secretary of Energy's position here and in favor of the position FERC would take. I'm not sure if it's by tradition, by culture, experience or practice that it actually hasn't been asserted in the past. But so in practice then, what's the question at hand here? Say I'm a large load, say I'm a data center or a prospective data center and I want to go get cited in a regulated utility territory. Is the question who has authority over the tariff that is offered to me or the interconnection queue and how it is managed or like what is the actual balance of power that's in question here? There are two parts of that hookup, right? If you're the data center in Georgia or in Oregon and you go up to hook up to the grid, your utility has authority to take you through that study process and hook you up and tell you what it's going to cost. The state has jurisdiction over that decision when you hook up to the grid in vertically integrated states, right? Even if it's hooking up to the transmission system, whereas what this large load, this A-noper is saying is known FERC is going to now have jurisdiction. So FERC could standardize rules for that piece of the hookup across all utilities or some set of principles that these utilities must follow in a way that they have not done before. The second piece of the jurisdictional question is who has authority over the sale from the generator to the data center as the retail sale and that piece remains with the states? Yes, I'll just say as someone who originally came into this as a developer and an analyst, it was always a puzzle to me, right? Why? We had standardized interconnection procedures that were promulgated by FERC for generators but nothing for loads and setting aside all the concerns around rate regulation or even really anything that happens after you're already hooked up to the system. But just the process leading up to actually getting connected, it was always a puzzle to me and of course, you know, Allison sort of articulates how it plays out in terms of the legal and jurisdictional dynamics. But I think and of course, in the letter itself, you know, the secretary references the fact that look, we've had long, established, standardized generator interconnection procedures in part because FERC recognized that interconnection is inherently part of, you know, open access to the transmission system and it sounds like they're making a similar argument. Okay, so it sounds like the, I don't know, the key thing here is if this goes through the jurisdictional shift, then if a large load wants to interconnect to the transmission system, the utility will still be the one who has to manage that interconnection and introduce the tariff and all that in a regulated territory. But the oversight might shift from the state public utility commission to FERC. If this is true and that would allow FERC to do something that's more standardized and sort of national versus it all being piecemeal. That's basically the gist of it. Yeah, that's right. Okay. All right, let's get to the meat and the substance of what the secretary is actually proposing. Can you just give Allison just a high level overview of the sort of what you view as the key elements of the proposal? Absolutely. I mean, it's 14 pages long and you know, we issued a regional transmission planning role at FERC last year. It was 1200 pages long. So this is very bare bones, high level conceptual and there's a lot of devil in the details. But the main things that the rule does is one, it asserts the jurisdiction we just talked about, which has been untested largely. Two, it suggests that loads connecting with new generation together should be studied together, which saves both costs and time. And three, and the exciting part that I know Tyler's going to want to jump in on is that it suggests that if loads are willing to be curtailed, curtailable, if loads will stop when asked stop taking power from the grid when asked by the grid operator, they should experience accelerated interconnection and that study time should last no longer than 60 days. Can I ask though, before we talk about the tailible piece and even and also the hybrid facility piece, is it not saying overall there should be a faster load interconnection process for these large loads? And instead it's saying only in these circumstances, we need to fix the problem for collocated generation and load and we need to introduce a faster process for curtailable load. Is it not like an overarching we got to connect data centers faster kind of thing? It is that. I think you see kind of an embracing of the co-location model. But overall, it is kind of a cut through the confusion and the uncertainty and hopefully the lack of transparency around how you get hooked up to the grid as new load today in this time where supply is tightening. And maybe that's the first that's the part of the jurisdictional thing. It's like, okay, we need to streamline and standardize this whole process to make it better across the board. And then also specifically, let's do something new with regard to facilities that are collocated with generation or that can be curtailable. That's right. I mean, if you think back, you know, 23 years ago, 25 years ago, when FERC issued order 2003 to standardize large generator interconnection, we've now had, you know, since that time, two decades of experience of the problems and the changes and the solutions which have been kind of haphazard and piecemeal over time to get at this explosion of requests for supply to interconnect to the system. And now you have an administration saying, you know, whether or not you like it, we're going to assert this jurisdiction that we're confident that FERC has and we're going to not put put us on a track for the next 10 years to face similar problems. You heard the phrase speed to power a lot lately, but here's what it really means. AI data centers are being told that it will take years to get grid power. They can't afford to wait. So they're turning to on-site power solutions like Bloom Energy. Bloom can deliver clean, ultra reliable, affordable power that's always on in as little as 90 days. Bloom's fuel cells offer data centers other important advantages. They adapt to volatile AI workloads. They have an ultra low emissions profile that usually allows for faster and simpler permitting and their cost effective too. That's why leaders from across the industry trust Bloom to power their data centers. Ready to power your AI future? Visit bloom energy.com to learn more. The grid wasn't built for what's coming next. Electricity demand is surging from data centers to EVs and utilities. They need reliable, affordable solutions that don't require building expensive new infrastructure. Energy Hub helps by turning everyday devices like smart thermostats, EVs, home batteries and more into virtual power plants. These flexible energy resources respond in near real time to grid needs, balancing supply and demand. Plus, they can be deployed in under a year and at 40 to 60 percent lower cost than traditional infrastructure. That means more reliability, lower costs, cleaner energy, you can't get much better than that. And that is why over 120 utilities across North America trust Energy Hub to manage more than 1.8 million devices. Learn more at energyhub.com or click the link in the show notes. Do you ever wonder why it takes so long to get clean energy projects up and running? Do you have permitting reform on the brain? Are you NEPA reform curious? The new paste study from Third Way pinpoints the non-cost barriers that stand in the way of clean energy deployment and keep new solar and transmission projects in limbo. They surveyed more than 200 industry professionals to understand what's slowing down deployment and offer practical solutions on how to fix it. To read Third Way's full report and to learn more about the PACE initiative, visit thirdway.org page. Okay, Tyler, I want to ask you first, you're the curtalable load guy, but before we talk about the curtalable load stuff, I am interested in the second thing that Allison said is in there, which is what is called in the letter hybrid facilities, right? That's the co-located generation plus load. And my read of it is that part of what they're doing here is saying look, like in a default scenario, you submit a load interconnection request, you submit a generation interconnection request, those two things are considered independently and it's not really unified. So this tries to unify them and say, consider them together. But beyond that, these things are kind of intertwined because generation might be the mechanism, if it's behind the meter, it might be the mechanism to be curtalable. So as you think about the big picture world of data centers and their energy provision, where does generation fit in in your mental framework? Yeah, maybe it's worth just stating very clearly upfront that one of the things that has been revealed over the past year, based on this co-location docket and PJM and other considerations and other jurisdictions is that we really have an antiquated load interconnection study process and study criteria because as you said, it's really divided from the generation side. And so this has very significant consequences if you are talking about co-located generation and load because what you might have going on, is that a generator can be essentially offsetting the withdrawal from the grid from a given load during the most stress period, which is what the utilities are actually studying to determine your network upgrades. And so if they don't consider the ability of that onsite generator to offset your withdrawal, you may be much more likely to trigger the need for major network upgrades. And of course, that's more expensive and it can take multiple years. So that's sort of part of the delay. I guess to your sort of broader question there, I mean, I suppose it's no secret that the preferred option for flexibility or we could say curtalability on the part of these large loads is either onsite generation or storage. I do think we've heard a lot about the generation option, and we hadn't really seen large-scale battery storage deals just until the past few weeks. I think the first one that I noticed that was in the public domain was from Iron Mountain, which announced that they were going to do, they were going to size battery storage. Two-hour battery storage is 100% of their facility in New Jersey and then the same in Virginia. And then just last week we saw the significant announcement from Calibran and Align Data Centers that will do also a two-hour battery for a new data center in the Pacific Northwest. And that specifically they said it was helping to accelerate its interconnection on the order of years. So this, even just this, I mean, it sounds very simple and essential, like instead of setting them separately, setting them together. But that would be a very significant development, I think, in the way we do load planning and could mitigate the need for a substantial amount of upgrades and accelerate the interconnection process. How much visibility, I mean, you mentioned those, all those projects are two-hour batteries. And one thing that's not clear to me in this firc order or just in general is how much visibility we have, and maybe this is going to be locationally specific, but how much visibility we have into the required duration of flexibility at any given time. I know there's a high level, you've done a bunch of great work on this, Tyler, on like, over the course of a year, how much do you require in order to maintain like system level resource adequacy? But as we start to think about, is what's going to sit behind the meter exclusively in regeneration, or is it going to be storage or something else? How often does a two-hour battery do the trick, basically? And like, how often are we going to need more? What's going to dictate that? Yeah. No, this is one of the questions. I mean, I think it's widely recognized that in the vast majority of events that you're talking about, that the two to kind of six-hour range is sort of the sweet spot when you just kind of look at most of these periods of system stress. And so even with a two-hour battery, right, if it's sized at 100% of your nameplate and the goal is to reduce your draw by 50%, well, that becomes a two-hour battery, sorry, a four-hour battery, and then you can sort of adjust it thereafter. So we should also recognize that a two-hour battery can become a longer duration battery if you're using it for less than your nameplate. But I would point, there's another study that just came out a few weeks ago. It was by E3, and they used Southwest Power Pool as the market. And they looked at four-hour duration, I think it was eight or ten-hour duration from either a battery or whatever the onsite option was. And what they found is that even with four hours, you end up with what we called an effective load carrying capability, in many cases, like above 50%, which is actually pretty close to the ELCC of some generated options or even longer duration storage options. So I think there is substantial ELCC value that can come from even relatively short-duration batteries. And in a lot of cases, again, especially if you're not trying to go all the way to zero. One of the bigger questions that arises is if you're trying to spec this data center to be able to ride through some massive grid outage where you're talking about over 24 hours or even out to 48 hours, of course, you can't do that with a battery. And this has become a very interesting conversation around these kind of events. Because first off, if these are transmission interconnected loads, you think about what type of event you'd be talking about. You'd have an outage of more than 24 to 40 hours. I mean, what we're really talking about is an event on par with the largest blackouts that have ever occurred in US history that literally led to the formation of NERC. And so obviously they're extremely unlikely. It's not to say that they can never happen. But the other thing, you start to think about what is going to happen during such a historic event and the assumption that you are going to be prioritized to get diesel shipped to your data center as opposed to all the other competing needs, including serious emergency and life threatening situations. I think that's been an assumption that might not necessarily hold. And usually you can't actually get more than a certain number of hours of diesel on site and storage. And if you go beyond a certain volume, I mean, it actually becomes a little bit dangerous and there's a lot of concern about that. So yeah, this is like one of the most interesting, I think, debates right now is like, do you really need to spec to 48 hours, even if you do in terms of like the size of the gen sets, can you actually get that much fuel on site or can you expect it to be delivered? Or is it sufficient to go with something that's like a two to six or sort of eight hour solution? There's like two things, two pieces here that I think are connected to each other, but also are distinct that I always find people conflate, which is using some on site asset could be a battery, could be a generator, could be curtailable load as a mechanism to get interconnected faster. And that's sort of the crux of what this FERC directive is all about. And then there's the, what are you using for backup power, which is what you were just talking about, right? And you theoretically can use the same, some of the same asset, battery in particular, has like a role to play potentially in both. But you can easily imagine that there are distinct things and distinct assets. I mean, currently that's how it's, it is, right? Like the diesel generator is just a backup asset and it probably doesn't have run time limits. They're going to keep it from, you know, operating too much. So depending on the amount of curtailment you need to do, like that just might not work for it anyway. So I don't know, I feel like I want to be careful not to consider those two things the same thing. Does that make sense? That makes complete sense. And yeah, the diesel's meant for right, absolute worst case scenario emergency purposes. And so my sense is that all those two hour battery storage deals that we just mentioned at data centers, they still have gen sets likely diesel for those longer duration emergency events. And they're not at all mutually exclusive. And in fact, it may be that predominantly we see that kind of arrangement going forward where we do have battery storage. You know, at a high level, if you're going to stop taking energy off the grid as a large load customer, you can curtail and just stop. You can go to your backup diesel gen sets or your gas rice sets, which are emerging, but you can face limitations in either case. You can decrease the intensity of your compute, which you know, your guests in the past have talked about companies trying to do that. You can transfer your compute. You've had other guests on your show trying to talk about that, right? Or you could have a third party curtailment on your behalf, some sort of contract, whether it be the virtual power plant or otherwise, that would provide the decreased stress on the grid. And I think these, the kind of service of curtailment service and where it's kind of coming from and the kind, whether it's energy, whether you're offering capacity curtailment and for how long and how you're getting paid, those are all really important details that haven't been defined anywhere. And so when you think about the bucket of issues that are going to arise in this proceeding, those are some that rise to the top for me. Yeah, I've been, I'm curious to get both of your take on this. I've been starting to conceptualize a little bit of a framework in my head for data center flexibility or large load flexibility in general, which is like a resource curve, sort of. It's a little different from your traditional like oil resource curve kind of thing. But as you said, Allison, there's a bunch of things you can do. And the way I think about it is the X axis is how much flexibility you can deliver, how much capacity I guess are like hours times megawatts probably. And then the Y axis is cost basically. And you know, in principle, your lowest cost thing to do, assuming you can stay within your customer SLAs is just load flexibility straight up. It's managing compute differently, either shifting geographically or shifting temporarily. Right. But there's only so much you can do of that. It's going to be limited. So it's low end of the resource curve, but not that wide on the chart. And then you start moving up the chart and you get batteries and generators and all this other stuff. And I guess the question, Allison is, do you think that FERC in this proceeding will, is FERC going to be in the position to have to sort of distinguish amongst these resource, the mechanisms to get to curtailment or flexibility and offer ELCC type metrics for them and things like that, and then define all those rules? Are they going to leave that to the utilities and say, look, you know, utility, you define what curtailment looks like, what curtailable load looks like. But if there is a curtailable load in your territory, then you need to run this procedure to get interconnected. Yeah. I mean, I'll give you the within kind of the FERC box answer. And then maybe the political context in which this conversation is taking place and what that means for it. And within the FERC box answer, it is rare that FERC regulates down to that level of specificity, right? I mean, you have seen on the supply side capacity accreditation methodology and come in from the various RTOs, for example, and FERC largely defers subject to kind of any potentially discriminatory impacts. And so, historically, the agency has also really been kind of a thousand flowers blooming type place and been ever since standard market design failed in 2000. The agency has been really skittish about requiring standardized anything across the board. It often does principles, satisfy these six principles or when you're thinking about the types of flexibility, ensure that you value this, this, and this. So, my instinct without getting into the politics is they're not going to get that specific on first take. And then you layer on the fact that the 403 B letter from Secretary Wright suggested this should be done by April. So it's the end of October. And A-NOPER means you're going to take comments on an A-NOPER, then write a NOPER, then take comments on a NOPER, then issue a rule that would be rocket speed in FERC world. So I think there's no way they can get to that level of specificity on so many of the details of these questions. And that's my biggest concern here. We don't want something rushed that ends up failing to really take advantage of the opportunity that these flexibility alternatives provide and incentivize them in a way that works for the providers. Yeah. And maybe I'll just dive in there for a second because I think one of the good news stories is that we actually arguably have a little bit of precedent for this type of what we might call quasi firm service. It was really more meant for generators. And it's actually called conditional firm transmission service. It's really, to my knowledge, hardly ever been used outside some cases in the Pacific Northwest. It was actually a service. I think it was created in the sort of late 2000s. And the whole idea was where you couldn't get fully firm transmission service and you didn't want to go fully non-firm. Could you get something that was conditional firm? And actually, I should shout out to Rob Gramley because I think he was actually representing the American Wind Association at the time after he had less FERC to get this done. And so one of the big debates in that proceeding became can the transmission provider define a certain number of hours that would be needed over the course of a year and that you could be curtailable, such that you could qualify for this conditional firm service. And of course, there was significant pushback from the transmission providers. And they said, it's too hard to offer a certain number of hours a year. So instead, we're just going to tell you what the system conditions would be when curtainment would be likely to occur. And so that sort of gave an out. But I think the really, the holy grail here that we're sort of talking about is it has to be bounded flexibility or bounded curtainment in terms of defined maximum number of hours in a year and then something with respect to the duration of the events. And if you look at, for example, Google's comments in the PJM, this big process, they have this critical issue of past, trying to figure all this out, they say, look, we might be willing to actually participate in the demand response program. It's just that right now it's unbounded, right? So there's just like no limit on the number of hours that could occur. And so that, I think, is what we're getting at. And by the way, there are other models to look at to the UK as this whole curtailable connections program. And they actually remarkably, if they end up curtailing either the generator or the load more than what their sort of guarantee says that they actually compensate the customer. I don't know that we're going to get to that extent in this kind of program offering. But at minimum, right, it would seem to make sense that we have that as a voluntary option for the flexible loads that are able and willing to use it. Another thing that comes to mind that you didn't mention yet is that we've talked a little bit about behind the meter and what the hybrid facilities might look like. But we also want to make sure that front of the meter opportunities to provide this type of support don't get left out of the conversation. I heard someone say at a conference yesterday, behind the meter in front of the meter is going to evolve into around the meter, right? And that's a concept that we need to be careful doesn't get lost. So if you have a, for example, front of meter storage solution that might be able to provide curtailability close to, but not behind the same point of interconnection as a new large load that this proceeding contemplate those opportunities as well. And that within the way that the letter is written would, like when they talk about hybrid facilities, are they talking about exclusively behind the meter exclusively on site or potentially near site, like in the same zone or whatever? Is it clear? I read it to say, you know, withdrawal and injection behind the same point. It's, it doesn't suggest elimination of other opportunities. It just doesn't speak to them. What do you think Tyler? I don't think they went into that level of specificity, but, you know, for example, with what Southwest Power Pool is proposing, I think the metric they use, we'd have to check the final version, but I think it was like, it needs to be within two substations of you is sort of like if you're going to be somewhat co-located or have certain benefits from an associated generator with the load. So I think there's a lot of opportunity to get creative there. And one other thing I just want to say about this, you know, almost inevitable proceeding that seems to be about to happen is that, look, like even if it doesn't lead to a final rule and even if right there are significant, such significant concerns about the jurisdictional dynamics that FERC doesn't want to go there. By the way, I want to respect that there are some legitimate jurisdictional concerns. I want to respect this. I think just the substance that this will hopefully elicit in terms of how to make this kind of service work is going to be incredibly valuable. And even if it doesn't lead to a final federal rule that existing jurisdictions can sort of take that content and take that back to their ISR or to or at the state level to hopefully get these kind of offerings in place. Yeah, that's a great point. And I don't want, you know, there's a lot in the negative bucket. There's lots of positives and opportunity here. This is a giant problem. This is a path forward. Let's not let perfect be the enemy of the good. Let's make progress. You know, there's lots of concern about independence of the commission, about jurisdiction, et cetera, et cetera that must be acknowledged as we go down this path. Tyler, I'm just curious to get your take on this flexibility resource curve, not necessarily the concept specifically, but the basket of things that will be available to provide curtailability. And in your mind, is the right, I guess the question is, is the right approach in your mind to say, you know, you need to be able to provide X hours of flexibility with the Y duration events or whatever, and then let the market figure out how much of that is going to be generation storage and load flexibility? Or does it make sense to actually, I don't want to say put your thumb on the scale, but like try to dictate a little bit how much of what gets implemented there? Yeah, no, and actually our research lab at Duke University has been because, you know, we're modeling sort of the bulk power system. And so ultimately, you want it to be as sort of generalizable as possible, right, so that you can sort of represent any load that is utilizing this type of flexibility for all these purposes. And so you want to kind of parameterize it and make those parameters as generalizable as possible, also because then we can actually create a market, right, where a variety of different options can compete either onsite or even to some extent offsite. I mean, this gets even more interesting and complex when you start to think about, you know, these large loads potentially procuring the flexibility from other loads in the same balancing authority, the kind of simplest version of it's just for capacity, right, just resource adequacy. But then if you're actually talking about to sort of mitigate transmission congestion, you know, think about the electrical proximity of those customers and how you sort of do that study, I think that's sort of like the very advanced version. I hope we can get there, but at minimum, like let's get in place for those that are doing it behind the meter. And yeah, whether it's load shifting, you know, actually shifting around the computational workloads or its, you know, its battery storage or its generation, or even, you know, just reducing operation, you know, I think there are cases where just from a planning perspective, like you wouldn't assume that a new load is going to be drawing at its max pull during certain types of weather, you know, weather conditions. Is there a risk that this actually, I mean, you know, we've just seen the emergence of some of these first like, like the bring your own VPP type of thing, right, which is not on site, but is near site. Is there a risk that this process actually disadvantages that stuff in its decency? Yes. There's always a risk. I mean, you know, to add to Tyler's previous response, which I totally agree with, the other nice thing about not getting specific about technologies is that it'll, it'll, it's legitimate under the Federal Power Act, which is for guiding principle, right? And so to the extent that there are attempts to do things like take away the opportunity for bring your own virtual power plant, bring your own supply in other ways, that doesn't satisfy the nondiscrimination requirements under the statute. So to my mind, we need to make sure there are guardrails in place to protect for that purpose. I'll just say to, you know, this is so different from the way we do kind of air connection studies right now where the way it works right now, right, is you're basically looking at these steady state fixed snapshots of the system, right? Like single points in time. And you might do like a winter case, a summer case, and maybe one like during the shoulder months, but it is just a single fixed point in time. Whereas what we're talking about is like extending that out maybe not all the way to 8760, right? At least maybe to like 1000 hours in the year. And there are a few, I think, existing transmission providers that could perform that kind of study, but we're going to have to just get better and more advanced at doing this. And so I think part of this, frankly, is like we just need to like train more people on how to do this kind of more advanced study. And I'm hopeful that this proceeding will sort of shine a spotlight on those needs and capabilities so that we can sort of promulgate it more broadly. All right, so wrapping it up, I want to talk about what comes next, Alison, you mentioned the timeline. So this is also supposed to happen. There's supposed to be like a final order by April. Is that a firm deadline of any sort, confer, just take longer if they need to? Because that is remarkably fast. I'm remembering how long it took to do like, I don't various other fricors historically. Yeah, it took us four years to issue from a noter to final rule, the regional transmission planning order. And then there was an order on re-hearing in 1928 and then 1920 B. It's not realistic what authority does the, you know, the Secretary of Energy have in that case, the bully pulpit, political pressure. But I think when that letter came out, there were a lot of fric staffers who were thinking, oh, no, my Thanksgiving and holiday plans. So I think you have to imagine that if the commission is showing good progress, that is, is this satisfactory place to be, but we will see what happens on that front. All right. Well, thank you so much for both of you for taking the time and walking me through this. This is going to be something to monitor closely over the next few months. So we'll have you back on when we know what's what, but appreciate the time. Sounds great. Thanks so much, Trio. Alison Clemente served as FERC commissioner from 2020 to 2024 and is now a partner with Digital Infrastructure Advisory Firm, ASG and the principle of 804 advisory. Tyler Norris is a PhD student at the Duke University's Nicholas School of the Environment. This shows a production of latitude media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Marklond. Stephen Lacey is our executive editor. I'm Shale Khan and this is Catalyst.",
    "release_date": "2025-10-30",
    "duration_ms": 2437000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI9037010912.mp3?updated=1761799864",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-11-10T01:52:22.324907"
  },
  {
    "title": "Five big questions about the future of energy",
    "description": "We\u2019ve covered AI\u2019s massive power appetite in depth over the past year \u2013 with good reason. It\u2019s the driving force behind much of the change and uncertainty in the energy world right now, from the error bars around our demand for electricity to the lineup of technologies vying to meet that demand.\u00a0\n\nIn this episode Shayle talks to his colleague Andy Lubershane, head of research and partner at Energy Impact Partners, about five big questions arising in this uncertain load-growth environment. They cover topics like:\n\nThe underappreciated factors that could flip the supply crunch to oversupply, like algorithmic efficiency gains, on-device inference, and off-grid data centers\n\nThe winners of the AI-drive power boom, including utilities and grid equipment suppliers, and the potential losers like industry that relies on cheap power\n\nWhether there will be a \u201cCambrian explosion\u201d or consolidation of nuclear reactors designs\n\nThe prospects for enhanced geothermal after Fervo\u2019s Cape Station comes online\n\nThe future of grid-enhancing technologies like advanced conductors and dynamic line ratings, and whether they will make it out of \u201cutility pilot hell\u201d\n\nResources:\n\nSteel for Fuel: Why does nobody know how much energy AI will consume? \n\nOpen Circuit: How do we know if we\u2019re in an AI bubble?\u00a0\u00a0\n\nCatalyst: The US nuclear groundswell\u00a0\u00a0\n\nCatalyst: How geothermal gets built\u00a0\u00a0\n\nLatitude Media: In Georgia, stakeholders still can\u2019t agree on data center load growth numbers\u00a0\u00a0\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.\n\nCatalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting BloomEnergy.com.",
    "summary": "The podcast discusses the uncertainties in energy demand due to data centers, AI growth, and power supply. It touches on the challenges faced by utilities and industries in a supply-constrained market. The conversation delves into winners (power system equipment providers) and losers (large industrial electricity consumers) in the AI-driven electricity load growth scenario. The potential for off-grid data centers and the impact of algorithmic efficiency on energy demand are explored. The discussion also includes the difficulties in citing new industrial plants in the current market. Specific technologies like nuclear, geothermal, and microgrids are analyzed in the context of the evolving energy landscape.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shail Khan, and this is Catalyst. There's a lot more uncertainty on the demand side of the equation, largely coming from how much energy data centers are going to be consuming. That's where you may even get order of magnitude levels of uncertainty in terms of future power demand. We definitely have order of magnitude level uncertainty in future power demand, like even just the forecast. Take all the different prognosticators on how much load growth there's going to be in 2035. And there are huge margins between those forecasts because nobody actually knows. Coming up, five intriguing questions about the future of electricity with Andy Lubershane. The AI boom is here, but the grid wasn't built for it. Bloom energy is helping the AI industry take charge. Bloom energy delivers affordable, always on ultra reliable on-site power. That's why chip makers, hyperscalers, and data center leaders are already using Bloom to power their operations with electricity that scales from megawatts to gigawatts. This isn't yesterday's fuel cell. This is on-site power built to deliver at AI speed. To learn more about how fuel cells are powering the AI era, visit bloomenergy.com or click the link in the show notes. Surging electricity demand is testing the limits of the grid, but Energy Hub is helping utilities stay ahead. Energy Hub's platform transforms millions of connected devices, like thermostats, EVs, batteries, and more into flexible energy resources. That means more reliability, lower costs, and cleaner power without new infrastructure. Energy Hub partners with over 120 utilities nationwide to build virtual power plants that scale. Learn how the industry's leading flexibility provider is shaping the future of the grid, visit energyhub.com. Clean energy is under attack, and it's more important than ever to understand why projects fail and how to get them back on track. The center left think tank, Third Way, surveyed over 200 clean energy professionals to answer that exact question. Their newest study identifies the top non-cost barriers to getting projects over the finish line, from permitting challenges to nimbism. To read Third Way's full report and to learn more about their PACE initiative, visit thirdway.org slash PACE, or click the link in the show notes. I'm Shale Khan. I lead the early stage venture strategy at Energy Impact Partners. Welcome. All right, this one is fun. Andy Luberschain, my partner at EIP, our head of research, who you know and love, if you listen to this podcast, is back. And this time he and I just came up with a list of what we think are five interesting questions that we're both thinking about, largely as they pertain to the world of AI power, load growth, and all the technologies that are benefiting from that. So we have some general questions about what's going to happen in this market, and then some specific questions about technologies that may be benefiting from it, but still needs to prove themselves out at scale. I won't belabor it too much. Here's Andy. Andy, hello. Shale, I'm back. You're back. All right, we're just going to do a bunch of interesting questions that you and I have been going back and forth about all under the umbrella of the thing we keep talking about the thing everybody's talking about in energy world, which is the load growth in the electricity sector driven predominantly by AI. So we're going to talk about some of that specifically and then some of the, I guess, reverberating effects that we've seen on technologies. I think we'll start a little broad, and then we'll get more specific and talk about a few technologies specifically. But let's start with a broad one. It's the theme of the decade. It is the theme of the decade. It's certainly the theme of this decade so far. Well, actually, the first question is maybe a, it stems from that, which is sort of a question of, will it be the theme of the next decade or the end of this decade? So here's the question, right? I think undeniably, we are currently in a state of undersupply. The supply demand balance between available electricity capacity at large scale, at least, and supply is mismatched. And that's why we have these interconnection cues. And that's why we have the long wait times. That's why gas turbines are sold out and all this kind of stuff that we talked about. And that is definitely the state of affairs today. I don't think anybody's going to debate that. Here's the interesting question. When could supply demand mismatch flip back the other way? And if it happened, what would cause it? In other words, when could we enter a state where, oh, wait a second, there was a lot of overbuild. And suddenly, there are a bunch of empty shells of data centers and or electricity, load growth, underperforms, relative expectations, et cetera. Do you have a view on like when that might happen and why? It is so difficult for me to try to answer this question. It's very difficult to imagine that balance flipping the other way. It's possible for me to imagine the supply demand mismatch becoming alleviated over the next five years, or that the supply does not grow as much as we currently anticipate. And so demand is able to keep up better than we've been expecting in the next five years. For it to flip back the other way is practically impossible. I feel like my past, you know, 18 months, every conversation I've had has been talking about all the reasons why we are in the scenario we're in, which is that supply is falling short of demand by such a wide margin or appears to be falling short of demand by such a wide margin. But stretching my imagination, I think I guess there's two ways that could happen. It's two ways that we could alleviate this imbalance and maybe flip back. One would be that on the demand side we're very wrong or the variables causing demand to grow so rapidly change substantially. And the other would be that the variables on the supply side change substantially. I think demand is more likely because there's a lot more uncertainty on the demand side of the equation. And that uncertainty is largely coming from how much energy data centers are going to be consuming. And there's like multiple reasons for that uncertainty, but I think that's where you may even get order of magnitude levels of uncertainty in terms of future power demand. We definitely have order of magnitude level uncertainty in future power demand. Like even just the forecast, take all the different prognosticators on how much load growth there's going to be, how much power demand from AI there's going to be in 2035. And there are huge margins between those forecasts because nobody actually knows. And like you said, there's a bunch of different dynamics that could drive it in one direction or another. One that I think is interesting sort of question mark on future demand. David Kahn from Sequoia, who's a friend of mine has been putting out this kind of like series of posts. He started with the $600 billion question maybe a year ago, which was like at that point $600 billion in CAPX announcements had been made in data centers. And he's like, that has to pay itself back somehow through actual demand in the economy for the services from that AI. And that's the number. That's the bogey that they have to hit. And now the number is who knows how much more. And he's been doing a series since then that's sort of looking at like where's all the money for the CAPX coming from, who's actually paying for it and taking the demand risk. And there's definitely some risk that there is more investment being made into data centers than there is real economic demand for those services. But it's not clear to me how long it would take for that to trickle through to, oh wait, load growth isn't going to be as large. Like part of this is I think that the inertia on both sides of this, or it's maybe the opposite of inertia, the momentum on both sides of this is going to mean that like if the train stops, it's going to take a while to stop. I think that's right. I wrote a post about on steel for fuel about this little while ago called, why does nobody know how much energy AI will consume? And I think that's one of the big variables that you just pointed out, which is just the demand for AI is still uncertain. There's a big error bar in terms of how much individuals and corporations are going to be consuming AI, which is related to how much better are these models going to get. How quickly are enterprises going to figure out how to apply them in their businesses, as well as as you pointed out, just the profitability of the industry. And I do feel like that is a variable that currently feels like it has a tremendous amount of momentum. It feels like almost nothing could stop the train of investment of tens of billions of dollars, hundreds of billions of investment that's going into AI research and AI data center construction right now. But there are a trickle of stories now in mainstream press and in business press, the Wall Street Journal, etc. That mentioned the word bubble, right? And it isn't entirely clear how sustainable this level of investment is. And bubbles are economic bubbles are the sorts of things that can pop quickly and surprisingly they have in the past. And so I'm not taking a position right now personally on how bubbly the behavior is out there in the market. But that is clearly a risk. That's one vector of risk in energy demand growth world is that something causes investors to fairly quickly lose confidence in what can feel like an economic bubble of sorts. And by the way, that would not just be devastating for the energy sector in some ways. That's pretty much propping up the entire US economy at this point. It is. I think it's unlikely that's going to happen. I mean, well, let me say, first of all, we're deep-seek. People thought for a minute that that was going to be that, right? It was like dramatically more energy efficient per flop. And, oh, hey, maybe this electricity load growth isn't real. And then, you know, two minutes later, we were right back where we were. I can see. I will say started interrupt. I was going to say deep-seek was sort of a second degree related to a second variable that I think is causing all this uncertainty. And AI-driven energy demand, which is not the demand for AI services or the profitability of the AI sector. It's algorithmic evolution, right? It's the fact that we might continue to invest in AI, and there might be a tremendous amount of demand for AI. But AI models become much more efficient on both the training side and the inference side such that you can consume the same amount of AI just with 10 times more energy. And so, you know, we are seeing that happen in real time. Like, AI algorithms are becoming more efficient. They have been for the past 10 years. Just a question of whether the pace of efficiency improvement changes dramatically or not. And for a moment, deep-seek made it feel like, oh, my goodness, maybe we're in a whole different paradigm when it comes to AI algorithmic efficiency. Well, ultimately, it was the Jevons Paradox Challenge, right? It was like a, hey, it's way more efficient, so we're just going to do more. And maybe there's some limits to that, but I don't know where it is. The two things you could imagine being like big new shocks to the system that kind of blow up the demand story would be on device inference, widespread on device inference. So it turns out, let's just imagine that like actually, you know, maybe training of new models needs to be done in these big centralized data centers, but we're going to need fewer and fewer of them as we start to asymptote in terms of their capabilities. And then meanwhile, inference moves way, way, way to the edge and becomes super compressed and you can put your little GPU on your phone and run a model on it. And so inference actually doesn't drive the next big wave of AI data centers. That's one thing I could imagine. The second thing I could imagine in a totally different direction is that you get widespread off-grid data centers, which there's been lots of speculation and talk about the possibility of taking stuff off-grid. You don't really see it happening yet. But like, if I'm imagining what are major disruptions, those are the two that feel like, you know, at scale, they could meaningfully change the picture of the supply-demand balance. I think that's the one major supply side disruption you can see, right? Because there is there are not there are, yeah, for off-grid data centers, exactly. There's there's not order of magnitude uncertainty in terms of how much power capacity we can build on the grid, right? Like that's that's something that, you know, we can add a certain number of tens of gigawatts through the remainder of the decade, but we're not adding hundreds of gigawatts, right? Like that just is not possible. So the one way you could potentially step up by maybe in order of magnitude, the amount of new data center load you serve is by bypassing the grid and developing data centers in the American Southwest where you have plentiful land to build data centers themselves, massive solar facilities, batteries, etc. And a little bit of backup gas generation such that you can you can build with the same level of reliability and confidence you have and then get a fiber connection rather than have to build a whole lot of new transmission. It's way easier to imagine building lots of new fiber to a concentrated region of the country where you have really good solar resources than it is to imagine building, you know, massive amounts of new resources. And then you have massive amounts of new interregional transmission lines for electric power. Okay, so maybe to wrap this one up, I'll force us each into just like a yes, no here. 2030 let's pick five years from now. Has the supply demand balance meaningfully shifted back in the other direction? Or are we still in this like very supply constrained market? I bet no. Yeah, I bet no as well. Okay, so next question is going to be going to dovetail off of this. Let's talk about winners and losers. We'll keep this one pretty quick. You got to pick one winner from this whole AI electricity load growth thing and you got to pick one loser and let's try to make it not the totally obvious winners and losers like. Oh man, I was going to go. I was going to go with the one. No, pick whoever you want. I don't know what's obvious to you, but okay, pick pick one winner who wins in this. Anybody selling basic power system equipment, right? If you're making transformers or switch gear or conductor or turbines of almost any kind or engines of almost any kind, if you're making a way of producing generating electricity or delivering it, you're probably doing pretty well right now. And I think we'll be for at least five years to come as we just talked about. Yeah. It's a boring one, but yes, that's one category. I mean, I was going to go even more boring and say utilities, which is just an extension of the same thing that you've been saying. So yeah, but that's the right. You want to be in the supply constraint market. You want to be on the supply side. That's, that's kind of obvious. What about a loser though? Yeah, that's a trickier question. I mean, I think we've talked about this on the pod before you and I are at least you and I have talked about it in some other context, but companies that are depending on low cost electricity for the product. What's the cost of electricity for electrification? Or electrification, by the way, or just depending on low cost electricity period. Like I've been thinking about like, if you wanted to cite a new aluminum smelter, which is already electrified, you don't need, you're not like electrifying an industrial process that wasn't before. You're trying to put, I mean, which by the way, right? Like we have tariffs on aluminum now, we should be producing more aluminum in the United States. Probably the single thing, by the way, this was going to be my answer too, but I think it's like an underappreciated problem here, which is if you're a large industrial electricity load and you want to cite a new plant, you are way at the back of the queue right now. One, you're going to pay more and two, citing is going to get really, really hard. Yeah, I mean, electricity consumers generally, unfortunately, I think are going to be losers from this boom in demand. And it's easy to sort of like blame data centers for that. It's not the fault of any individual data center development. And there are, in fact, ways you could see how, in any given utility service territory, adding a new data center, if it's done well, and you have the right contract in place with a credit worthy data center. And then you can actually credit worthy data center operator and, you know, they're paying for their fair share of system upgrades and then some that you could actually reduce costs for all the other rate pairs from a single data center. The problem is all this pressure collectively on the power system and on the supply demand balance for every element of that system is causing prices to go up all over the all over the place, right? Any amount of growth is more expensive than it used to be. And in addition to that, we're encountering all this growth at a time when the system, the core system needs upgrading and hardening and all that to boot, right? Yeah, and I think, like, yes, the price is one problem and the citing is a second problem. And they're both challenging, but I think the citing one might actually even be worse for large loads, just because, like, if you are trying to, if you're 100 megawatts, like, that's the load that you need, and you're trying to find a site that can host 100 megawatts, there is very little chance that one of the 100 data center real estate developers has not already tried to find that site and buy it and there's competition for it. It's just really difficult to do. And then your willingness to pay is going to be lower, probably, because probably whatever you're doing is some industrial process that's lower profitability, at least conceptually profitable relative to data centers where the money is flowing freely. It is really challenging. Some day, theoretical profitability, exactly. Yeah, exactly. I mean, I will say this is where we get back to the concept of off-grid large power facilities, large load cited off-grid, right? I mean, we have not seen it start to happen yet, but there's a strong theoretical case to be made that if you have an industrial facility in particular where you don't need to be cited in any particular location. So long as you can get, as long as there's people that can work there, so long as you are near enough to highways and rail access, other modes of transportation to bring your goods to market and people to come work at your factory, there should be places now. I think there was a great paper late last year from scale microgrids and stripe analyzing the opportunity for mostly solar powered microgrids in the U.S. Southwest. There's plenty of room to build those sorts of facilities. And actually, manufacturing facilities probably lend themselves to doing that even better than data centers because they really are much less latency sensitive. I mean, so again, as long as you have a route to get your stuff to market, then it doesn't matter so much where you're cited. Yeah, although that's becoming increasingly true of data centers as well, at least depends on the use case, but you're seeing some good sight at all over the place now. Okay, let's move on to talk about a few specific technologies that have been the kind of, I'd say the darlings or at least some of the darlings of this wave of AI powered of mangrove. So we're going to do one interesting question on nuclear, one on geothermal, one on gas grid enhancing technologies. Okay, here's the nuclear one that I think is the interesting question in front of us in nuclear, which is, let's assume there will be a nuclear renaissance in the United States. Let's just pause it. It's going to happen. Will it be comprised of a Cambrian explosion of a bunch of new reactor designs? A lot of these, let's call them Gen4 reactors, SMRs, micro reactors, all this kind of stuff. There are dozens of venture back companies, which are gaining a lot of steam and momentum. The US DOE is running this reactor pilot program with 11 of them. Will we see the deployment of, you know, 10 or more new reactor types into commercial systems? Or will this nuclear renaissance basically be comprised of AP1000s, which is the one Gen3 plus reactor that has been deployed internationally over and over again, manufactured by Westinghouse, or maybe the one SMR reactor that seems to be furthest along, which is the G Hitachi BWRX 300. That's the one that's going to get built in Ontario and maybe TVA territories. That'll be the first one. So the basic question is like, are we going to see the ton of new reactors deployed in the market? Or are we mostly just going to see the one or two that sort of have already gotten mostly through the gauntlet? I think the short story is there already is a nuclear renaissance happening globally. Hasn't quite caught on yet here in North America or in most of Europe, at least Western Europe. And we can see the answer playing out, which is that there's just a few reactor designs that are getting traction. And basically, it's the ones that you mentioned, especially the AP1000 at this point. And, you know, China is very much driving that. And it's actually one area of technology in which China is still buying a significant amount of technology from Western, a Western vendor. And I think that that same pattern is going to play out in the nuclear renaissance as much as it happens anywhere in the world. There just can't be a Cambrian explosion of new reactors. The industrial logic of the nuclear industry just doesn't lend itself to that. I think best case scenario, it's bad for the industry if you end up with, you know, four or five competing reactor designs that are relevant in any given region. Because really what you need for nuclear to come down the cost curve is you need economies of scale throughout the supply chain and you need to really come down the learning curve when it comes to deployment. And I would say that learning curve extends all the way from policymakers and regulators down to, you know, people doing construction on the site. And that's only going to happen if you pick one or two designs basically per region and you just deploy the hell out of them. So, you know, maybe we'll get one Gen 4 reactor champion, one SMR champion in each region. Any more than that, I'm not sure it's really sustainable. Yeah, I think in the long arc of history here, you know, the gas turbines are a decent proxy here and there is an old, old gopply of gas turbine suppliers, right? Like the three big ones are GE Mitsubishi Siemens and they control 70 plus percent of the market. I don't really see why nuclear should be so different from that. And so if they're going to end up, you know, I guess you could argue maybe there are different use cases for smaller versus larger, but that's also kind of true gas turbines as well. So I feel like to me, I don't really see the argument why there should ultimately be many, many of them. And if there are ultimately only going to be a couple or a few, then yeah, it feels to me like you got to make the counter argument to why the ones that are furthest along can't ultimately be the ones that are furthest along, by the way, with companies that have balance sheets behind them, as opposed to all the startup reactor companies, it just feels tough, right? We have some public companies, right? Like, Oklahoma is a public company now selling reactors, new scales a public company now trying to sell reactors. Like, I don't know. I don't know how it doesn't end up just being kind of a bloodbath for a bunch of those companies and then a few of them sort of make it through at the end of the day. Yeah, I think the next five to 10 years are pretty crucial for any of those more startup-y reactor companies because there's clearly not room for more than a few of them. So this is the time to cement themselves as the maybe one or two that make it through the ringer and start to gain significant scale and totally agree with you on the sort of comparison to the gas turbine market. You could also say there's a parallel here in the aviation market, right? Any really big, complex piece of machinery that takes tremendous amount of institutional knowledge, not to mention IP to build and has really high safety and regulatory concerns attached to it? Right, so rocket engines, yeah, sure. Yeah. Yeah. You heard the phrase speed to power a lot lately, but here's what it really means. AI data centers are being told that it will take years to get grid power. They can't afford to wait, so they're turning to on-site power solutions like Bloom Energy. Bloom can deliver clean, ultra reliable, affordable power that's always on in as little as 90 days. Bloom's fuel cells offer data centers other important advantages. They adapt to volatile AI workloads. They have an ultra low emissions profile that usually allows for faster and simpler permitting and their cost effective too. That's why leaders from across the industry trust Bloom to power their data centers. Ready to power your AI future? Visit bloom energy.com to learn more. The grid wasn't built for what's coming next. Electricity demand is surging from data centers to EVs and utilities. They need reliable, affordable solutions that don't require building expensive new infrastructure. Energy Hub helps by turning everyday devices like smart thermostats, EVs, home batteries, and more into virtual power plants. These flexible energy resources respond near real time to grid needs, balancing supply and demand. Plus, they can be deployed in under a year and at 40 to 60 percent lower cost than traditional infrastructure. That means more reliability, lower costs, cleaner energy, you can't get much better than that. And that is why over 120 utilities across North America trust Energy Hub to manage more than 1.8 million devices. Learn more at energyhub.com or click the link in the show notes. Do you ever wonder why it takes so long to get clean energy projects up and running? Do you have permitting reform on the brain? Are you NEPA reform curious? The new PACE study from Third Way pinpoints the non-cost barriers that stand in the way of clean energy deployment and keep new solar and transmission projects in limbo. They surveyed more than 200 industry professionals to understand what's slowing down deployment and offer practical solutions on how to fix it. To read Third Way's full report and to learn more about the PACE initiative, visit thirdway.org slash PACE. All right, that's nuclear. Let's do geothermal. And we'll focus on the subclass of geothermal. That's the kind of new one, which is enhanced geothermal. So, enhanced geothermal has garnered a lot more attention of late, I would say. In part, thanks to the fact that Chris Wright, Secretary of Energy in the US is pretty bullish on it amongst a bunch of others. And the standard bear, the flag bear of the enhanced geothermal is FERVO. FERVO is currently in construction on their first commercial project. It's called Cape Station. And I think their expectation is that they hope to have 100 megawatts out of a, I think it's a 400 megawatt project, online in 2026. So, that'll be the first enhanced geothermal project ever built, commercial enhanced geothermal project ever built. And I think the interesting question to me is, if we assume success there, it's assumed FERVO does bring 100 megawatts online in 2026. How big a watershed moment is that for EGS? How much does that tell us about EGS's ability to scale globally and quickly from there? Or is it closer to, and this is not going to be fair to EGS because it is not the same situation, but just to give you the other pull of a possibility, in nuclear fusion, it was like the watershed moment was when somebody reaches energy break even, Q is greater than one, and NIF did it. But it was on a totally uneconomic reactor. So, there's sort of a race to be second to do it on a reactor design that could theoretically be commercial. So, it's not really the watershed moment that I think people thought it might be. So, anyway, for FERVO with Cape Station, like, let's say they succeed, what does that make you think about EGS? It's a great question because I think I have complicated feelings about this one. I think the answer, you could frame the answer either way. In one sense, I think it is a watershed moment. It's a flag that is planted for enhanced geothermal that demonstrates that it can be done, that it can be done reasonably cost effectively. I'm sure there's a tremendous amount of learning that happened throughout the development of first the demonstration project that FERVO has built, and then onward to this full-scale commercial project. And we also know that there are a number of companies that are hoping to be fast followers in the enhanced geothermal space that believe that they too can leverage the existing oil and gas supply chain for the new system. And so, there's a lot of other things that we've learned about the new supply chain for hydraulic fracturing and horizontal drilling. And some of the same service providers that FERVO has utilized where some of that knowledge now resides and also develop enhanced geothermal projects. And so, there's reason to believe that FERVO is the leader and Cape Station is kind of the starting gun. And so, the idea is that geothermal, unfortunately, is inherently going to be a slower technology to roll out even after that starting gun than something like solar was. You could argue we're at a point, FERVO is kind of initiating a geothermal market, and that we're at a point, sort of like solar was at, maybe in 2008 or 2009, when the very first relatively small, utility-scale solar projects were being built. But solar, especially at that point, when there was so much open land near transmission and reconnection access points, it was just so easy at that point. Once the economics made a certain amount of sense, and there was policy support for large-scale solar to roll out extremely quickly. And geothermal just inherently because of the fact that there still is risk in drilling and exploration for geothermal resources, that there's a more complex supply chain that needs to be mobilized. I think that geothermal will inherently take more time. So, while Cape Station is this marquee event, and it should feel like afterwards things move very quickly, I think we might be disappointed for a few years while we wait and see more projects pretty slowly move to get off the ground. So, I think we're going to want it to be this inflection point kind of event, but I think that the inflection is going to be much slower and happen over the next five years. And that geothermal will really be positioned to take off more so in the 2030s than in the late 2020s. That's my bet at the moment. Yeah, I think that's probably right. The big difference between solar and geothermal, obviously, solar photovoltaic panels are a product. You can put them anywhere. There will be different ambient conditions and they'll perform differently, but you sort of know what they are and the product is the same everywhere. In the geothermal, right, there's all this subsurface risk that you have to mitigate. It's all pretty site specific. So, we've done it in oil and gas. That's the whole concept here. We've found a way to scale hydraulic fracturing across lots of different geologies and lots of different regions. But there as well, it did take quite a while from the first well that was ever fracked, for example. Yeah, and again, I would say one more interesting facet of geothermal, I think, is that its relationship, enhanced geothermal's relationship with the oil and gas industry is also both a positive and a negative. On the positive front, you have this highly skilled engineering and technical field workforce that can very quickly, theoretically be mobilized to support the geothermal industry. There's hundreds of thousands of people around the country that work in oil and gas today, some of whom probably have a keen interest in making a transition into providing cleaner energy and have the skills to do it. So, that's the positive. On the negative side, you also have what at times can be an extremely lucrative industry who can pay those people for their skills. So, I actually think to some extent the timing and the pace of geothermal deployment might also depend on the alternative for that skilled workforce. So, like high oil and gas prices, really rich oil and gas market, harder to convince drillers to go to work in your geothermal field. That's riskier, takes longer to pay off, etc. For extended period, low oil and gas prices and that market's feeling slower and less exciting, you may be able to mobilize more of the existing workforce. All right, let's do one last one on grid enhancing technologies. So, grid enhancing technologies is an umbrella term for a bunch of different things. You can describe some of your favorites within them. Here's the interesting question. I think anytime anybody hears about them and what they can do and what they cost for the first time, and then they hear about the current situation in the power sector, the obvious reaction to it is like, well, that's a no-brainer. Of course, we should do that basically everywhere that we can. It's a cheap way to get more capacity flowing through the lines that we already have, avoiding the need to build new transmission, which is really, really difficult to get done. It has started to happen, but I think it has been frustratingly slow to a lot of people. Why is the question? I guess first describe your favorite gets technologies and then why have they been slow to take off in a macro sense? Yeah, I think the two that make the most sense to people and again seem like no-brainers on paper at times are advanced conductor technology. So basically, this electric conductor's new wires made of different materials that can carry more power using the same footprint of transmission lines. So basically more power with the same or less weight. And if you can carry more power with the same or less weight on basically the same kind of towers that you're already carrying power on today, you can theoretically re-conductor an existing transmission line. Or you can swap out the old conductors for new conductors without changing the height of the towers or anything that would trigger a new permitting requirement or potentially get some of your friendly local nimbs involved and just strictly carry more energy over the same transmission corridor with basically no downside. Sounds great. The other category of gets which I think deserves a lot of the positive attention that it receives and is often also sounds at least on paper like a no-brainer is dynamic line ratings where today transmission lines are rated fairly conservatively for kind of the worst case or near worst case environmental conditions. Oftentimes on a seasonally adjusted basis, so you have a rating in the summer and a rating in the winter. Just to make sure that no matter what at any given time, you can safely carry the amount of power over those lines that you are carrying. Dynamic line ratings allow you to monitor the lines in real time for temperature and the amount of the lines are sagging or the amount that they're blowing back and forth in the wind so that you can rate them dynamically in real time. And potentially in many cases carry more power over that line than your static conservative rating would allow. And so, yeah, again, the question is why have we not seen more rapid adoption of these categories of technology in the past? Number one, I would say, is because we haven't been in the extraordinary demand growth conditions that we are today, even just three years ago. So there wasn't just enough of an impetus in the past that the market is more conducive for grid enhancing technologies today. But, you know, secondarily, I think the reason that we haven't seen gets be deployed as quickly as it sounds like they should be, there's a few reasons. One is the natural conservatism of the utility industry. And, you know, what I mean by that is this is an industry that is accustomed to operating particularly transmission and distribution assets with, you know, minimal intervention, human intervention, as these are big, long, remote assets deployed everywhere. You don't want to have to have operators there all the time. For decades and decades at a stretch, this is high voltage infrastructure that, you know, has immediate acute human safety implications and environmental safety implications. And so adopting any new technology, like this is the most conservative part of a relatively conservative industry. And I think for good reason, it's just very difficult. There's a lot of validation required before they're going to put something into the transmission system that is really fundamentally new in any way. And secondarily, I think because oftentimes there is a lot more complexity in deploying gets than it, than it kind of sounds like in the initial pitch. You know, the grid is this big complex integrated system and just improving the impacity of one stretch of transmission, for example, changes the way that the grid operates in ways that you need to, you know, study holistically. If you're going to change that stretch of transmission line and expect to get higher throughput, you also need to upgrade the substations at both ends. For example, you know, sometimes that might require you to, you know, make other upgrades on the system at the same time. And so my sense is that the adoption cycle forgets is just inherently longer than, then frankly, like I would have liked to have seen it be. And I am hopeful that we're in a different paradigm now because of higher demand, but still expecting that we're going to need to be patient with rollout of any of this technology. Yeah, I think what we've learned in this sector is like almost never is there like a galvanizing event or moment that suddenly everything moves extraordinarily quickly and adoption of new technology occurs overnight. Like it does in some other sectors, to be fair, and there are good reasons why that's not true in electricity. But when something does start, a new technology does gain momentum. It usually has like a decade plus, maybe a couple of decades worth of steam that it can ride on. So the thing for me about Gats is like, it feels like the momentum is building now finally in a way that it hasn't been historically. And if so, then the expectation shouldn't be that like you get a 10x overnight increase in gets deployments, but it should be that you are steadily deploying more and more gets for like a very long time. And that to me seems like it should be the answer. Yes, there's risk and we have to get over the risk, but assuming that everything works as we expect it should, it is an obvious set of technologies to deploy the electric grid period. Yeah, until recently a lot of gets technology has been stuck in what we occasionally call it EIP utility pilot hell, which is a bad place to get stuck. Or if not pilot hell, then really consigned to niche use cases where you have no other option. And it's obvious that gets will solve a very, very specific problem in a specific location. I think we're starting to move beyond that, but again, agree it's not going to be probably similar to geothermal. Like there's just for different reasons. Inflection point does not mean one to 10. It means one to two and then two to three and then maybe three to five and gradually over the course of 10 to 20 years you get to that 10x. All right, Andy, we're out of time. More questions to come. My question for you, Shayla, did you realize that catalyst was going to become also not just one of the top energy podcasts, but also one of the top data center and AI podcasts. Would you have made that that to yourself a year and a half now? You know, it's interesting. I'm constantly trying to self reflect on whether I'm spending too much time on this podcast talking about this topic and like all the, all the little nuances of it and the tentacles that it is drying. But it is that important, I think, and it is that dynamic and it is that uncertain. And so I forgive myself. I'm just saying the fact that you can call to mind on device inference as easily as carbon dioxide removal is a new thing for you personally and for catalyst. It's true. This is what's happening in the world. So we got to figure it out. All right, Andy, we'll talk again soon. Thanks, Shayla. See you. Andy Librshane is the head of research and a partner at Energy Impact Partners with me. This shows production of Latitude Media. You can head over to LatitudeMedia.com for links to today's topics. Latitude is supported by Prelude Ventures. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Marquand. Stephen Lacey is our executive editor. I'm Shayla Kahn, and this is Catalyst.",
    "release_date": "2025-10-23",
    "duration_ms": 2663000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI5220384902.mp3?updated=1761191027",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-11-10T01:56:14.145866"
  },
  {
    "title": "Frontier Forum: The new power map for AI infrastructure",
    "description": "As AI reshapes the industrial landscape, companies are questioning whether the grid can keep pace. Permitting delays, transmission constraints, and reliability risks are forcing developers to rethink where power comes from.\n\nIn this episode, KR Sridhar, CEO of Bloom Energy, lays out a radically different vision. He believes that many data centers will ultimately operate like refineries \u2014 powered by captive, off-grid generation that prioritizes resilience, speed, and local control over traditional grid economics.\n\nSridhar argues that solid-state fuel cells have become an ideal solution to meet data center needs at AI speed and scale. They can be deployed in months rather than years, follow digital loads in real time, and integrate with future zero-carbon fuels like hydrogen.\n\n\u201cI truly believe that this is a cyclical trend that\u2019s going to continue for well over a decade,\u201d said Sridhar.\n\nThis episode features an edited version of our live Frontier Forum conversation about what a future-proof AI power strategy really looks like. We talk about the tension between off-grid and grid-connected approaches, the importance of speed to power, carbon capture, and supply chains over the next decade of growth.\n\nThe conversation also touches on Bloom\u2019s new white paper, Fuel Cells: A Technology Whose Time Has Come, which argues that onsite generation can deliver AI-scale reliability and lower emissions.\n\nYou can watch the full Frontier Forum conversation with audience Q&amp;A here.",
    "summary": "The podcast discusses the increasing electricity demand in data centers, driven by AI expansion. It explores the challenges of aligning grid power with data center needs and the potential of on-site power solutions like fuel cells. The conversation delves into the necessity of digital-age power for AI infrastructure and the shift towards decentralized power generation. The debate between grid-connected and off-grid data centers is examined, with a focus on speed, sustainability, and future-proofing energy solutions. The importance of addressing power supply volatility, sustainability goals, and the role of carbon capture and green hydrogen in achieving a low-carbon footprint is also highlighted.",
    "transcript": " This is a frontier forum brought to you by Latitude Studios. There's a lot of debate about where exactly data center electricity demand is headed, but we know it's steeply up and could account for more than 10% of US demand in the next few years. That's up from 2% a couple years ago. That's an unbelievable leap. And that leap really, if the US has to stay in the AI race, has to happen in the next 4 to 5 years. KR3DR is the CEO of Fuel Cell Maker Bloom Energy. About a third of the company's solid oxide fuel cells are cited at data centers. We're talking hundreds of megawatts with that number growing quickly. So KR knows the industry well, and he's never seen anything like the current AI infrastructure boom. It's the speed and the scale. I truly believe that this is a circular trend that's going to continue for well over a decade. Data centers are now being planned and built faster than the electricity system can handle. That has sparked another debate about the role of on-site generation, and whether data centers will be built off grid. Now some folks argue that co-located behind-the-meter resources will be rare, and that the smartest economic move for data centers will be to connect to the grid, not walk away from it. KR believes deeply in the importance of the grid, but he also thinks that on-site solutions, including fuel cells, will be critical for building infrastructure at the pace of AI expansion. What the centralized power plants and the electric grid did to humanity in the 20th century is nothing but unbelievably amazing. That model still has a place because electricity is the ultimate perishable, and having that large flywheel called the grid is very important. So it's an and not an or. However, when you go to data centers, here's what happens. There are many misalignments between data centers and the grid. A data center consumes DC power, the grid sends AC power, and the quality of the power on the electricity system, which shifts depending on loads turning on and off and changes to the generation mix, is not anywhere near what a data center can accept. What do you do? You put all these band-aids, make it high quality using UPS quality power. Step it down from high voltage to medium voltage to low voltage, all the transformers, all the power conversion devices. They're all band-aids because they are taking a mechanical-age electricity and trying to adapt it to a digital-age computer. A fuel cell is a solid state power device that KR says is purpose-built for a digital facility like a data center. He compares today's power strategies to trying to build a faster horse and buggy when we really need a car for the AI era. When you use Bloom, you have a one-step process. When you use a conventional combustion technology, you use a six-step process. The horse is consuming a lot of water, is consuming a lot of hay, and a lot of other things to walk the mile. We consume very little to walk that mile. The downside generation will be critical for winning the AI race. So we did a survey two years ago, and now, and 18 months ago, the survey, you know, the respondents said, 1% of the survey respondents said they will use on-site power that is not connected to the grid. So sentiment is changing enormously simply because of lack of availability of the power and understanding that we have to use this. So this AI movement is forcing data centers to finally say the digital-age needs digital electrons coming from a digital source. I think that's a good idea. So this AI movement is forcing data centers to finally say the digital-age needs digital electrons coming from a digital source. I think once they get used to this and understand all the advantages of this digitally purpose-built on-site power, they are not going to switch back. Let me take the other side of that. People who believe that most of these data centers will be grid connected would say the grid offers scale and optionality that no off-grid system can potentially match. It's much more cost effective. You have to overbuild the system for an off-grid data center. And there's a lot of grid capacity left that you just need to get creative to unlock that grid capacity. What do you say to those arguments about why off-grid data centers may not blossom in the way that you're painting? I'm going to surprise you by saying that I would agree with those arguments for a very simple reason. If you have excess capacity in the grid and you can get that without putting that on your balance sheet, why would you be stupid enough to put that on your balance sheet and take that responsibility yourself? Absolutely, yes. That's exactly what all the data centers have been doing so far. But here is why the commons, which is the US taxpayer and the US government, build this excess capacity in the grid that was paid for by the ratepayers. One way or another, either through taxes or through their bills. That grid had excess capacity, so the data centers were able to use it. Now if the grid were to build excess capacity purely to meet this huge demand and expects the taxpayer to pay for it, so the hyperscaler can benefit, that politics is never going to fly. So the idea that somehow data centers will go and build their own on-site power, if the grid were available, is a fallacy. The notion that the grid, the commons, will somehow build all this capacity, so one or two users can use all that capacity at the expense of everybody else, is also a political wish, which I don't think will happen in reality. And then what about just speed in general? How much is speed to power, the phrase of the moment? How much is that going to drive the off-grid approach? That is going to be the key. The truth is this is a true race. It's a race between corporations trying to get supremacy. It's a race between the western world and the rest of the world competing for AI supremacy. One thing that everybody will agree is being number two or number three is not an option. Speed is going to be everything, and there it's just simple knowledge for people who understand the grid, that you cannot build the grid at that speed no matter what you do. The common refrain is if only permitting and permissions go away. Yes, that is a big, large issue. Imagine somebody waves a magic van, like one, the king, and I can build high-voltage transmission lines straight over your house and you have no right to object to it. So what? What's the amount of copper needed? What's the amount of transformers needed? What are the leads times for it? Where are you going to get that from? Where are the skill sets to build these things? How much capital is required? How fast will that happen? Realistically, over a five, six, seven-year period, it cannot catch up with the demand base that exists out there. That doesn't mean that we shouldn't upgrade the grid. That's why I started off saying the grid has a very important place. It's an all-of-the-about strategy. We should be investing heavily in optimizing the grids, the things that you talked about. Can we use AI to make the grid better and extract more out of less? Can we make it more efficient? Can we figure out other ways of optimizing it? Can we add more transmission? Can we add better technologies to remove the bottlenecks where they exist and get more out of the grid? The answer is yes. But is that the solution? No, it is part of a solution. Tell me about what other industries you look to where this is a model. Are there large industrial loads that you see that have accomplished this? When I look around, there is not a single industry that builds factories that have very high power demands that doesn't have capital power. Let me walk you through it. You take metals, whether it's a steel mill or it's aluminum. If you go to an Alcoa plant in Indiana, they'll have a capital power plant that's a few hundred megawatts. Because that kind of load, they cannot depend on the grid. If they were to do a shutdown for maintenance and you suddenly drop hundred megawatts to two hundred megawatts off the grid, what happens to the grid? You cannot do that. It has to be capital power. You go to cement, large cement plants, wholesome. If you go and look at it, they will have their capital power plants. But then you go to refineries. Same thing. But here is what happens. Here is another reason why they do it. It's not just because the load is so large. Number one, you get the reliability and resiliency you need when you have a capital power plant and not when you depend on the grid. These are continuous processes. If they shut down for whatever reasons that happen in the grid, that cost of in-process inventory, everything is too high. Number one, reliability and resiliency. Number two, there are synergies that come from keeping that capital power plant right where you use the power. Here is why. A refinery, for example, will use the off-gasses that are coming out as a fuel into their power plant and they can get better economics. A steel mill, a cement plant, same thing. Then you look at paper mills. Paper mills do the same thing. You go and look at chemicals, pharmaceuticals. It's the same. Anywhere you have seen an extremely large load in a factory, it's a capital power plant. We are finally building high electricity intensity AI factories. These data centers are AI factories. They have been the exception and not the rule. They are now going to have to follow the rule. I definitely take your point for the all-of-the-above approach. You've said that this is not a winner-take-all market. If we apply that to the actual groupings of on-site technologies for these data centers, what do you expect to see when we look at fuel cells, gas turbines, renewables, eventually nuclear? How do you imagine them all fitting together? What kind of system designs are you currently seeing? It'll be all of the above and more. New things that come along. What I would say is microgrids, supporting such infrastructure, will definitely have a multiplicity of generation sources. Each one comes with very different attributes. You may need particular attributes depending on what you're in need. As soon as you start purpose building for your needs, you're going to exactly tailor it for the needs that you have. You can imagine for some extremely peak loads, there being some engines or turbines. For a steady base load, there being a fuel cell because we'll be the most efficient from a fuel perspective and not create pollution. Zero pollution. That's super important for an AI factory because unlike a refinery, unlike a cement plant, which will, by definition, only be located far away. Inherently, they create so much pollution that having just a captive power add to that pollution is de-minimous. Unlike that, AI factory being built in a population center, especially for inference, cannot afford to have air pollution. That becomes an extremely important aspect. Then you got the load swinging up and down. For that, you're going to have a variety of storage technologies. From batteries to flywheels to ultra-caps to super-caps, they will all become part of the microgrid. How you adapt it, what particular technologies you use will depend exactly on what kind of a data center it is. What does its load profile look like? It has to be tailored and met perfectly. I think we are in the dawn of a reimagining electricity and how it is delivered to a customer. That brings me to another question. It's kind of a two-part question. One was on the differences between training and inference workloads. Is it possible that there is even more upside to growth than we previously thought? Based on historical data, including our conventional CPU data centers. If you go back to any form of human use of a new technology and new tool, what you see in history is, while things start in a very centralized fashion far away, ultimately, the value is only created and the growth comes when that technology comes to the edge, closer to where people are, closer to where machines are, closer to where people are. That is the predominant use. That applies to computing. That applies to telephony. Everything else. What do I mean by that? Today, most of the AI load we are all focused on and are talking about is the hundreds of megawatts to a gigawatt kind of a data center for training. While the total capacity of that will keep increasing over the next eight to ten years, in terms of percentages, while today the training is 90% of the AI load and 10% is inference, or somewhere in that neighborhood, three to four years from now, I predict that 90% of the load will be inference and 10% will be training. Even though the 10% in training then will be significantly larger than the 90% training today. That's what I mean by proportionality. They'll both grow, but proportionally, this is where it will be. The inference data centers for AI, unlike the CPU data centers of today that are one to ten megawatts, typically on the edge, are going to be somewhere between five to 30 megawatts on the edge. If you live in Austin, if you live in Dallas, if you live in Chicago, if you live in Manhattan, if there are hundred inference data centers that need to come, that need 10 to 30 megawatts of load, God bless you if you went to your local utility and asked them for that load, and they're able to provide that to you. If you thought transmission was a problem, wait until you figure out what distribution is. That's the surface streets, right? Transmission is your highways. If you think the highways, congested, wait until you get to the surface streets to see what the surface streets are doing, right? So you need on-site power for that. So I completely agree with Arman that that's going to be a huge growth area and the beauty of our technology, which is modular, scalable, with no compromise in efficiency or performance or reliability, whether you do it in five megawatt chunks or 50 megawatt chunks or 500 megawatt chunks. That advantage that we bring, again, is the car unlike the horse. What are the other pain points you're seeing from customers when it comes to power supply and delivery? Are there any others that we haven't touched on that you think are particularly acute? Yes. The data center customers, the AI practitioners know this extremely well. The AI load, unlike the CPU load, is very volatile. It has milliseconds, seconds, and minutes variation that go from 10% of a load to 100% of the load. So you're talking about, if it's 100 megawatt data center as an example, you're talking about the load swinging from 10 megawatts to 100 megawatts up and down. And some of those variations happen in millisecond frequency. Some of them happen in seconds frequency, some in minutes frequency. And the amount of backend band-aids that they are putting to deal with this, as well as the harmonics it creates because of converting AC to DC, have all become very large pain points. Number one, with our architecture of being able to provide DC, it becomes easier. That's one pain point. The second pain point that they are working on as we speak is shifting the mindset to a 800-volt DC bus going into Iraq. Let me explain this to you. When you are a large power plant that produces 50 megawatts, 500 megawatts worth of power, because so much power comes out and think of it as a pipe, the wire as a pipe. Think of water flowing in it and think of the total power coming in as the amount of water, the mass of water coming out. You can either increase the pressure or increase the diameter to increase the flow. Because copper gets very large, if you keep it at low voltage, they only bring it out at high voltage. You have to convert them all. And that process in our system is completely operated because we can natively provide 800-volt DC going to the grid. We think that's where the world will go. So that's a huge thing that is a pain point, but we have an elegant solution to solve that problem. The third pain point that they see is cooling. The amount of cooling load necessitates that it's a liquid cooling that has to happen in the data center of Iraq's. And there again, we have an elegant solution for them. So when we look at the needs of customers, what they're looking for, do you see any tension between the speed to power and sustainability goals? And do they have any concerns about a gas strategy? I would say for pretty much all the data center operators today, the sustainability goal has shifted from being in the top three to being in the bottom three. However, if you offered them a pathway to, I can give you the quick power today, but I can future-proof you, and I can give you add-ons that get you to net zero or extremely low carbon footprint. They're super interested. They still want to do it, but they will not do it at the expense of waiting for power. Okay, that's where the sentiment is. That's where bloom plays really well. Because we consume less fuel, with natural gas, we have a lower carbon footprint compared to any other on-site generation technology. And without natural gas, we are not going to be able to power the air revolution. That's a fact. Okay, that's a fact. And so we are lower comfort. However, because of the uniqueness of our technology, and your viewers can go watch that on our website. There's a very good explanation of carbon capture. We put out a very clean stream of carbon dioxide coming out. So our users can capture the carbon dioxide coming out of our system and sequester it. And I think the world will get to that place very soon. So that's one way of zero carbon. Should a green molecule become available in the future? Hydrogen, pneumonia, RNG, biogas, you name it. And it be available in large quantities. Our systems are already pre-wired for that. You don't even have to tell us. You can feed that gas in and we will happily accept it and produce electricity. So we are future-proofed. So in other words, think of us as we are giving you a 5G phone now, even though you don't have 5G in your neighborhood. But you don't have to wait for 5G to come to your neighborhood to use it. You can be using your LTE or 3G right now. And when 5G comes, it automatically switches over. Our systems are built for a zero carbon molecule. But that molecule, when it comes, the customer can switch, but they don't have to wait for it to come. And I remember when Bloom first emerged, there was a lot of talk about the potential of using green hydrogen in these fuel cells. And you know, green hydrogen has been very difficult and expensive. There's limited supply. And you talked about carbon capture, this partnership with Chart Industries, I believe, which comes first. Carbon capture at scale or green hydrogen? Carbon capture at scale, hands down for electricity generation. So it depends on what you're asking for, right? The green hydrogen going into green harmonia, coming from certain places, going to produce fertilizers and other hard to abate industries. That would make sense. It would not make sense to make a green ammonia and use that for electricity generation. Because the world is short on electricity. Green hydrogen is produced by bottling electricity. Why would you bottle something when you have current shortage? You don't save money if you can't feed yourself today. Who is asking for carbon capture? Are they corporate customers with sustainability goals? Are there other reasons? Is it primarily the folks who have clean energy targets and are now burning more gas? Look, I think most corporations want to be responsible. You know, nobody, I can't, you know, I've not met a CEO yet who's waking up in the morning saying, I want to add to climate change. Okay, that's not the goal of any corporation. So when we tell them that we have this solution, they say as long as it meets the economics, you know, they're willing to pay a premium. As long as it meets the economics and it can be within scale, if you don't slow me down, I'm willing to buy that solution from you. That we've heard across the board from everybody, across the board from everybody. So what we're seeing is every data center customer is super interested in our carbon capture solution. If they are deploying that solution in a region where the geology is suitable for sequestration, they're all interested. Now, what they also want to speed and the solution we offer is carbon capture and sequestration is a bolt on to our technology. We will provide you the power of the lowest carbon footprint today. Two years from now, when everything else is ready and available like the classic swells, all the pumping, all the technology with chart ready to be implemented, we will bolt that on for you. And they're very happy to sign on to a solution like that. We are, of course, in a wildly different trade environment now. And I know that you had worked over the years to move your supply chain away from China. Can you talk about your advantages and any risks you have in your supply chain currently? And, you know, are any of the major policy changes changing the way you're procuring materials and equipment? So, when I started Bloom 24 years ago, the idea was very simple, right? Centralized systems with centralized directive managed by very few is a command and control, communistic kind of approach. The distributor is democracy. So, we were building a democratic electrical system with a distributed system. That was an objective. So, for that reason, that thought process permeated through everything we did. We said, we want to build a supply chain where no one single region can solely choke a supply chain for growth. So, we only pick materials. We only picked processes. We only picked supply chain partners. As long as we could geographically diversify across the board and not create any tensions in that system, because the goal of the company was one day to be able to provide this around the world and create energy abundance. It was built at the very loft ego. So, therefore, we set it up that way and never constrained ourselves to any one country from the day we started. Today, when we look back, that was a very good strategy. From where the world is going and the interdependence, and that interdependence becomes unhealthy if that interdependence is on only one person and it's a one way. That's not interdependence anymore. And we don't have that. Number one. Number two, the question you asked, unlike other energy generation technologies, if you take turbines as a class, engines as a class, you go to the second, third tier suppliers, you will find common bottlenecks. Unless that second, third tier is able to grow, the rest of them can grow. It's only a matter of allocation of between the people who consume it, who's going to get more and who's going to get less, even as they grow. We are decoupled from that supply chain because of the completely different way that our technology works. And if you think of copper and transformers and things like that, because we natively produce DC power at the right voltage, we are just eliminating the need for a lot of those materials that are in short supply. So if you're trying to take the limited amount of materials available and say, where can I get the most bang for my buck? You will see that our technology and our supply chain and how we think about that entire value stream will provide a lot more power with the same amount of copper compared to any other technology. And are you seeing sufficient gas pipeline capacity? The gas pipelines and the availability of gas is there. In fact, as you very well know, we are exporting a lot of LNG out of the country. So clearly we have excess of that. Now, in terms of the pipeline coming to exactly where your location is from the medium pressure pipeline to the last mile distribution, there could be issues there in that there in terms of adding that infrastructure, depending on what state you live in. It can be a matter of weeks or it can be a matter of years. If you happen to live in New York City or the state of New York, two years ago, where it was completely anti-pipeline, that would be very hard to get. But in other states, it's very easy to get. But I think that's all changing. That attitude is changing, getting the reality of this is the cleanest way we can do it and better cannot be the enemy of good. Let's talk about customers a little bit. You've deployed a lot of capacity with customers like Equinix and AEP. You've been to deal with Oracle to deliver onsite fuel cells for cloud data centers in, I think, only 90 days. And that's unthinkable when we think about traditional grid connections. What are customers asking for? Are they currently supplementing with grid power or are they truly looking for these onsite off grid solutions? All of the above. Anytime a customer can connect to a grid, they would want to and we would recommend that they connect to the grid because that's insurance for almost nothing. Why would you not take it? It's an option. Why would you not take it? So, wherever the grid is available, even if we are primary power and we have all the reliability, it costs almost nothing compared to the grid. And nothing compared to the entire data center spend to be able to get that connection. But when that connection is not available, our solution offers a highly reliable option where they can go without it compared to any of the solution out there. So, today I would say we have a mix of both traditionally over the last few years if you looked at it, even though we were the primary source, even though we were mission critical. In many cases, they were connected to the grid. But we have an example, 2013 when we did eBay, of having run that data center without losing power even once for over a 10 year period of islanded solution where we were the only solution. The grid was connected, but it didn't have the capacity that they needed, bloom offered that capacity. So, for us, islanded mode, that's what the system was designed for. It's very easy. It's not an afterthought. I want to turn now to the actual use of AI to model and optimize these systems. Can you talk about how you're using AI for digital twins and what that does to the actual performance of the systems and how you're modeling them at these large data center sites? That's a great question. So, if you just think about how we planned it, and when we look back, we're actually very happy that we did it that way. Each one of our Lego block modules, I'm sure your viewers have seen what a bloom power module looks like. They look like big refrigerators. They produce about 65 kilowatts. And there are 64 little fuel cell stacks sitting inside. Think of them as your chips. Every chip is wired and it's providing its health data regularly to us, wherever in the world it is. That's how we did it from the day we shipped our first system in 2008. So, our 1500 megawatts or 1.5 gigawatts worth of install base is shipping us billions of data every day on how it's performing. Very lately, we started calling them digital twins because the world created that terminology. So, we have a digital trend for every fuel cell stack that goes in every system, feeding us data. We used heuristics and machine learning and other knowledge to be able to extract and learn from it. And that was part of our innovation cycle. Every system we installed was also a lab experiment for us, giving us data, allowing us to learn and become better. We are a learning organization. Now, look at what AI has done. With that data today, not only can we learn what is working and make something better, we can probably real-time manage the performance of each one of those little refrigerators, slightly different from the next one. Just customized for what it can do and extract more out of it. I think it's a phenomenal opportunity and we are all over it. I want to go back to the urgency that you expressed earlier about why we need to do everything we can to dominate the AI race. So, talk about the consequences of not scaling fast enough. How do you see the US stacking up globally and what are the positive outcomes if we can get this right? So, when you look at the AI race, it's not just about intelligence and information and number of road manual processes we do today that can be automated and therefore improve productivity. If you just take that productivity gain alone, we can improve our GDP growth by a couple percentage points. That would mean thousands of dollars in the pockets of every average American, every average person that lives out there. So, economically, just imagine what it creates from a GDP growth and the trillions of dollars it puts into our economy and what does. So, that's one simple way to think about it. The next thing to think about is not just business processes, but we are living in a sensor ubiquitous world. Every physical device has sensors today and it's affordable to put these sensors, IoT. The ability to use IoT properly using physical AI and how to make everything perform better become more predictable last longer. Changes every aspect of every industry. Whoever wins this race is going to dominate in manufacturing, is going to dominate in materials processing, is going to dominate then using all that information in the speed of R&D for the data. So, I think your viewers will agree that if we lose our technology superiority, we will lose our economic superiority. That hinges on AI. Now take another layer. National security. Wars are going to be fought and won by AI superiority, not by boots on the ground superiority. So, it's a national security issue, it's an economic prosperity issue, it's about who controls the numbers issue. Now, the world pretty much is between a free world and not a free world. Who wins this race is going to dictate which way the rest of the world is going to be dominated by. And that's a scary thought. If you think we can be number two. It's absolutely scary to me that we can be number two. So, that's where I would say policy notwithstanding. Doesn't matter who is in Congress, who is in White House, there'll be one thing that every government is going to agree is we cannot afford to not win the AI race. K.R. Sridhar is the CEO of Bloom Energy. What a moment we are in. Thank you so much for sharing your thoughts. Thank you. It was a pleasure. K.R. Sridhar is the CEO of Bloom Energy. To learn more about how fuel cells and on-site power can help your business stay ahead, read Bloom's 2025 white paper by clicking the link in the show notes. And to find out more about how Bloom is bringing fast, reliable power to data centers in the AI era, go to bloomenergy.com. This is an edited version of a frontier forum recorded in front of a virtual audience. We took lots of live questions from the audience and there's a ton of technical detail on Bloom's fuel cells. So, if you want to go deeper, you can click the link in the show notes to watch the full video at latitudemedia.com.",
    "release_date": "2025-10-21",
    "duration_ms": 2164000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI3758108092.mp3",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-11-10T01:59:13.061112"
  },
  {
    "title": "Calibrating hype with Akshat Rathi",
    "description": "In the climate space, every idea sits somewhere along the hype continuum. Some command outsize attention. Others fly under the radar despite big potential. And a rare few hit the sweet spot, earning exactly the buzz they deserve.\n\nBut how do you tell which is which?\n\nIn this episode, Shayle teams up with Akshat Rathi, senior reporter for climate at Bloomberg News and host of the Zero podcast, to sort it out. Akshat and Shayle run through a list of hot topics and place each one on the hype continuum. They cover topics like:\n\n\n  \nUsing DERs to meet load growth\n\n\n\n  \nCo-locating generation with data centers\n\n\n\n  \nInfrastructure bottlenecks like generation, transmission, and transformers\n\n\n\n  \nThe roles of venture capital and the Paris Agreement in shaping markets\n\n\n\n  \nA grab-bag of other topics like sodium-ion, advanced geothermal, and advanced nuclear\n\n\n\n\nResources:\n\nCatalyst: The new wave of DERs\u00a0\n\nCatalyst: When to colocate data centers with generation\u00a0\u00a0\n\nZero: The Device Throttling Our Electrified Future\n\nZero: The Gas Turbine Shortage Might Be a Climate Problem\u00a0\u00a0\n\nCredits: Hosted by Shayle Kann. Produced and edited by Daniel Woldorff. Original music and engineering by Sean Marquand. Stephen Lacey is our executive editor.\u00a0\n\nCatalyst is brought to you by EnergyHub. EnergyHub helps utilities build next-generation virtual power plants that unlock reliable flexibility at every level of the grid. See how EnergyHub helps unlock the power of flexibility at scale, and deliver more value through cross-DER dispatch with their leading Edge DERMS platform, by visiting energyhub.com.\n\nCatalyst is brought to you by Bloom Energy. AI data centers can\u2019t wait years for grid power\u2014and with Bloom Energy\u2019s fuel cells, they don\u2019t have to. Bloom Energy delivers affordable, always-on, ultra-reliable onsite power, built for chipmakers, hyperscalers, and data center leaders looking to power their operations at AI speed. Learn more by visiting BloomEnergy.com.",
    "summary": "The podcast discusses various aspects of the energy transition, focusing on topics like gastropines, AI, fuel cells, distributed energy resources (DERs), co-location of generation with data centers, bottlenecks in grid development, generating capacity, transmission challenges, and the importance of transformers. The hosts engage in a game show-style discussion with a guest to determine if these topics are overhyped, underhyped, or just right in terms of meeting load growth challenges. They explore how these factors impact the energy industry globally, highlighting issues like long lead times for gas turbines, grid connection delays, and the significance of transformers in powering various sectors.",
    "transcript": " Latitude media covering the new frontiers of the energy transition. I'm Shail Khan and this is Catalyst. I have been reporting on gastropines, which are currently, as my editor put it, the labubu of climate solutions. That's the first. Very high demand. That seems right. Very high demand and there's a year's waiting list before you can get the one you want. Coming up, just a whole bunch of things with Akshay Rathi from Bloomberg. The AI boom is here, but the grid wasn't built for it. Bloom energy is helping the AI industry take charge. Bloom energy delivers affordable, always on ultra reliable on-site power. That's why chip makers, hyperscalers, and data center leaders are already using Bloom to power their operations with electricity that scales from megawatts to gigawatts. This isn't yesterday's fuel cell. This is on-site power built to deliver at AI speed. To learn more about how fuel cells are powering the AI era, visit bloomenergy.com or click the link in the show notes. Emerging electricity demand is testing the limits of the grid, but Energy Hub is helping utilities stay ahead. Energy Hub's platform transforms millions of connected devices, like thermostats, EVs, batteries, and more into flexible energy resources. That means more reliability, lower costs, and cleaner power without new infrastructure. Energy Hub partners with over 120 utilities nationwide to build virtual power plants that scale. And how the industry's leading flexibility provider is shaping the future of the grid, visit energy hub.com. Clean energy is under attack, and it's more important than ever to understand why projects fail and how to get them back on track. The center left think tank Third Way surveyed over 200 clean energy professionals to answer that exact question. Their newest study identifies the top non-cost barriers to getting projects over the finish line, from permitting challenges to nimbism, to read Third Way's full report, and to learn more about their pace initiative, visit thirdway.org slash pace, or click the link in the show notes. I'm Shale Khan. I lead the early stage venture strategy at Energy Impact Partners. Welcome. All right, so this week is a fun one. My friend, Akshat Rathi, is a senior reporter at Bloomberg News and also the host of his own podcast called Zero. So he covers climate and energy stuff at Bloomberg. And he has an interesting different perspective in the world from me in that he won, is more plugged into this sort of high-level government stuff. We talked, for example, in this podcast about Paris, the Paris Accord, which I don't think I've really ever talked about on this podcast before. And also he has a more global perspective. He's based in the UK himself, but he's also looking at global dynamics in a way that I'm admittedly, and I've said this before, always a little bit to US-centric or US and Europe-centric and things that I tend to see and talk about. So he and I got to chatting and thought it'd be interesting to compare notes. So we do it through a fun game show style approach, and we covered a whole bunch of interesting things. So this is a fun one. Here's Akshat. Shil, welcome to Zero. Thank you, Akshat. Welcome to Catalyst. Well, this is a different kind of podcast, and we're making it across an ocean, but with similar politics in the background. Yeah. I have a question for you. You're in London, right? Yeah. Okay. So, and you work for Bloomberg Green, and one thing that I have found is that in the US, this is where like green or clean or climate is in their name, have been having an interesting set of discussions around nomenclature. Is that true for? I mean, Bloomberg is obviously based in the US, but so I'm curious how you see it from the Bloomberg perspective, high level, but also in the UK and in Europe. Yeah. So Bloomberg News is a global news organization, but yes, it is US headquartered. I would say there isn't that much muscling of terms as it's happening in the US government, but there's certainly a backlash on the politics here. So there is a party called the Reform Party that's on the rise over here doing in the polls better than the main two parties, labor and conservatives, and they are against net zero. So we are usually a step behind America, but then eventually catch up. Well, in this case, I'm not sure I want you to catch up, but we'll see what happens. All right. So this is going to be fun. What's our plan here? So we're going to do something different where we're going to go through a number of topics that our producers have selected, where we're going to say whether we think it's underhyped, overhyped, or hyped just right and explain our positions for why we think so. Let's start with meeting load growth for electricity, which is a hot topic everywhere, but specifically within that, what do you think about distributed energy resources? What are they and where do you land? Right. So distributed energy resources is like a grab bag category of a bunch of different things. I would say the way I define it is anything that you deploy at a customer site, so it's sitting behind the meter, the customer side of the meter, that either generates energy, stores energy, or shifts load. So canonical examples, generates energy, think rooftop solar, stores energy, think power walls and batteries, home batteries, batteries and buildings, shifts load, think smart thermostats, right, where you can shift it. And there's a broader array of things that fall within that, but they get grouped together and is distributed energy resources. On the question of DER's for meeting load growth, which is a question here, is it overhyped, underhyped, or hyped just right? I think my argument here would be in the circles that I travel in, which are pretty like wonky energy tech circles, it's becoming increasingly hyped because we're just starting to see this wave of announcements that are inexorably coming, where there are going to be partnerships between hyperscalers or data center operators and third parties who do aggregations of DER's to deliver capacity to get data centers on the grid and things like that. That's coming, no question. And so it's starting to bubble up into the hype world. But I think that is a, my world is a pretty small portion of the world. And broadly, I think even in data center universe, it has not really been considered very much yet. Obviously, it is we're going to build a data center. Can we get grid capacity? If not, can we build a big gas turbine? And so my argument is because of all that, it is still underhyped as of today. Yeah, that's an interesting perspective. I would say in my world, which is a little bit broader because I talk to policy folks and I talk to big businesses and I talk to VCs, is that it actually used to be more hyped because it was one way in which everybody was justifying having all these big renewables goals being set up because, look, don't worry about it. I know it is variable, but we'll have these technologies that are just going to come online within the next few years and they'll make it easier to manage this variability in renewables. At that time, it was pretty hyped. Now I would say it's underhyped because it's not something people talk about that much, but those solutions are actually starting to bubble up. So we had the CEO of Octopus Energy, which is the largest utility here in the UK on the pod. And this year they've launched a BYD lease program where you can just pay monthly to get a BYD electric car and they would give you 12,000 miles of free range in a year as long as you make sure that you put your car into the charging port whenever you're home because they'll use the car for a virtual pop-lant essentially. That sort of thing is now becoming very frequent. I get text messages from Octopus Energy saying, hey, 12 PM to 2 PM today, free electricity, use all you want. And so I feel like now those technologies are here and they can actually be put to use. And so they're kind of underhyped because you could actually now see those things out there. Yeah, I think that DERs are a second. They're in a second wave now, right? I've talked about this before on the pod, but there was a wave of excitement around DERs, which I think is what you are referring to maybe a decade ago. And at that point it was like, oh, we're going to decentralize the grid and maybe this is going to disintegrate utilities and none of that happened. But I think what is interesting that's different is the frame of the question here, which is is it overhyped or hyped just right for the purpose of meeting load growth? And that was not a thing we cared about a decade ago when DERs first emerged. So I think this second wave that we're seeing now DERs as a solution to managing load growth is interesting. It's one of the things that makes it more attractive now alongside a bunch of other factors. As you mentioned, like the rise of EVs and EV chargers as a significant DER source of capacity is another one. Okay, so we're born on the same side here. We both think it's currently underhyped. Let me ask you another one that I've talked about a little bit before in this pod, which is putting co-location with generation. So basically putting generation largely gas on site with data centers is that as far as meeting the load growth challenge, do you think that is overhyped under hyped or hyped or hyped just right? I think it is hyped because I have been reporting on gas turbines, which are currently, as my editor put it, the labubu of climate solutions. That's a very... I get it. That seems right. Very high demand and there's a year's waiting list before you can get the one you want. And so sure, in principle, if you could get a gas turbine, you would build it close to where the data center is, but can you even get a gas turbine? Yeah, there is that. So you think it's hyped just right because it is a thing that would happen a lot, but the supply chain bottleneck is the rate limiter? I think that is generally true. I do think, as I've talked about a little bit before on my pod, I think it is a little bit overhyped, the degree to which not to say that we don't need a ton of new generation and not to say that we don't need a ton of new capacity on the grid, but the degree to which that capacity needs to be co-located with the data center to me is a little bit overhyped. We will see it happen. It's already happening in some places, right? You're seeing this in the US. We have the XAI Colossus data center has a bunch of gas turbines, Metas building a big facility that's going to have a bunch of gas. And a number of these projects will, but I think it's going to end up being the minority of new data centers, not the majority of new data centers. I think the implications from what you would see in the news is that every new data center is going to come equipped with a bunch of gas turbines. And not to say that we won't build a ton of new gas generating capacity. I just don't think it's all going to be co-located with the data centers. Yeah. I mean, the reason we are bringing this up is because building the grid has been so hard, right? The goal would be with co-location. You don't have to wait for a grid connection to come through before you can build your data center. And sure, if you can't pay really high prices for the gas turbines, which some tech companies can, then you might be able to do it, but most places won't. It's also there's an interesting embedded question in that too, which is, at some point will costs matter. Well, no, actually two questions. At some point will costs matter and at some point will uptime not matter quite as much, right? Because if you have the same uptime requirements for your AI data center as cloud data centers have historically and you want to front run your grid connection by installing generation, you can't just build gas turbines, right? You have to build a lot of infrastructure because you have to have really, really high reliability. So you're going to build your gas turbines. You're going to build, you're going to oversize some batteries. That's going to be really high CAPX. You might add additional layers to your UPS system. There's a lot that you would build and that comes at a real cost. Where we are in the cycle right now, the reality is that the total embedded energy cost in compute is small compared to the CAPX of everything else, but mostly the chips. So maybe energy cost doesn't really matter, even if you're deploying a bunch more CAPX into that stuff. But there's an interesting question of will that ultimately matter, which would dictate the degree to which this equation of speed to power versus cost of power shifts a little bit. I think inevitably it will shift. I just don't know when that is. There's also the chance that these guys would go and build the power capacity where the grid connection is available, which there are many parts of the world where there is grid connection available. So yeah, I think it is a little too hyped with co-location for data centers. Some will get it, but most people won't. Let's come to the next topic, which is following on from trying to build the grid. Here at zero, we've done a series called bottlenecks because we are seeing these bottlenecks to build the grid, especially in Europe and North America show up in so many places. There's so many bottlenecks that it's not just about permitting and politics, but about actual objects. So where is generating capacity on your hype meter? On the hype meter as a bottleneck, right? So the question is basically, are we overly, like overhyped would mean we're too worried about it, underhyped would mean we're not worried enough about it. I think that the long lead times for gas turbines in particular is very well established and reported and understood by everyone, which I guess argues that it's hyped because people do appreciate that that is a real challenge. There also though has been a lot of good data to suggest that generating capacity in the next few years is not the dominant bottleneck. There is enough capacity in enough places to deliver the kind of load growth for the next few years that the grid will need. So I think this will get to the next one where I think there is a real concern, but I would say generating capacity to me might be a little bit overhyped. The queues are long if you want to build new gas turbines, but I don't think that is going to be the rate limiter on load growth, at least through like 2030. I think that is right. We spoke to a utility CEO, the next-year CEO in the US, who said, if gas turbines are going to be that expensive, that's just going to make solar-pillers batteries so much more attractive and we'll build those instead. So there is certainly no reason to think all forms of generating capacity is in a bottleneck and so all sorts of other things could be built. But there are other challenges in bottlenecks which are much more severe to me that we need to worry about. So that maybe gets to the next one, I think, which is transmission. Deliverability basically is the way to put it. Do you think that is overhyped underhyped as a bottleneck? I think it's super underhyped. Let me give you a European example, which is that the Netherlands has 12,000 businesses waiting for an electricity connection. That company is like ASML, which is the Netherlands biggest company, most valuable company, isn't able to build extra factory because it cannot get an electricity connection for the next few years. And is the Netherlands in the same situation as the US where we just basically stopped building new transmission lines years ago? I mean, not zero, but effectively zero for the past few years. Is that the situation in the Netherlands or in Europe in general? Or is it there? No, it's not that severe because a lot of European countries are interconnected and they have been building more interconnections between them. It's more like there are places around Europe where there is more generating capacity and there are demand centers which are typically further away from those generating capacity that aren't getting the power supply as it's needed. So the transmission hasn't been able to keep up with the demand that is showing up. And I don't think maybe I should know better, but I don't think of the Netherlands as having been a big data center hub historically. Do you know? No, I don't think so. I mean, it certainly has some industry. A lot of it is oriented towards electricity because it's high end manufacturing. But no, I don't think it's a big data center hub. Yeah, which I mean is even more concerning if you think about the context of there's this big long queue to get grid connected in a market that hasn't yet seen the boom in very large load interconnection requests that we're seeing in lots of other places. There is the aspect of DER, like distributed energy resources that is affecting the grids in Europe. So Netherlands has the highest per capita rooftop solar in the world, which for a European country would be like, oh, that's odd. But that's what you get. You get cheap solar panels and you get some policy available and you'll deploy a lot of solar panels. But it's causing real problems at the transmission and distribution network locally because the grid has been built over the last 100 years and many of the objects of the grid are much, much older. So that brings us to the next topic, which is transformers. Transformers are this object that's it between generation between a power plant and your home because they have to make sure that the voltage at which power is delivered is just right for the devices that you're going to attach to it. And where do you land on whether transformers are over hyped under hyped just right height? So again, if the question is over hyped under hyped as a bottleneck to load growth to meeting load growth, then I think that they're over hyped. I want to clarify, transformers are incredibly important and there is a bottleneck and the lead times are very long. And I've witnessed it firsthand both from the utility side and the load side. It is a problem. I don't think it is the rate limiting factor even today. With long lead times, it's an annoyance to get anything built. Things will still get built and certainly those lead times for transformers are still shorter than they are for gas turbines. But I do think we also will see a lot of new transformer manufacturing capacity come online in the next few years. The lead times are going to come down and I think we'll see a way of new technology. I'm an investor in a company called Heron Power which is building solid state power electronics. I think stuff like that is going to revolutionize that sector. But even in the absence of things like that, I don't think it is going to be the thing that stops the load growth from getting met. So I'm going to say overhyped on that one. I would go underhyped if I look at the non-data center crowd because transformers are needed for everything. And in my reporting, I've come across many projects, housing projects that are completely on hold for 18 months because they're missing this one object that they need before they can power the area. And so it is affecting a lot more people. But the people who can afford to pay for it can jump the queue and get the object and yes, they can get there. And then on the manufacturing side, again, my reporting says these companies are very conservative. They will invest but only if they have surety that there will be demand for transformers for 20 years or so, which they are not yet sure. No, I totally agree with that. And it's already evident in the fact that we've had these long lead times for transformers back to 2021-2022. It's not a new thing and it has been persistent for longer than I think a lot of people expected in part because of the conservatism of that industry who, to be fair to them, they've gone through cycles before where they've over-invested and then been underutilized and so on. And so they're reticent to do that again. I think that the expansion of capacity is not entirely going to come from the incumbents, though. I think there's going to be a wave of upstart companies whether they're doing traditional oil-filled transformers or solid-state stuff like Harren is doing. I think they're going to fill in a lot of the supply gap in the next few years, even if the Hitachi's of the world expand capacity slower than you would want them to. You heard the phrase speed to power a lot lately. But here's what it really means. AI data centers are being told that it will take years to get grid power. They can't afford to wait, so they're turning to on-site power solutions like Bloom Energy. Bloom can deliver clean, ultra-reliable, affordable power that's always on in as little as 90 days. Bloom's fuel cells offer data centers other important advantages. They adapt to volatile AI workloads, they have an ultra-low emissions profile that usually allows for faster and simpler permitting, and they're cost-effective too. That's why leaders from across the industry trust Bloom to power their data centers. Ready to power your AI future? Visit bloomenergy.com to learn more. The grid wasn't built for what's coming next. Electricity demand is surging from data centers to EVs, and utilities they need reliable, affordable solutions that don't require building expensive new infrastructure. Energy Hub helps by turning everyday devices like smart thermostats, EVs, home batteries, and more into virtual power plants. These flexible energy resources respond in near real-time to grid needs, balancing supply and demand. Plus, they can be deployed in under a year and at 40 to 60 percent lower cost than traditional infrastructure. That means more reliability, lower costs, cleaner energy, you can't get much better than that. And that is why over 120 utilities across North America trust Energy Hub to manage more than 1.8 million devices. Learn more at energyhub.com or click the link in the show notes. Do you ever wonder why it takes so long to get clean energy projects up and running? Do you have permitting reform on the brain? Are you NEPA reform curious? The new pace study from Third Way pinpoints the non-cost barriers that stand in the way of clean energy deployment and keep new solar and transmission projects in limbo. They surveyed more than 200 industry professionals to understand what's slowing down deployment and offer practical solutions on how to fix it. To read Third Way's full report and to learn more about the pace initiative, visit thirdway.org slash pace. Okay. So that's bottlenecks. Let's take a diversion for a second here. You had a thing that I have no context on, but you want to talk about this that's relevant to my day job, which is you wanted to talk about VC as a tool for scaling climate tech. What do you want to you're going to grill me on something here? I just don't know what I would happily agree on many, many topics with this one. I wanted to put us on the hype or under high because it's your business. But from the perspective of venture capital helping scale climate solutions, I think VC is over hyped because that model might work in America. And I don't know if it's working because there are not that many companies that have really been VC oriented, big climate champions right now. But rest of the world are building climate solutions. And a lot of those companies aren't VC backed. Your BVID wasn't a VC backed company while it became an electric car giant. So to me, that is one model. It can work in certain cases and it's not working in so many cases that perhaps it's time to look at other forms of ideas that would allow maybe existing companies to pivot towards climate solutions, which also can happen. I think what we've broadly seen historically is that venture capital, particularly venture capital in the US, I will say, plays an outsized role relative to the dollars that go into it plays an outsized role in getting new technologies into commercialization, right? Like from the lab to the market. And usually there's one or two pioneering companies that are venture backed that do that and are the first. So if you want to talk about BYD, like what would BYD be if Tesla hadn't been Tesla? It's an interesting question. If there had never been a Tesla, would BYD still be what it is today? Would it have scaled as quickly or was the existence proof of a company like Tesla the thing that caused BYD in the Chinese government to deploy a lot of capital into it? You've seen versions of that, I think, a lot of times. Yeah. One other example would be CATL, which is the largest battery company. What would it be if LFP, lithium-ion phosphate had not been invented in America and then first commercialized by E1 to 3, which is a venture backed company? That's right. That's a good example of there is a separate question that I think is a good question, which is do the investors, the venture investors and the companies that they invest in reap the full reward of that invention and commercialization? And E1 to 3 is a perfect example of like that company went public and then went bankrupt pretty quickly. So maybe the answer to that is no, although some early investors did make some money on it. So I don't know. That is a different question and that's one that I grapple with all the time. But if you're just taking the big step back 30,000 foot ecosystem view, I still think it is true that again, relative to the dollars deployed, because where the dollar is actually getting deployed, they're getting in total volume, they're either getting deployed into the BYDs and CATLs of the world, or they're getting deployed in infrastructure. The real money goes to building big solar projects and things like that. On a sort of impact to dollars invested ratio, I would still claim venture capital plays an outside role. I would just point back on one thing, which is there's a phrase that Dan Wang, this guy who wrote Breakneck used, which I liked, which is America is good at going from zero to one, creating something that didn't exist. But China is good at going from one to 100, really giving you the solution at the cheapest price possible. A lot of that zero to one in America happens because of the level of science funding and the talent that it has and the basic sciences and the research, which in the current administration is going to go down or is going down already. There is a risk that that thing that VC enables, which is translation of this immense amount of basic science into a usable product, becomes harder. Yes, I agree with that. And I think it is incumbent upon the US to do everything that we can to maintain that position as the part of the world that is best at going zero to one, and then arguably competing for the one to 100 as well. But I don't think it's a foregone conclusion that we will. I'm saying historically it is clear to me that that is true. Looking forward, we've got work to do there. I have another weird one that I've picked for you, which is what do you think about the hype on the Paris agreement helping shape businesses towards climate solutions? Hype, underhype, or just hyped right? So I don't think I have a good answer to this one because, to be totally honest, I don't hear about or think about the Paris agreement at all, like basically zero. So maybe that tells you something. It's certainly in my world, it's not hyped. I can tell you that. But I couldn't tell you whether it should be hyped or not. Do you have a view? Are there things that are downstream of the Paris agreement that are affecting my world that I just don't know came from Paris? Or is it just sort of a it's there and it's a big geopolitical thing, but it's not really affecting business? Yeah, I would say this is the bit as a journalist that I find fascinating because I see many worlds and the Paris agreement actually works as I've seen across all these worlds. But some people have very strong views and other people have no views. So let me explain. In climate diplomacy, Paris agreement is this document that was agreed on in 2015 by all countries in the world, which is in principle voluntary, but sets a goal of 1.5 or 2 degrees Celsius for warming that all countries must keep to. It was 25 years in the making. That's how long it took to come to a conclusion. And then what you got was a voluntary agreement and a lot of the people in diplomacy are happy and sad at the same time. Like we have something, but it's not good enough. And they think about it a lot. We meet at COP meetings every year, which we're going to do in Brazil next month, in November at COP 30. And we'll talk a lot more about those words. And the people in that world continue to be frustrated by Paris. But Paris's impact on businesses is actually quite big because it has become translated into government policy. And then government policy gets translated into all forms of directions that businesses are taking, which they may not always explicitly tell you it is because of Paris. They'll say it's because of government policy and every government has its own policy. But its tentacles have far reaching impact, even if most people don't see the direct connection. I believe that. And I also think probably I'm colored by the fact that I'm in the US where our relationship to Paris has been fraught and back and forth over the years, as opposed to Europe, where it's been consistent. And so probably the policies have had more time to take hold. But it is interesting, your point that people either have very strong opinions about Paris or no opinions. I'm clearly in the latter camp there. All right, we're short on time, but I think we should do a quick grab bag of additional, we could just do a few technologies and decide whether we think they are overhyped under hyped or hyped just right. All right, so let's start with a battery chemistry. Sodium ion batteries over hyped under hyped just right. So I know you've done a couple of episodes, so you probably have a better, more informed view on this. My view is that as somebody who's written about batteries for the last decade, getting new chemistries out to a commercial use case is really hard when companies do that, kudos to them. But given the pace at which lithium ion chemistries have fallen in price and the amount of manufacturing capacity that's been built around the world, if I look at Bloomberg any F projections, sodium ion will make an impact, but such a sliver even after 20, 30 years that I feel like today, sodium ion is overhyped because people think it could make a big difference, but really it may not. I think it's hype just right largely. First of all, I don't think it's that hyped. To be honest, people talk about it, but I think for all the reasons you described, then everybody having had the same history that you're talking about, people are reticent to go all in on this is the next big battery chemistry apart from click-baity articles. But I think there is a degree of hype that is warranted largely because it seems pretty clear that some of the big Chinese manufacturers, CATL, included are investing a lot in sodium ion and are actually deploying. I mean, your point about getting new battery technologies into the field is true. They're doing it though in China. It's coming. The question to me is the one you're alluding to, what portion of the market does it take up? In the last version of a big chemistry shift, which is a smaller chemistry shift than this one, that we saw was the rise of LFP. That has been a huge deal. Will sodium ion be the next wave of the same type of a thing, or is it going to be a more niche solution because it's lower energy density and so on? I don't think we know that yet, but it's coming from CATL and its cohort. Let's just see what happens. The next one is advanced geothermal, where do you land on hype? Underhype, hype just right. In the US, advanced geothermal is very hyped, I would say. It's hyped for a bunch of reasons. Some of them legitimate success of companies like Fervo Energy, pioneering it. Also, there's a political lens to that because our new administration, our Secretary of Energy in particular, Chris Wright is super bullish on geothermal in general. He likes geothermal and nuclear. Those are the two overlapping clean energy things that he's full-throatedly in support of. As a result of that, it is highly hyped. That maybe warrant. Let's see where it goes, but it's very early days. Fervo is hoping to have its first 100 megawatts of its first big project online sometime next year. I guess my argument here would be, it's somewhere in the hyped just right to maybe slightly overhyped stage in the US. I'm curious, though, from your more global perspective, the degree to which advanced geothermal has talked about. This is one where typically the rest of the world is following the US on its hype cycle. Clearly, there's a lot more talk about advanced geothermal because US has shown some success already. Surely, being able to apply a shield drilling type technology could make many other parts of the world, which don't always have great geothermal resources, certainly more valuable. The difficulty I feel like is that it's overhyped because the level of other types of talent and infrastructure that you need to deliver on advanced geothermal exists in the US, but will take a long time in other parts of the world to build. Engineers who can manage shell type drilling or a study of the subsurface that would allow you to do it in the right way, all of that just takes a long time. I don't think that expertise exists in the rest of the world quite as much. We are going to see not as many projects built as many people think there might be. It's an interesting point, right? Because one of the reasons that it is so attractive in the US is that we do have all of this expertise from oil and gas that is being leveraged for advanced geothermal. But of course, we are a country that has that expertise, has the resources, has been doing horizontal directional drilling for a long time, invented it, in fact. So yeah, that's a good point. Okay, last one, continuing on the same vein of technologies that Secretary of Energy Chris Wright likes advanced nuclear, overhyped underhyped just right. Yeah, it's got this similar vibe as advanced geothermal in that, in principle, wouldn't it be nice if we could build a small modular nuclear reactors, because then we could make many of them and try and then lower the price of nuclear, which has been going up and up and up in Europe and North America. And again, the hype is following on from America, where countries like the UK are now asking Rolls Royce to make a small modular reactor. But the reality is the only countries that have functioning civilian, small modular reactors are Russia and China. And the countries who have been building nuclear, not just domestically, but also around the world, are Russia and China. And South Korea, if you want to include like a thousand right. Agree, yeah, South Korea too. And so, do suddenly imagine that a new technology will unlock nuclear in Europe and North America, to me, is overhyped. I think there's an interesting question we're heading into in nuclear world, which is sort of between, I don't know what to call advanced nuclear and not advanced nuclear, but let me just like, I'll frame up to groups. Group one says, we need to just, we have a couple of technologies that are kind of the front runners from large OEMs, right? So this would be the AP 1000 from Westinghouse, and maybe the BDOERX 300 from G Hitachi, which is an SMR. That's a 300 megawatt reactor. Those are the furthest along, they're bankable OEMs. Forget all this other noise. We need to go deploy as many of those as possible, right? And then the other argument is, there's a whole bunch of new technologies coming from a whole raft of companies that could be deploying SMRs, micro reactors or whatever. And some of those need to go win as well. And I don't think it's clear yet which direction we had, if either. One thing I do worry about is that I personally think in the long run, nuclear probably ends up looking like gas turbines look today, which is there, in the gas turbine world, there are basically three OEMs that control like 70, 75% of the market. It's an oligopoly, essentially. And I don't see a really strong reason why it would be different in nuclear. So we need to get through this winnowing. We need to cull the herd a little bit to get to the point where we're deploying enough number of individual reactors to get down that cost curve. And that's going to be a messy process to get there. So I guess my answer is that I think advanced nuclear, if I broadly define it to include all of those things, seems hyped about right to me. But I worry that we are overhyping too many individual technologies, reactor types, companies, relative to what we actually need to do in that market. Yeah, that's an interesting one. I mean, on gas turbines, yeah, it's Siemens, GE, and Mitsubushi, which are the three giants, to the extent where China has been trying to make a gas turbine manufacturing industry, and it's not really succeeding, because it's just so specialized, and the supply chains are so tight. And the supply chain also ends up being one where the same parts are used by jet engines, for example, so like real complications. Nuclear, to me, it's hard to see if it comes down to those three, especially with the kind of political layer attached to nuclear, where transfer of fuel and which country can build in which country is tightly coordinated by the IAEA, the International Atomic Energy Agency. That makes it much harder for me to see just the sort of market forces that drove gas turbine M&A happening in nuclear. Does that argue in what you're saying for there actually being more OEMs than there are in gas turbines? I think there will be national champions and more of them, whereas Siemens, GE, and Mitsubushi are more than national champions, they are sure their countries support them, but they also have such large customers in other countries. It's interesting, because I could see an argument for what you're describing, sort of pushing in the reverse direction, which is that actually it's going to be getting through the gauntlet of being a nuclear reactor OEM is going to be so hard, because of all the additional layers, that even fewer companies are going to get through it. Adding the national lens to it maybe complicates it a little bit, but I could see that being an argument for maybe there aren't three, maybe there's really one or two, rather than there being 30. Well, you're imagining a more peaceful world than I think we are going into, but I hope you're right. All right, I think we covered a bunch of stuff here. This was fun. Thank you, Akshap, for doing this with me. Yeah, this is great fun. Thank you for suggesting the idea. Akshap Rathi is a senior reporter at Bloomberg News and the host of the Zero Podcast. This show is a production of Latitude Media. You can head over to latitudemedia.com for links to today's topics. Latitude is supported by Prelude Ventures. This episode was produced by Daniel Waldorf, mixing and theme song by Sean Markwan. Stephen Lacey is our executive editor. I'm Shale Khan, and this is Catalyst.",
    "release_date": "2025-10-16",
    "duration_ms": 2474000,
    "url": "https://chrt.fm/track/G78F99/traffic.megaphone.fm/PSMI1580408431.mp3?updated=1760622777",
    "source": "Catalyst with Shayle Kann",
    "timestamp": "2025-11-10T02:02:52.201439"
  }
]